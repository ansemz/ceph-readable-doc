.\" Man page generated from reStructuredText.
.
.TH "CEPH" "1" "Feb 10, 2019" "dev" "Ceph"
.SH NAME
ceph \- Ceph dev
.
.nr rst2man-indent-level 0
.
.de1 rstReportMargin
\\$1 \\n[an-margin]
level \\n[rst2man-indent-level]
level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
-
\\n[rst2man-indent0]
\\n[rst2man-indent1]
\\n[rst2man-indent2]
..
.de1 INDENT
.\" .rstReportMargin pre:
. RS \\$1
. nr rst2man-indent\\n[rst2man-indent-level] \\n[an-margin]
. nr rst2man-indent-level +1
.\" .rstReportMargin post:
..
.de UNINDENT
. RE
.\" indent \\n[an-margin]
.\" old: \\n[rst2man-indent\\n[rst2man-indent-level]]
.nr rst2man-indent-level -1
.\" new: \\n[rst2man-indent\\n[rst2man-indent-level]]
.in \\n[rst2man-indent\\n[rst2man-indent-level]]u
..
.SH SYNOPSIS
.nf
\fBceph\-authtool\fP \fIkeyringfile\fP
[ \-l | \-\-list ]
[ \-p | \-\-print\-key ]
[ \-C | \-\-create\-keyring ]
[ \-g | \-\-gen\-key ]
[ \-\-gen\-print\-key ]
[ \-\-import\-keyring \fIotherkeyringfile\fP ]
[ \-n | \-\-name \fIentityname\fP ]
[ \-a | \-\-add\-key \fIbase64_key\fP ]
[ \-\-cap \fIsubsystem\fP \fIcapability\fP ]
[ \-\-caps \fIcapfile\fP ]
[ \-\-mode \fImode\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-authtool\fP is a utility to create, view, and modify a Ceph keyring
file. A keyring file stores one or more Ceph authentication keys and
possibly an associated capability specification. Each key is
associated with an entity name, of the form
\fB{client,mon,mds,osd}.name\fP\&.
.sp
\fBWARNING\fP Ceph provides authentication and protection against
man\-in\-the\-middle attacks once secret keys are in place.  However,
data over the wire is not encrypted, which may include the messages
used to configure said keys. The system is primarily intended to be
used in trusted environments.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-l, \-\-list
will list all keys and capabilities present in the keyring
.UNINDENT
.INDENT 0.0
.TP
.B \-p, \-\-print\-key
will print an encoded key for the specified entityname. This is
suitable for the \fBmount \-o secret=\fP argument
.UNINDENT
.INDENT 0.0
.TP
.B \-C, \-\-create\-keyring
will create a new keyring, overwriting any existing keyringfile
.UNINDENT
.INDENT 0.0
.TP
.B \-g, \-\-gen\-key
will generate a new secret key for the specified entityname
.UNINDENT
.INDENT 0.0
.TP
.B \-\-gen\-print\-key
will generate a new secret key for the specified entityname,
without altering the keyringfile, printing the secret to stdout
.UNINDENT
.INDENT 0.0
.TP
.B \-\-import\-keyring *secondkeyringfile*
will import the content of a given keyring to the keyringfile
.UNINDENT
.INDENT 0.0
.TP
.B \-n, \-\-name *name*
specify entityname to operate on
.UNINDENT
.INDENT 0.0
.TP
.B \-a, \-\-add\-key *base64_key*
will add an encoded key to the keyring
.UNINDENT
.INDENT 0.0
.TP
.B \-\-cap *subsystem* *capability*
will set the capability for given subsystem
.UNINDENT
.INDENT 0.0
.TP
.B \-\-caps *capsfile*
.INDENT 7.0
.INDENT 3.5
will set all of capabilities associated with a given key, for all subsystems
.UNINDENT
.UNINDENT
.INDENT 7.0
.TP
.B \-\-mode *mode*
will set the desired file mode to the keyring e.g: 0644, defaults to 0600
.UNINDENT
.UNINDENT
.SH CAPABILITIES
.sp
The subsystem is the name of a Ceph subsystem: \fBmon\fP, \fBmds\fP, or
\fBosd\fP\&.
.sp
The capability is a string describing what the given user is allowed
to do. This takes the form of a comma separated list of allow
clauses with a permission specifier containing one or more of rwx for
read, write, and execute permission. The \fBallow *\fP grants full
superuser permissions for the given subsystem.
.sp
For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
# can read, write, and execute objects
osd = "allow rwx"

# can access mds server
mds = "allow"

# can modify cluster state (i.e., is a server daemon)
mon = "allow rwx"
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A librados user restricted to a single pool might look like:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mon = "allow r"

osd = "allow rw pool foo"
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A client using rbd with read access to one pool and read/write access to another:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mon = "allow r"

osd = "allow class\-read object_prefix rbd_children, allow pool templates r class\-read, allow pool vms rwx"
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
A client mounting the file system with minimal permissions would need caps like:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mds = "allow"

osd = "allow rw pool data"

mon = "allow r"
.ft P
.fi
.UNINDENT
.UNINDENT
.SH OSD CAPABILITIES
.sp
In general, an osd capability follows the grammar:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
osdcap  := grant[,grant...]
grant   := allow (match capspec | capspec match)
match   := [ pool[=]<poolname> | object_prefix <prefix>
            | namespace[=]<rados\-namespace>
            | tag <application\-name> <key>=<value> ]
capspec := * | [r][w][x] [class\-read] [class\-write]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The capspec determines what kind of operations the entity can perform:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
r           = read access to objects
w           = write access to objects
x           = can call any class method (same as class\-read class\-write)
class\-read  = can call class methods that are reads
class\-write = can call class methods that are writes
* or "all"  = equivalent to rwx, plus the ability to run osd admin commands,
              i.e. ceph osd tell ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The match criteria restrict a grant based on the pool being accessed.
Grants are additive if the client fulfills the match condition. For
example, if a client has the osd capabilities: "allow r object_prefix
prefix, allow w pool foo, allow x pool bar", then it has rw access to
pool foo, rx access to pool bar, and r access to objects whose
names begin with \(aqprefix\(aq in any pool.
.SH CAPS FILE FORMAT
.sp
The caps file format consists of zero or more key/value pairs, one per
line. The key and value are separated by an \fB=\fP, and the value must
be quoted (with \fB\(aq\fP or \fB"\fP) if it contains any whitespace. The key
is the name of the Ceph subsystem (\fBosd\fP, \fBmds\fP, \fBmon\fP), and the
value is the capability string (see above).
.SH EXAMPLE
.sp
To create a new keyring containing a key for client.foo with a 0644 file mode:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-authtool \-C \-n client.foo \-\-gen\-key keyring \-\-mode 0644
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To associate some capabilities with the key (namely, the ability to
mount a Ceph filesystem):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-authtool \-n client.foo \-\-cap mds \(aqallow\(aq \-\-cap osd \(aqallow rw pool=data\(aq \-\-cap mon \(aqallow r\(aq keyring
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To display the contents of the keyring:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-authtool \-l keyring
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
When mounting a Ceph file system, you can grab the appropriately encoded secret key with:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mount \-t ceph serverhost:/ mountpoint \-o name=foo,secret=\(gaceph\-authtool \-p \-n client.foo keyring\(ga
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-authtool\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please
refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8)
.SH SYNOPSIS
.nf
\fBceph\-clsinfo\fP [ \fIoptions\fP ] ... \fIfilename\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-clsinfo\fP can show name, version, and architecture information
about a specific class object.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-n, \-\-name
Shows the class name
.UNINDENT
.INDENT 0.0
.TP
.B \-v, \-\-version
Shows the class version
.UNINDENT
.INDENT 0.0
.TP
.B \-a, \-\-arch
Shows the class architecture
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-clsinfo\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please
refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8)
.SH SYNOPSIS
.nf
\fBceph\-conf\fP \-c \fIconffile\fP \-\-list\-all\-sections
\fBceph\-conf\fP \-c \fIconffile\fP \-L
\fBceph\-conf\fP \-c \fIconffile\fP \-l \fIprefix\fP
\fBceph\-conf\fP \fIkey\fP \-s \fIsection1\fP ...
\fBceph\-conf\fP [\-s \fIsection\fP ] [\-r] \-\-lookup \fIkey\fP
\fBceph\-conf\fP [\-s \fIsection\fP ] \fIkey\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-conf\fP is a utility for getting information from a ceph
configuration file. As with most Ceph programs, you can specify which
Ceph configuration file to use with the \fB\-c\fP flag.
.sp
Note that unlike other ceph tools, \fBceph\-conf\fP will \fIonly\fP read from
config files (or return compiled\-in default values)\-\-it will \fInot\fP
fetch config values from the monitor cluster.  For this reason it is
recommended that \fBceph\-conf\fP only be used in legacy environments
that are strictly config\-file based.  New deployments and tools should
instead rely on either querying the monitor explicitly for
configuration (e.g., \fBceph config get <daemon> <option>\fP) or use
daemons themselves to fetch effective config options (e.g.,
\fBceph\-osd \-i 123 \-\-show\-config\-value osd_data\fP).  The latter option
has the advantages of drawing from compiled\-in defaults (which
occasionally vary between daemons), config files, and the monitor\(aqs
config database, providing the exact value that that daemon would be
using if it were started.
.SH ACTIONS
.sp
\fBceph\-conf\fP performs one of the following actions:
.INDENT 0.0
.TP
.B \-L, \-\-list\-all\-sections
list all sections in the configuration file.
.UNINDENT
.INDENT 0.0
.TP
.B \-l, \-\-list\-sections *prefix*
list the sections with the given \fIprefix\fP\&. For example, \fB\-\-list\-sections mon\fP
would list all sections beginning with \fBmon\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-lookup *key*
search and print the specified configuration setting. Note:  \fB\-\-lookup\fP is
the default action. If no other actions are given on the command line, we will
default to doing a lookup.
.UNINDENT
.INDENT 0.0
.TP
.B \-h, \-\-help
print a summary of usage.
.UNINDENT
.SH OPTIONS
.INDENT 0.0
.TP
.B \-c *conffile*
the Ceph configuration file.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-filter\-key *key*
filter section list to only include sections with given \fIkey\fP defined.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-filter\-key\-value *key* \(ga\(ga=\(ga\(ga *value*
filter section list to only include sections with given \fIkey\fP/\fIvalue\fP pair.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-name *type.id*
the Ceph name in which the sections are searched (default \(aqclient.admin\(aq).
For example, if we specify \fB\-\-name osd.0\fP, the following sections will be
searched: [osd.0], [osd], [global]
.UNINDENT
.INDENT 0.0
.TP
.B \-r, \-\-resolve\-search
search for the first file that exists and can be opened in the resulted
comma delimited search list.
.UNINDENT
.INDENT 0.0
.TP
.B \-s, \-\-section
additional sections to search.  These additional sections will be searched
before the sections that would normally be searched. As always, the first
matching entry we find will be returned.
.UNINDENT
.SH EXAMPLES
.sp
To find out what value osd 0 will use for the "osd data" option:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-conf \-c foo.conf  \-\-name osd.0 \-\-lookup "osd data"
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To find out what value will mds a use for the "log file" option:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-conf \-c foo.conf  \-\-name mds.a "log file"
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To list all sections that begin with "osd":
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-conf \-c foo.conf \-l osd
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To list all sections:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-conf \-c foo.conf \-L
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To print the path of the "keyring" used by "client.0":
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-conf \-\-name client.0 \-r \-l keyring
.ft P
.fi
.UNINDENT
.UNINDENT
.SH FILES
.sp
\fB/etc/ceph/$cluster.conf\fP, \fB~/.ceph/$cluster.conf\fP, \fB$cluster.conf\fP
.sp
the Ceph configuration files to use if not specified.
.SH AVAILABILITY
.sp
\fBceph\-conf\fP is part of Ceph, a massively scalable, open\-source, distributed storage system.  Please refer
to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8),
.SH SYNOPSIS
.nf
\fBceph\-create\-keys\fP [\-h] [\-v] [\-t seconds] [\-\-cluster \fIname\fP] \-\-id \fIid\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-create\-keys\fP is a utility to generate bootstrap keyrings using
the given monitor when it is ready.
.sp
It creates following auth entities (or users)
.sp
\fBclient.admin\fP
.INDENT 0.0
.INDENT 3.5
and its key for your client host.
.UNINDENT
.UNINDENT
.sp
\fBclient.bootstrap\-{osd, rgw, mds}\fP
.INDENT 0.0
.INDENT 3.5
and their keys for bootstrapping corresponding services
.UNINDENT
.UNINDENT
.sp
To list all users in the cluster:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth ls
.ft P
.fi
.UNINDENT
.UNINDENT
.SH OPTIONS
.INDENT 0.0
.TP
.B \-\-cluster
name of the cluster (default \(aqceph\(aq).
.UNINDENT
.INDENT 0.0
.TP
.B \-t
time out after \fBseconds\fP (default: 600) waiting for a response from the monitor
.UNINDENT
.INDENT 0.0
.TP
.B \-i, \-\-id
id of a ceph\-mon that is coming up. \fBceph\-create\-keys\fP will wait until it joins quorum.
.UNINDENT
.INDENT 0.0
.TP
.B \-v, \-\-verbose
be more verbose.
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-create\-keys\fP is part of Ceph, a massively scalable, open\-source, distributed storage system.  Please refer
to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8)
.SH SYNOPSIS
.nf
\fBceph\-debugpack\fP [ \fIoptions\fP ] \fIfilename.tar.gz\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-debugpack\fP will build a tarball containing various items that are
useful for debugging crashes. The resulting tarball can be shared with
Ceph developers when debugging a problem.
.sp
The tarball will include the binaries for ceph\-mds, ceph\-osd, and ceph\-mon, radosgw, any
log files, the ceph.conf configuration file, any core files we can
find, and (if the system is running) dumps of the current cluster state
as reported by \(aqceph report\(aq.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fIceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during
startup.
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-debugpack\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please
refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8)
ceph\-post\-file(8)
.SH SYNOPSIS
.nf
\fBceph\-dencoder\fP [commands...]
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-dencoder\fP is a utility to encode, decode, and dump ceph data
structures.  It is used for debugging and for testing inter\-version
compatibility.
.sp
\fBceph\-dencoder\fP takes a simple list of commands and performs them
in order.
.SH COMMANDS
.INDENT 0.0
.TP
.B version
Print the version string for the \fBceph\-dencoder\fP binary.
.UNINDENT
.INDENT 0.0
.TP
.B import <file>
Read a binary blob of encoded data from the given file.  It will be
placed in an in\-memory buffer.
.UNINDENT
.INDENT 0.0
.TP
.B export <file>
Write the contents of the current in\-memory buffer to the given
file.
.UNINDENT
.INDENT 0.0
.TP
.B list_types
List the data types known to this build of \fBceph\-dencoder\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B type <name>
Select the given type for future \fBencode\fP or \fBdecode\fP operations.
.UNINDENT
.INDENT 0.0
.TP
.B skip <bytes>
Seek <bytes> into the imported file before reading data structure, use
this with objects that have a preamble/header before the object of interest.
.UNINDENT
.INDENT 0.0
.TP
.B decode
Decode the contents of the in\-memory buffer into an instance of the
previously selected type.  If there is an error, report it.
.UNINDENT
.INDENT 0.0
.TP
.B encode
Encode the contents of the in\-memory instance of the previously
selected type to the in\-memory buffer.
.UNINDENT
.INDENT 0.0
.TP
.B dump_json
Print a JSON\-formatted description of the in\-memory object.
.UNINDENT
.INDENT 0.0
.TP
.B count_tests
Print the number of built\-in test instances of the previously
selected type that \fBceph\-dencoder\fP is able to generate.
.UNINDENT
.INDENT 0.0
.TP
.B select_test <n>
Select the given build\-in test instance as a the in\-memory instance
of the type.
.UNINDENT
.INDENT 0.0
.TP
.B get_features
Print the decimal value of the feature set supported by this version
of \fBceph\-dencoder\fP\&.  Each bit represents a feature.  These correspond to
CEPH_FEATURE_* defines in src/include/ceph_features.h.
.UNINDENT
.INDENT 0.0
.TP
.B set_features <f>
Set the feature bits provided to \fBencode\fP to \fIf\fP\&.  This allows
you to encode objects such that they can be understood by old
versions of the software (for those types that support it).
.UNINDENT
.SH EXAMPLE
.sp
Say you want to examine an attribute on an object stored by \fBceph\-osd\fP\&.  You can do this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ cd /mnt/osd.12/current/2.b_head
$ attr \-l foo_bar_head_EFE6384B
Attribute "ceph.snapset" has a 31 byte value for foo_bar_head_EFE6384B
Attribute "ceph._" has a 195 byte value for foo_bar_head_EFE6384B
$ attr foo_bar_head_EFE6384B \-g ceph._ \-q > /tmp/a
$ ceph\-dencoder type object_info_t import /tmp/a decode dump_json
{ "oid": { "oid": "foo",
      "key": "bar",
      "snapid": \-2,
      "hash": 4024842315,
      "max": 0},
  "locator": { "pool": 2,
      "preferred": \-1,
      "key": "bar"},
  "category": "",
  "version": "9\(aq1",
  "prior_version": "0\(aq0",
  "last_reqid": "client.4116.0:1",
  "size": 1681,
  "mtime": "2012\-02\-21 08:58:23.666639",
  "lost": 0,
  "wrlock_by": "unknown.0.0:0",
  "snaps": [],
  "truncate_seq": 0,
  "truncate_size": 0,
  "watchers": {}}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Alternatively, perhaps you wish to dump an internal CephFS metadata object, you might
do that like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ rados \-p metadata get mds_snaptable mds_snaptable.bin
$ ceph\-dencoder type SnapServer skip 8 import mds_snaptable.bin decode dump_json
{ "snapserver": { "last_snap": 1,
   "pending_noop": [],
   "snaps": [],
   "need_to_purge": {},
   "pending_create": [],
   "pending_destroy": []}}
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-dencoder\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please
refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8)
.SH SYNOPSIS
.nf
\fBceph\-deploy\fP \fBnew\fP [\fIinitial\-monitor\-node(s)\fP]
.fi
.sp
.nf
\fBceph\-deploy\fP \fBinstall\fP [\fIceph\-node\fP] [\fIceph\-node\fP\&...]
.fi
.sp
.nf
\fBceph\-deploy\fP \fBmon\fP \fIcreate\-initial\fP
.fi
.sp
.nf
\fBceph\-deploy\fP \fBosd\fP \fIcreate\fP \fI\-\-data\fP \fIdevice\fP \fIceph\-node\fP
.fi
.sp
.nf
\fBceph\-deploy\fP \fBadmin\fP [\fIadmin\-node\fP][\fIceph\-node\fP\&...]
.fi
.sp
.nf
\fBceph\-deploy\fP \fBpurgedata\fP [\fIceph\-node\fP][\fIceph\-node\fP\&...]
.fi
.sp
.nf
\fBceph\-deploy\fP \fBforgetkeys\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-deploy\fP is a tool which allows easy and quick deployment of a
Ceph cluster without involving complex and detailed manual configuration. It
uses ssh to gain access to other Ceph nodes from the admin node, sudo for
administrator privileges on them and the underlying Python scripts automates
the manual process of Ceph installation on each node from the admin node itself.
It can be easily run on an workstation and doesn\(aqt require servers, databases or
any other automated tools. With \fBceph\-deploy\fP, it is really easy to set
up and take down a cluster. However, it is not a generic deployment tool. It is
a specific tool which is designed for those who want to get Ceph up and running
quickly with only the unavoidable initial configuration settings and without the
overhead of installing other tools like \fBChef\fP, \fBPuppet\fP or \fBJuju\fP\&. Those
who want to customize security settings, partitions or directory locations and
want to set up a cluster following detailed manual steps, should use other tools
i.e, \fBChef\fP, \fBPuppet\fP, \fBJuju\fP or \fBCrowbar\fP\&.
.sp
With \fBceph\-deploy\fP, you can install Ceph packages on remote nodes,
create a cluster, add monitors, gather/forget keys, add OSDs and metadata
servers, configure admin hosts or take down the cluster.
.SH COMMANDS
.SS new
.sp
Start deploying a new cluster and write a configuration file and keyring for it.
It tries to copy ssh keys from admin node to gain passwordless ssh to monitor
node(s), validates host IP, creates a cluster with a new initial monitor node or
nodes for monitor quorum, a ceph configuration file, a monitor secret keyring and
a log file for the new cluster. It populates the newly created Ceph configuration
file with \fBfsid\fP of cluster, hostnames and IP addresses of initial monitor
members under \fB[global]\fP section.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy new [MON][MON...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [MON] is the initial monitor hostname (short hostname i.e, \fBhostname \-s\fP).
.sp
Other options like \fI\%\-\-no\-ssh\-copykey\fP, \fI\%\-\-fsid\fP,
\fI\%\-\-cluster\-network\fP and \fI\%\-\-public\-network\fP can also be used with
this command.
.sp
If more than one network interface is used, \fBpublic network\fP setting has to be
added under \fB[global]\fP section of Ceph configuration file. If the public subnet
is given, \fBnew\fP command will choose the one IP from the remote host that exists
within the subnet range. Public network can also be added at runtime using
\fI\%\-\-public\-network\fP option with the command as mentioned above.
.SS install
.sp
Install Ceph packages on remote hosts. As a first step it installs
\fByum\-plugin\-priorities\fP in admin and other nodes using passwordless ssh and sudo
so that Ceph packages from upstream repository get more priority. It then detects
the platform and distribution for the hosts and installs Ceph normally by
downloading distro compatible packages if adequate repo for Ceph is already added.
\fB\-\-release\fP flag is used to get the latest release for installation. During
detection of platform and distribution before installation, if it finds the
\fBdistro.init\fP to be \fBsysvinit\fP (Fedora, CentOS/RHEL etc), it doesn\(aqt allow
installation with custom cluster name and uses the default name \fBceph\fP for the
cluster.
.sp
If the user explicitly specifies a custom repo url with \fI\%\-\-repo\-url\fP for
installation, anything detected from the configuration will be overridden and
the custom repository location will be used for installation of Ceph packages.
If required, valid custom repositories are also detected and installed. In case
of installation from a custom repo a boolean is used to determine the logic
needed to proceed with a custom repo installation. A custom repo install helper
is used that goes through config checks to retrieve repos (and any extra repos
defined) and installs them. \fBcd_conf\fP is the object built from \fBargparse\fP
that holds the flags and information needed to determine what metadata from the
configuration is to be used.
.sp
A user can also opt to install only the repository without installing Ceph and
its dependencies by using \fI\%\-\-repo\fP option.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy install [HOST][HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is/are the host node(s) where Ceph is to be installed.
.sp
An option \fB\-\-release\fP is used to install a release known as CODENAME
(default: firefly).
.sp
Other options like \fI\%\-\-testing\fP, \fI\%\-\-dev\fP, \fI\%\-\-adjust\-repos\fP,
\fI\%\-\-no\-adjust\-repos\fP, \fI\%\-\-repo\fP, \fI\%\-\-local\-mirror\fP,
\fI\%\-\-repo\-url\fP and \fI\%\-\-gpg\-url\fP can also be used with this command.
.SS mds
.sp
Deploy Ceph mds on remote hosts. A metadata server is needed to use CephFS and
the \fBmds\fP command is used to create one on the desired host node. It uses the
subcommand \fBcreate\fP to do so. \fBcreate\fP first gets the hostname and distro
information of the desired mds host. It then tries to read the \fBbootstrap\-mds\fP
key for the cluster and deploy it in the desired host. The key generally has a
format of \fB{cluster}.bootstrap\-mds.keyring\fP\&. If it doesn\(aqt finds a keyring,
it runs \fBgatherkeys\fP to get the keyring. It then creates a mds on the desired
host under the path \fB/var/lib/ceph/mds/\fP in \fB/var/lib/ceph/mds/{cluster}\-{name}\fP
format and a bootstrap keyring under \fB/var/lib/ceph/bootstrap\-mds/\fP in
\fB/var/lib/ceph/bootstrap\-mds/{cluster}.keyring\fP format. It then runs appropriate
commands based on \fBdistro.init\fP to start the \fBmds\fP\&.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy mds create [HOST[:DAEMON\-NAME]] [HOST[:DAEMON\-NAME]...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The [DAEMON\-NAME] is optional.
.SS mon
.sp
Deploy Ceph monitor on remote hosts. \fBmon\fP makes use of certain subcommands
to deploy Ceph monitors on other nodes.
.sp
Subcommand \fBcreate\-initial\fP deploys for monitors defined in
\fBmon initial members\fP under \fB[global]\fP section in Ceph configuration file,
wait until they form quorum and then gatherkeys, reporting the monitor status
along the process. If monitors don\(aqt form quorum the command will eventually
time out.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy mon create\-initial
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBcreate\fP is used to deploy Ceph monitors by explicitly specifying
the hosts which are desired to be made monitors. If no hosts are specified it
will default to use the \fBmon initial members\fP defined under \fB[global]\fP
section of Ceph configuration file. \fBcreate\fP first detects platform and distro
for desired hosts and checks if hostname is compatible for deployment. It then
uses the monitor keyring initially created using \fBnew\fP command and deploys the
monitor in desired host. If multiple hosts were specified during \fBnew\fP command
i.e, if there are multiple hosts in \fBmon initial members\fP and multiple keyrings
were created then a concatenated keyring is used for deployment of monitors. In
this process a keyring parser is used which looks for \fB[entity]\fP sections in
monitor keyrings and returns a list of those sections. A helper is then used to
collect all keyrings into a single blob that will be used to inject it to monitors
with \fI\%\-\-mkfs\fP on remote nodes. All keyring files are concatenated to be
in a directory ending with \fB\&.keyring\fP\&. During this process the helper uses list
of sections returned by keyring parser to check if an entity is already present
in a keyring and if not, adds it. The concatenated keyring is used for deployment
of monitors to desired multiple hosts.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy mon create [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is hostname of desired monitor host(s).
.sp
Subcommand \fBadd\fP is used to add a monitor to an existing cluster. It first
detects platform and distro for desired host and checks if hostname is compatible
for deployment. It then uses the monitor keyring, ensures configuration for new
monitor host and adds the monitor to the cluster. If the section for the monitor
exists and defines a monitor address that will be used, otherwise it will fallback by
resolving the hostname to an IP. If \fI\%\-\-address\fP is used it will override
all other options. After adding the monitor to the cluster, it gives it some time
to start. It then looks for any monitor errors and checks monitor status. Monitor
errors arise if the monitor is not added in \fBmon initial members\fP, if it doesn\(aqt
exist in \fBmonmap\fP and if neither \fBpublic_addr\fP nor \fBpublic_network\fP keys
were defined for monitors. Under such conditions, monitors may not be able to
form quorum. Monitor status tells if the monitor is up and running normally. The
status is checked by running \fBceph daemon mon.hostname mon_status\fP on remote
end which provides the output and returns a boolean status of what is going on.
\fBFalse\fP means a monitor that is not fine even if it is up and running, while
\fBTrue\fP means the monitor is up and running correctly.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy mon add [HOST]

ceph\-deploy mon add [HOST] \-\-address [IP]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is the hostname and [IP] is the IP address of the desired monitor
node. Please note, unlike other \fBmon\fP subcommands, only one node can be
specified at a time.
.sp
Subcommand \fBdestroy\fP is used to completely remove monitors on remote hosts.
It takes hostnames as arguments. It stops the monitor, verifies if \fBceph\-mon\fP
daemon really stopped, creates an archive directory \fBmon\-remove\fP under
\fB/var/lib/ceph/\fP, archives old monitor directory in
\fB{cluster}\-{hostname}\-{stamp}\fP format in it and removes the monitor from
cluster by running \fBceph remove...\fP command.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy mon destroy [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is hostname of monitor that is to be removed.
.SS gatherkeys
.sp
Gather authentication keys for provisioning new nodes. It takes hostnames as
arguments. It checks for and fetches \fBclient.admin\fP keyring, monitor keyring
and \fBbootstrap\-mds/bootstrap\-osd\fP keyring from monitor host. These
authentication keys are used when new \fBmonitors/OSDs/MDS\fP are added to the
cluster.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy gatherkeys [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is hostname of the monitor from where keys are to be pulled.
.SS disk
.sp
Manage disks on a remote host. It actually triggers the \fBceph\-volume\fP utility
and its subcommands to manage disks.
.sp
Subcommand \fBlist\fP lists disk partitions and Ceph OSDs.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy disk list HOST
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBzap\fP zaps/erases/destroys a device\(aqs partition table and
contents.  It actually uses \fBceph\-volume lvm zap\fP remotely, alternatively
allowing someone to remove the Ceph metadata from the logical volume.
.SS osd
.sp
Manage OSDs by preparing data disk on remote host. \fBosd\fP makes use of certain
subcommands for managing OSDs.
.sp
Subcommand \fBcreate\fP prepares a device for Ceph OSD. It first checks against
multiple OSDs getting created and warns about the possibility of more than the
recommended which would cause issues with max allowed PIDs in a system. It then
reads the bootstrap\-osd key for the cluster or writes the bootstrap key if not
found.
It then uses \fBceph\-volume\fP utility\(aqs \fBlvm create\fP subcommand to
prepare the disk, (and journal if using filestore) and deploy the OSD on the desired host.
Once prepared, it gives some time to the OSD to start and checks for any
possible errors and if found, reports to the user.
.sp
Bluestore Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy osd create \-\-data DISK HOST
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Filestore Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy osd create \-\-data DISK \-\-journal JOURNAL HOST
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBNOTE:\fP
.INDENT 0.0
.INDENT 3.5
For other flags available, please see the man page or the \-\-help menu
on ceph\-deploy osd create
.UNINDENT
.UNINDENT
.sp
Subcommand \fBlist\fP lists devices associated to Ceph as part of an OSD.
It uses the \fBceph\-volume lvm list\fP output that has a rich output, mapping
OSDs to devices and other interesting information about the OSD setup.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy osd list HOST
.ft P
.fi
.UNINDENT
.UNINDENT
.SS admin
.sp
Push configuration and \fBclient.admin\fP key to a remote host. It takes
the \fB{cluster}.client.admin.keyring\fP from admin node and writes it under
\fB/etc/ceph\fP directory of desired node.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy admin [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is desired host to be configured for Ceph administration.
.SS config
.sp
Push/pull configuration file to/from a remote host. It uses \fBpush\fP subcommand
to takes the configuration file from admin host and write it to remote host under
\fB/etc/ceph\fP directory. It uses \fBpull\fP subcommand to do the opposite i.e, pull
the configuration file under \fB/etc/ceph\fP directory of remote host to admin node.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy config push [HOST] [HOST...]

ceph\-deploy config pull [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is the hostname of the node where config file will be pushed to or
pulled from.
.SS uninstall
.sp
Remove Ceph packages from remote hosts. It detects the platform and distro of
selected host and uninstalls Ceph packages from it. However, some dependencies
like \fBlibrbd1\fP and \fBlibrados3\fP will not be removed because they can cause
issues with \fBqemu\-kvm\fP\&.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy uninstall [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is hostname of the node from where Ceph will be uninstalled.
.SS purge
.sp
Remove Ceph packages from remote hosts and purge all data. It detects the
platform and distro of selected host, uninstalls Ceph packages and purges all
data. However, some dependencies like \fBlibrbd1\fP and \fBlibrados3\fP will not be
removed because they can cause issues with \fBqemu\-kvm\fP\&.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy purge [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is hostname of the node from where Ceph will be purged.
.SS purgedata
.sp
Purge (delete, destroy, discard, shred) any Ceph data from \fB/var/lib/ceph\fP\&.
Once it detects the platform and distro of desired host, it first checks if Ceph
is still installed on the selected host and if installed, it won\(aqt purge data
from it. If Ceph is already uninstalled from the host, it tries to remove the
contents of \fB/var/lib/ceph\fP\&. If it fails then probably OSDs are still mounted
and needs to be unmounted to continue. It unmount the OSDs and tries to remove
the contents of \fB/var/lib/ceph\fP again and checks for errors. It also removes
contents of \fB/etc/ceph\fP\&. Once all steps are successfully completed, all the
Ceph data from the selected host are removed.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy purgedata [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [HOST] is hostname of the node from where Ceph data will be purged.
.SS forgetkeys
.sp
Remove authentication keys from the local directory. It removes all the
authentication keys i.e, monitor keyring, client.admin keyring, bootstrap\-osd
and bootstrap\-mds keyring from the node.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy forgetkeys
.ft P
.fi
.UNINDENT
.UNINDENT
.SS pkg
.sp
Manage packages on remote hosts. It is used for installing or removing packages
from remote hosts. The package names for installation or removal are to be
specified after the command. Two options \fI\%\-\-install\fP and
\fI\%\-\-remove\fP are used for this purpose.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-deploy pkg \-\-install [PKGs] [HOST] [HOST...]

ceph\-deploy pkg \-\-remove [PKGs] [HOST] [HOST...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Here, [PKGs] is comma\-separated package names and [HOST] is hostname of the
remote node where packages are to be installed or removed from.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-\-address
IP address of the host node to be added to the cluster.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-adjust\-repos
Install packages modifying source repos.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-ceph\-conf
Use (or reuse) a given \fBceph.conf\fP file.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-cluster
Name of the cluster.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-dev
Install a bleeding edge built from Git branch or tag (default: master).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-cluster\-network
Specify the (internal) cluster network.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-dmcrypt
Encrypt [data\-path] and/or journal devices with \fBdm\-crypt\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-dmcrypt\-key\-dir
Directory where \fBdm\-crypt\fP keys are stored.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-install
Comma\-separated package(s) to install on remote hosts.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-fs\-type
Filesystem to use to format disk \fB(xfs, btrfs or ext4)\fP\&.  Note that support for btrfs and ext4 is no longer tested or recommended; please use xfs.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-fsid
Provide an alternate FSID for \fBceph.conf\fP generation.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-gpg\-url
Specify a GPG key url to be used with custom repos (defaults to ceph.com).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-keyrings
Concatenate multiple keyrings to be seeded on new monitors.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-local\-mirror
Fetch packages and push them to hosts for a local repo mirror.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-mkfs
Inject keys to MONs on remote nodes.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-no\-adjust\-repos
Install packages without modifying source repos.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-no\-ssh\-copykey
Do not attempt to copy ssh keys.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-overwrite\-conf
Overwrite an existing conf file on remote host (if present).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-public\-network
Specify the public network for a cluster.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-remove
Comma\-separated package(s) to remove from remote hosts.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-repo
Install repo files only (skips package installation).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-repo\-url
Specify a repo url that mirrors/contains Ceph packages.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-testing
Install the latest development release.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-username
The username to connect to the remote host.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-version
The current installed version of \fBceph\-deploy\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-zap\-disk
Destroy the partition table and content of a disk.
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-deploy\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the documentation at \fI\%https://ceph.com/ceph\-deploy/docs\fP for more information.
.SH SEE ALSO
.sp
ceph\-mon(8),
ceph\-osd(8),
ceph\-volume(8),
ceph\-mds(8)
.SH SYNOPSIS
.nf
\fBceph\-volume\fP [\-h] [\-\-cluster CLUSTER] [\-\-log\-level LOG_LEVEL]
.in +2
[\-\-log\-path LOG_PATH]
.in -2
.fi
.sp
.nf
\fBceph\-volume\fP \fBinventory\fP
.fi
.sp
.nf
\fBceph\-volume\fP \fBlvm\fP [ \fItrigger\fP | \fIcreate\fP | \fIactivate\fP | \fIprepare\fP
\fIzap\fP | \fIlist\fP | \fIbatch\fP]
.fi
.sp
.nf
\fBceph\-volume\fP \fBsimple\fP [ \fItrigger\fP | \fIscan\fP | \fIactivate\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-volume\fP is a single purpose command line tool to deploy logical
volumes as OSDs, trying to maintain a similar API to \fBceph\-disk\fP when
preparing, activating, and creating OSDs.
.sp
It deviates from \fBceph\-disk\fP by not interacting or relying on the udev rules
that come installed for Ceph. These rules allow automatic detection of
previously setup devices that are in turn fed into \fBceph\-disk\fP to activate
them.
.SH COMMANDS
.SS inventory
.sp
This subcommand provides information about a host\(aqs physical disc inventory and
reports metadata about these discs. Among this metadata one can find disc
specific data items (like model, size, rotational or solid state) as well as
data items specific to ceph using a device, such as if it is available for
use with ceph or if logical volumes are present.
.sp
Examples:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume inventory
ceph\-volume inventory /dev/sda
ceph\-volume inventory \-\-format json\-pretty
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Optional arguments:
.INDENT 0.0
.IP \(bu 2
[\-h, \-\-help]          show the help message and exit
.IP \(bu 2
.INDENT 2.0
.TP
.B [\-\-format]            report format, valid values are \fBplain\fP (default),
\fBjson\fP and \fBjson\-pretty\fP
.UNINDENT
.UNINDENT
.SS lvm
.sp
By making use of LVM tags, the \fBlvm\fP sub\-command is able to store and later
re\-discover and query devices associated with OSDs so that they can later
activated.
.sp
Subcommands:
.sp
\fBbatch\fP
Creates OSDs from a list of devices using a \fBfilestore\fP
or \fBbluestore\fP (default) setup. It will create all necessary volume groups
and logical volumes required to have a working OSD.
.sp
Example usage with three devices:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm batch \-\-bluestore /dev/sda /dev/sdb /dev/sdc
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Optional arguments:
.INDENT 0.0
.IP \(bu 2
[\-h, \-\-help]          show the help message and exit
.IP \(bu 2
[\-\-bluestore]         Use the bluestore objectstore (default)
.IP \(bu 2
[\-\-filestore]         Use the filestore objectstore
.IP \(bu 2
[\-\-yes]               Skip the report and prompt to continue provisioning
.IP \(bu 2
[\-\-prepare]           Only prepare OSDs, do not activate
.IP \(bu 2
[\-\-dmcrypt]           Enable encryption for the underlying OSD devices
.IP \(bu 2
[\-\-crush\-device\-class] Define a CRUSH device class to assign the OSD to
.IP \(bu 2
[\-\-no\-systemd]         Do not enable or create any systemd units
.IP \(bu 2
.INDENT 2.0
.TP
.B [\-\-report]         Report what the potential outcome would be for the
current input (requires devices to be passed in)
.UNINDENT
.IP \(bu 2
.INDENT 2.0
.TP
.B [\-\-format]         Output format when reporting (used along with
\-\-report), can be one of \(aqpretty\(aq (default) or \(aqjson\(aq
.UNINDENT
.IP \(bu 2
.INDENT 2.0
.TP
.B [\-\-block\-db\-size]     Set (or override) the "bluestore_block_db_size" value,
in bytes
.UNINDENT
.IP \(bu 2
[\-\-journal\-size]      Override the "osd_journal_size" value, in megabytes
.UNINDENT
.sp
Required positional arguments:
.INDENT 0.0
.IP \(bu 2
.INDENT 2.0
.TP
.B <DEVICE>    Full path to a raw device, like \fB/dev/sda\fP\&. Multiple
\fB<DEVICE>\fP paths can be passed in.
.UNINDENT
.UNINDENT
.sp
\fBactivate\fP
Enables a systemd unit that persists the OSD ID and its UUID (also called
\fBfsid\fP in Ceph CLI tools), so that at boot time it can understand what OSD is
enabled and needs to be mounted.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm activate \-\-bluestore <osd id> <osd fsid>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Optional Arguments:
.INDENT 0.0
.IP \(bu 2
[\-h, \-\-help]  show the help message and exit
.IP \(bu 2
[\-\-auto\-detect\-objectstore] Automatically detect the objectstore by inspecting
the OSD
.IP \(bu 2
[\-\-bluestore] bluestore objectstore (default)
.IP \(bu 2
[\-\-filestore] filestore objectstore
.IP \(bu 2
[\-\-all] Activate all OSDs found in the system
.IP \(bu 2
[\-\-no\-systemd] Skip creating and enabling systemd units and starting of OSD
services
.UNINDENT
.sp
Multiple OSDs can be activated at once by using the (idempotent) \fB\-\-all\fP flag:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm activate \-\-all
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBprepare\fP
Prepares a logical volume to be used as an OSD and journal using a \fBfilestore\fP
or \fBbluestore\fP (default) setup. It will not create or modify the logical volumes
except for adding extra metadata.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm prepare \-\-filestore \-\-data <data lv> \-\-journal <journal device>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Optional arguments:
.INDENT 0.0
.IP \(bu 2
[\-h, \-\-help]          show the help message and exit
.IP \(bu 2
[\-\-journal JOURNAL]   A logical group name, path to a logical volume, or path to a device
.IP \(bu 2
[\-\-bluestore]         Use the bluestore objectstore (default)
.IP \(bu 2
[\-\-block.wal]         Path to a bluestore block.wal logical volume or partition
.IP \(bu 2
[\-\-block.db]          Path to a bluestore block.db logical volume or partition
.IP \(bu 2
[\-\-filestore]         Use the filestore objectstore
.IP \(bu 2
[\-\-dmcrypt]           Enable encryption for the underlying OSD devices
.IP \(bu 2
[\-\-osd\-id OSD_ID]     Reuse an existing OSD id
.IP \(bu 2
[\-\-osd\-fsid OSD_FSID] Reuse an existing OSD fsid
.IP \(bu 2
[\-\-crush\-device\-class] Define a CRUSH device class to assign the OSD to
.UNINDENT
.sp
Required arguments:
.INDENT 0.0
.IP \(bu 2
.INDENT 2.0
.TP
.B \-\-data
A logical group name or a path to a logical volume
.UNINDENT
.UNINDENT
.sp
For encrypting an OSD, the \fB\-\-dmcrypt\fP flag must be added when preparing
(also supported in the \fBcreate\fP sub\-command).
.sp
\fBcreate\fP
Wraps the two\-step process to provision a new osd (calling \fBprepare\fP first
and then \fBactivate\fP) into a single one. The reason to prefer \fBprepare\fP and
then \fBactivate\fP is to gradually introduce new OSDs into a cluster, and
avoiding large amounts of data being rebalanced.
.sp
The single\-call process unifies exactly what \fBprepare\fP and \fBactivate\fP do,
with the convenience of doing it all at once. Flags and general usage are
equivalent to those of the \fBprepare\fP and \fBactivate\fP subcommand.
.sp
\fBtrigger\fP
This subcommand is not meant to be used directly, and it is used by systemd so
that it proxies input to \fBceph\-volume lvm activate\fP by parsing the
input from systemd, detecting the UUID and ID associated with an OSD.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm trigger <SYSTEMD\-DATA>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The systemd "data" is expected to be in the format of:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<OSD ID>\-<OSD UUID>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The lvs associated with the OSD need to have been prepared previously,
so that all needed tags and metadata exist.
.sp
Positional arguments:
.INDENT 0.0
.IP \(bu 2
<SYSTEMD_DATA>  Data from a systemd unit containing ID and UUID of the OSD.
.UNINDENT
.sp
\fBlist\fP
List devices or logical volumes associated with Ceph. An association is
determined if a device has information relating to an OSD. This is
verified by querying LVM\(aqs metadata and correlating it with devices.
.sp
The lvs associated with the OSD need to have been prepared previously by
ceph\-volume so that all needed tags and metadata exist.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm list
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
List a particular device, reporting all metadata about it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm list /dev/sda1
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
List a logical volume, along with all its metadata (vg is a volume
group, and lv the logical volume name):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm list {vg/lv}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Positional arguments:
.INDENT 0.0
.IP \(bu 2
<DEVICE>  Either in the form of \fBvg/lv\fP for logical volumes,
\fB/path/to/sda1\fP or \fB/path/to/sda\fP for regular devices.
.UNINDENT
.sp
\fBzap\fP
Zaps the given logical volume or partition. If given a path to a logical
volume it must be in the format of vg/lv. Any filesystems present
on the given lv or partition will be removed and all data will be purged.
.sp
However, the lv or partition will be kept intact.
.sp
Usage, for logical volumes:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm zap {vg/lv}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Usage, for logical partitions:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm zap /dev/sdc1
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For full removal of the device use the \fB\-\-destroy\fP flag (allowed for all
device types):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm zap \-\-destroy /dev/sdc1
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Multiple devices can be removed by specifying the OSD ID and/or the OSD FSID:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm zap \-\-destroy \-\-osd\-id 1
ceph\-volume lvm zap \-\-destroy \-\-osd\-id 1 \-\-osd\-fsid C9605912\-8395\-4D76\-AFC0\-7DFDAC315D59
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Positional arguments:
.INDENT 0.0
.IP \(bu 2
<DEVICE>  Either in the form of \fBvg/lv\fP for logical volumes,
\fB/path/to/sda1\fP or \fB/path/to/sda\fP for regular devices.
.UNINDENT
.SS simple
.sp
Scan legacy OSD directories or data devices that may have been created by
ceph\-disk, or manually.
.sp
Subcommands:
.sp
\fBactivate\fP
Enables a systemd unit that persists the OSD ID and its UUID (also called
\fBfsid\fP in Ceph CLI tools), so that at boot time it can understand what OSD is
enabled and needs to be mounted, while reading information that was previously
created and persisted at \fB/etc/ceph/osd/\fP in JSON format.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume simple activate \-\-bluestore <osd id> <osd fsid>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Optional Arguments:
.INDENT 0.0
.IP \(bu 2
[\-h, \-\-help]  show the help message and exit
.IP \(bu 2
[\-\-bluestore] bluestore objectstore (default)
.IP \(bu 2
[\-\-filestore] filestore objectstore
.UNINDENT
.sp
Note: It requires a matching JSON file with the following format:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
/etc/ceph/osd/<osd id>\-<osd fsid>.json
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBscan\fP
Scan a running OSD or data device for an OSD for metadata that can later be
used to activate and manage the OSD with ceph\-volume. The scan method will
create a JSON file with the required information plus anything found in the OSD
directory as well.
.sp
Optionally, the JSON blob can be sent to stdout for further inspection.
.sp
Usage on data devices:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume simple scan <data device>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Running OSD directories:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume simple scan <path to osd dir>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Optional arguments:
.INDENT 0.0
.IP \(bu 2
[\-h, \-\-help]          show the help message and exit
.IP \(bu 2
[\-\-stdout]            Send the JSON blob to stdout
.IP \(bu 2
[\-\-force]             If the JSON file exists at destination, overwrite it
.UNINDENT
.sp
Required Positional arguments:
.INDENT 0.0
.IP \(bu 2
<DATA DEVICE or OSD DIR>  Actual data partition or a path to the running OSD
.UNINDENT
.sp
\fBtrigger\fP
This subcommand is not meant to be used directly, and it is used by systemd so
that it proxies input to \fBceph\-volume simple activate\fP by parsing the
input from systemd, detecting the UUID and ID associated with an OSD.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume simple trigger <SYSTEMD\-DATA>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The systemd "data" is expected to be in the format of:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<OSD ID>\-<OSD UUID>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The JSON file associated with the OSD need to have been persisted previously by
a scan (or manually), so that all needed metadata can be used.
.sp
Positional arguments:
.INDENT 0.0
.IP \(bu 2
<SYSTEMD_DATA>  Data from a systemd unit containing ID and UUID of the OSD.
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-volume\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the documentation at \fI\%http://docs.ceph.com/\fP for more information.
.SH SEE ALSO
.sp
ceph\-osd(8),
.SH SYNOPSIS
.nf
\fBceph\-volume\-systemd\fP \fIsystemd instance name\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-volume\-systemd\fP is a systemd helper tool that receives input
from (dynamically created) systemd units so that activation of OSDs can
proceed.
.sp
It translates the input into a system call to ceph\-volume for activation
purposes only.
.SH EXAMPLES
.sp
Its input is the \fBsystemd instance name\fP (represented by \fB%i\fP in a systemd
unit), and it should be in the following format:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
<ceph\-volume subcommand>\-<extra metadata>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
In the case of \fBlvm\fP a call could look like:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
/usr/bin/ceph\-volume\-systemd lvm\-0\-8715BEB4\-15C5\-49DE\-BA6F\-401086EC7B41
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Which in turn will call \fBceph\-volume\fP in the following way:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-volume lvm trigger  0\-8715BEB4\-15C5\-49DE\-BA6F\-401086EC7B41
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Any other subcommand will need to have implemented a \fBtrigger\fP command that
can consume the extra metadata in this format.
.SH AVAILABILITY
.sp
\fBceph\-volume\-systemd\fP is part of Ceph, a massively scalable,
open\-source, distributed storage system. Please refer to the documentation at
\fI\%http://docs.ceph.com/\fP for more information.
.SH SEE ALSO
.sp
ceph\-osd(8),
.SH SYNOPSIS
.nf
\fBceph\-fuse\fP [\-n \fIclient.username\fP] [ \-m \fImonaddr\fP:\fIport\fP ] \fImountpoint\fP [ \fIfuse options\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-fuse\fP is a FUSE (File system in USErspace) client for Ceph
distributed file system. It will mount a ceph file system specified
via the \-m option or described by ceph.conf (see below) at the
specific mount point. See \fI\%Mount CephFS using FUSE\fP for detailed
information.
.sp
The file system can be unmounted with:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
fusermount \-u mountpoint
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
or by sending \fBSIGINT\fP to the \fBceph\-fuse\fP process.
.SH OPTIONS
.sp
Any options not recognized by ceph\-fuse will be passed on to libfuse.
.INDENT 0.0
.TP
.B \-o opt,[opt...]
Mount options.
.UNINDENT
.INDENT 0.0
.TP
.B \-d
Run in foreground, send all log output to stderr and enable FUSE debugging (\-o debug).
.UNINDENT
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fIceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through ceph.conf).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-client_mountpoint/\-r root_directory
Use root_directory as the mounted root, rather than the full Ceph tree.
.UNINDENT
.INDENT 0.0
.TP
.B \-f
Foreground: do not daemonize after startup (run in foreground). Do not generate a pid file.
.UNINDENT
.INDENT 0.0
.TP
.B \-s
Disable multi\-threaded operation.
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-fuse\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
fusermount(8),
ceph(8)
.SH SYNOPSIS
.nf
\fBceph\-mds\fP \-i <\fIID\fP> [flags] [ \-\-hot\-standby <\fIrank\fP> ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-mds\fP is the metadata server daemon for the Ceph distributed file
system. One or more instances of ceph\-mds collectively manage the file
system namespace, coordinating access to the shared OSD cluster.
.sp
Each ceph\-mds daemon instance should have a unique name. The name is used
to identify daemon instances in the ceph.conf.
.sp
Once the daemon has started, the monitor cluster will normally assign
it a logical rank, or put it in a standby pool to take over for
another daemon that crashes. Some of the specified options can cause
other behaviors.
.sp
If you specify \-\-hot\-standby, you must specify the rank on the command
line. Alternatively, you can specify one of the mds_standby_for_[rank|name]
parameters in the config.  The command line specification overrides the config,
and specifying the rank overrides specifying the name.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-f, \-\-foreground
Foreground: do not daemonize after startup (run in foreground). Do
not generate a pid file. Useful when run via ceph\-run(8).
.UNINDENT
.INDENT 0.0
.TP
.B \-d
Debug mode: like \fB\-f\fP, but also send all log output to stderr.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-setuser userorgid
Set uid after starting.  If a username is specified, the user
record is looked up to get a uid and a gid, and the gid is also set
as well, unless \-\-setgroup is also specified.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-setgroup grouporgid
Set gid after starting.  If a group name is specified the group
record is looked up to get a gid.
.UNINDENT
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fIceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during
startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through
\fBceph.conf\fP).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-hot\-standby <rank>
Start as a hot standby for MDS <rank>.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-id/\-i ID
Set ID portion of the MDS name.
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-mds\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to the Ceph documentation at
\fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph(8),
ceph\-mon(8),
ceph\-osd(8)
.SH SYNOPSIS
.nf
\fBceph\-mon\fP \-i \fImonid\fP [ \-\-mon\-data \fImondatapath\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-mon\fP is the cluster monitor daemon for the Ceph distributed
file system. One or more instances of \fBceph\-mon\fP form a Paxos
part\-time parliament cluster that provides extremely reliable and
durable storage of cluster membership, configuration, and state.
.sp
The \fImondatapath\fP refers to a directory on a local file system storing
monitor data. It is normally specified via the \fBmon data\fP option in
the configuration file.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-f, \-\-foreground
Foreground: do not daemonize after startup (run in foreground). Do
not generate a pid file. Useful when run via ceph\-run(8).
.UNINDENT
.INDENT 0.0
.TP
.B \-d
Debug mode: like \fB\-f\fP, but also send all log output to stderr.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-setuser userorgid
Set uid after starting.  If a username is specified, the user
record is looked up to get a uid and a gid, and the gid is also set
as well, unless \-\-setgroup is also specified.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-setgroup grouporgid
Set gid after starting.  If a group name is specified the group
record is looked up to get a gid.
.UNINDENT
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fIceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during
startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-mkfs
Initialize the \fBmon data\fP directory with seed information to form
and initial ceph file system or to join an existing monitor
cluster.  Three pieces of information must be provided:
.INDENT 7.0
.IP \(bu 2
The cluster fsid.  This can come from a monmap (\fB\-\-monmap <path>\fP) or
explicitly via \fB\-\-fsid <uuid>\fP\&.
.IP \(bu 2
A list of monitors and their addresses.  This list of monitors
can come from a monmap (\fB\-\-monmap <path>\fP), the \fBmon host\fP
configuration value (in \fIceph.conf\fP or via \fB\-m
host1,host2,...\fP), or (for backward compatibility) the deprecated \fBmon addr\fP lines in \fIceph.conf\fP\&.  If this
monitor is to be part of the initial monitor quorum for a new
Ceph cluster, then it must be included in the initial list,
matching either the name or address of a monitor in the list.
When matching by address, either the \fBpublic addr\fP or \fBpublic
subnet\fP options may be used.
.IP \(bu 2
The monitor secret key \fBmon.\fP\&.  This must be included in the
keyring provided via \fB\-\-keyring <path>\fP\&.
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \-\-keyring
Specify a keyring for use with \fB\-\-mkfs\fP\&.
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-mon\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer
to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8),
ceph\-mds(8),
ceph\-osd(8)
.SH SYNOPSIS
.nf
\fBceph\-osd\fP \-i \fIosdnum\fP [ \-\-osd\-data \fIdatapath\fP ] [ \-\-osd\-journal
\fIjournal\fP ] [ \-\-mkfs ] [ \-\-mkjournal ] [\-\-flush\-journal] [\-\-check\-allows\-journal] [\-\-check\-wants\-journal] [\-\-check\-needs\-journal] [ \-\-mkkey ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-osd\fP is the object storage daemon for the Ceph distributed file
system. It is responsible for storing objects on a local file system
and providing access to them over the network.
.sp
The datapath argument should be a directory on a xfs file system
where the object data resides. The journal is optional, and is only
useful performance\-wise when it resides on a different disk than
datapath with low latency (ideally, an NVRAM device).
.SH OPTIONS
.INDENT 0.0
.TP
.B \-f, \-\-foreground
Foreground: do not daemonize after startup (run in foreground). Do
not generate a pid file. Useful when run via ceph\-run(8).
.UNINDENT
.INDENT 0.0
.TP
.B \-d
Debug mode: like \fB\-f\fP, but also send all log output to stderr.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-setuser userorgid
Set uid after starting.  If a username is specified, the user
record is looked up to get a uid and a gid, and the gid is also set
as well, unless \-\-setgroup is also specified.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-setgroup grouporgid
Set gid after starting.  If a group name is specified the group
record is looked up to get a gid.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-osd\-data osddata
Use object store at \fIosddata\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-osd\-journal journal
Journal updates to \fIjournal\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-check\-wants\-journal
Check whether a journal is desired.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-check\-allows\-journal
Check whether a journal is allowed.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-check\-needs\-journal
Check whether a journal is required.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-mkfs
Create an empty object repository. This also initializes the journal
(if one is defined).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-mkkey
Generate a new secret key. This is normally used in combination
with \fB\-\-mkfs\fP as it is more convenient than generating a key by
hand with ceph\-authtool(8).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-mkjournal
Create a new journal file to match an existing object repository.
This is useful if the journal device or file is wiped out due to a
disk or file system failure.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-flush\-journal
Flush the journal to permanent store. This runs in the foreground
so you know when it\(aqs completed. This can be useful if you want to
resize the journal or need to otherwise destroy it: this guarantees
you won\(aqt lose data.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-get\-cluster\-fsid
Print the cluster fsid (uuid) and exit.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-get\-osd\-fsid
Print the OSD\(aqs fsid and exit.  The OSD\(aqs uuid is generated at
\-\-mkfs time and is thus unique to a particular instantiation of
this OSD.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-get\-journal\-fsid
Print the journal\(aqs uuid.  The journal fsid is set to match the OSD
fsid at \-\-mkfs time.
.UNINDENT
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fIceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP for runtime configuration options.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through
\fBceph.conf\fP).
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-osd\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph(8),
ceph\-mds(8),
ceph\-mon(8),
ceph\-authtool(8)
.SH SYNOPSIS
.nf
\fBceph\-post\-file\fP [\-d \fIdescription] [\-u *user\fP] \fIfile or dir\fP ...
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-post\-file\fP will upload files or directories to ceph.com for
later analysis by Ceph developers.
.sp
Each invocation uploads files or directories to a separate directory
with a unique tag.  That tag can be passed to a developer or
referenced in a bug report (\fI\%http://tracker.ceph.com/\fP).  Once the
upload completes, the directory is marked non\-readable and
non\-writeable to prevent access or modification by other users.
.SH WARNING
.sp
Basic measures are taken to make posted data be visible only to
developers with access to ceph.com infrastructure. However, users
should think twice and/or take appropriate precautions before
posting potentially sensitive data (for example, logs or data
directories that contain Ceph secrets).
.SH OPTIONS
.INDENT 0.0
.TP
.B \-d *description*, \-\-description *description*
Add a short description for the upload.  This is a good opportunity
to reference a bug number.  There is no default value.
.UNINDENT
.INDENT 0.0
.TP
.B \-u *user*
Set the user metadata for the upload.  This defaults to \fIwhoami\(ga@\(gahostname \-f\fP\&.
.UNINDENT
.SH EXAMPLES
.sp
To upload a single log:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-post\-file /var/log/ceph/ceph\-mon.\(gahostname\(ga.log
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To upload several directories:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-post\-file \-d \(aqmon data directories\(aq /var/log/ceph/mon/*
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-post\-file\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph(8),
ceph\-debugpack(8),
.SH SYNOPSIS
.nf
\fBceph\-rbdnamer\fP \fInum\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-rbdnamer\fP prints the pool and image name for the given RBD devices
to stdout. It is used by \fIudev\fP (using a rule like the one below) to
set up a device symlink.
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
KERNEL=="rbd[0\-9]*", PROGRAM="/usr/bin/ceph\-rbdnamer %n", SYMLINK+="rbd/%c{1}/%c{2}"
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-rbdnamer\fP is part of Ceph, a massively scalable, open\-source, distributed storage system.  Please
refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
rbd(8),
ceph(8)
.SH SYNOPSIS
.nf
\fBceph\-run\fP \fIcommand\fP ...
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-run\fP is a simple wrapper that will restart a daemon if it exits
with a signal indicating it crashed and possibly core dumped (that is,
signals 3, 4, 5, 6, 8, or 11).
.sp
The command should run the daemon in the foreground. For Ceph daemons,
that means the \fB\-f\fP option.
.SH OPTIONS
.sp
None
.SH AVAILABILITY
.sp
\fBceph\-run\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph(8),
ceph\-mon(8),
ceph\-mds(8),
ceph\-osd(8)
.SH SYNOPSIS
.nf
\fBceph\-syn\fP [ \-m \fImonaddr\fP:\fIport\fP ] \-\-syn \fIcommand\fP \fI\&...\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\-syn\fP is a simple synthetic workload generator for the Ceph
distributed file system. It uses the userspace client library to
generate simple workloads against a currently running file system. The
file system need not be mounted via ceph\-fuse(8) or the kernel client.
.sp
One or more \fB\-\-syn\fP command arguments specify the particular
workload, as documented below.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-d
Detach from console and daemonize after startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fIceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during
startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through
\fBceph.conf\fP).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-num_client num
Run num different clients, each in a separate thread.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-syn workloadspec
Run the given workload. May be specified as many times as
needed. Workloads will normally run sequentially.
.UNINDENT
.SH WORKLOADS
.sp
Each workload should be preceded by \fB\-\-syn\fP on the command
line. This is not a complete list.
.INDENT 0.0
.TP
\fBmknap\fP \fIpath\fP \fIsnapname\fP
Create a snapshot called \fIsnapname\fP on \fIpath\fP\&.
.TP
\fBrmsnap\fP \fIpath\fP \fIsnapname\fP
Delete snapshot called \fIsnapname\fP on \fIpath\fP\&.
.TP
\fBrmfile\fP \fIpath\fP
Delete/unlink \fIpath\fP\&.
.TP
\fBwritefile\fP \fIsizeinmb\fP \fIblocksize\fP
Create a file, named after our client id, that is \fIsizeinmb\fP MB by
writing \fIblocksize\fP chunks.
.TP
\fBreadfile\fP \fIsizeinmb\fP \fIblocksize\fP
Read file, named after our client id, that is \fIsizeinmb\fP MB by
writing \fIblocksize\fP chunks.
.TP
\fBrw\fP \fIsizeinmb\fP \fIblocksize\fP
Write file, then read it back, as above.
.TP
\fBmakedirs\fP \fInumsubdirs\fP \fInumfiles\fP \fIdepth\fP
Create a hierarchy of directories that is \fIdepth\fP levels deep. Give
each directory \fInumsubdirs\fP subdirectories and \fInumfiles\fP files.
.TP
\fBwalk\fP
Recursively walk the file system (like find).
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\-syn\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph(8),
ceph\-fuse(8)
.SH SYNOPSIS
.nf
\fBceph\fP \fBauth\fP [ \fIadd\fP | \fIcaps\fP | \fIdel\fP | \fIexport\fP | \fIget\fP | \fIget\-key\fP | \fIget\-or\-create\fP | \fIget\-or\-create\-key\fP | \fIimport\fP | \fIlist\fP | \fIprint\-key\fP | \fIprint_key\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBcompact\fP
.fi
.sp
.nf
\fBceph\fP \fBconfig\-key\fP [ \fIdel\fP | \fIexists\fP | \fIget\fP | \fIlist\fP | \fIdump\fP | \fIput\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBdaemon\fP \fI<name>\fP | \fI<path>\fP \fI<command>\fP ...
.fi
.sp
.nf
\fBceph\fP \fBdaemonperf\fP \fI<name>\fP | \fI<path>\fP [ \fIinterval\fP [ \fIcount\fP ] ]
.fi
.sp
.nf
\fBceph\fP \fBdf\fP \fI{detail}\fP
.fi
.sp
.nf
\fBceph\fP \fBfs\fP [ \fIls\fP | \fInew\fP | \fIreset\fP | \fIrm\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBfsid\fP
.fi
.sp
.nf
\fBceph\fP \fBhealth\fP \fI{detail}\fP
.fi
.sp
.nf
\fBceph\fP \fBheap\fP [ \fIdump\fP | \fIstart_profiler\fP | \fIstop_profiler\fP | \fIrelease\fP | \fIstats\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBinjectargs\fP \fI<injectedargs>\fP [ \fI<injectedargs>\fP\&... ]
.fi
.sp
.nf
\fBceph\fP \fBlog\fP \fI<logtext>\fP [ \fI<logtext>\fP\&... ]
.fi
.sp
.nf
\fBceph\fP \fBmds\fP [ \fIcompat\fP | \fIfail\fP | \fIrm\fP | \fIrmfailed\fP | \fIset_state\fP | \fIstat\fP | \fIrepaired\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBmon\fP [ \fIadd\fP | \fIdump\fP | \fIgetmap\fP | \fIremove\fP | \fIstat\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBmon_status\fP
.fi
.sp
.nf
\fBceph\fP \fBosd\fP [ \fIblacklist\fP | \fIblocked\-by\fP | \fIcreate\fP | \fInew\fP | \fIdeep\-scrub\fP | \fIdf\fP | \fIdown\fP | \fIdump\fP | \fIerasure\-code\-profile\fP | \fIfind\fP | \fIgetcrushmap\fP | \fIgetmap\fP | \fIgetmaxosd\fP | \fIin\fP | \fIls\fP | \fIlspools\fP | \fImap\fP | \fImetadata\fP | \fIok\-to\-stop\fP | \fIout\fP | \fIpause\fP | \fIperf\fP | \fIpg\-temp\fP | \fIforce\-create\-pg\fP | \fIprimary\-affinity\fP | \fIprimary\-temp\fP | \fIrepair\fP | \fIreweight\fP | \fIreweight\-by\-pg\fP | \fIrm\fP | \fIdestroy\fP | \fIpurge\fP | \fIsafe\-to\-destroy\fP | \fIscrub\fP | \fIset\fP | \fIsetcrushmap\fP | \fIsetmaxosd\fP  | \fIstat\fP | \fItree\fP | \fIunpause\fP | \fIunset\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBosd\fP \fBcrush\fP [ \fIadd\fP | \fIadd\-bucket\fP | \fIcreate\-or\-move\fP | \fIdump\fP | \fIget\-tunable\fP | \fIlink\fP | \fImove\fP | \fIremove\fP | \fIrename\-bucket\fP | \fIreweight\fP | \fIreweight\-all\fP | \fIreweight\-subtree\fP | \fIrm\fP | \fIrule\fP | \fIset\fP | \fIset\-tunable\fP | \fIshow\-tunables\fP | \fItunables\fP | \fIunlink\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBosd\fP \fBpool\fP [ \fIcreate\fP | \fIdelete\fP | \fIget\fP | \fIget\-quota\fP | \fIls\fP | \fImksnap\fP | \fIrename\fP | \fIrmsnap\fP | \fIset\fP | \fIset\-quota\fP | \fIstats\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBosd\fP \fBpool\fP \fBapplication\fP [ \fIdisable\fP | \fIenable\fP | \fIget\fP | \fIrm\fP | \fIset\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBosd\fP \fBtier\fP [ \fIadd\fP | \fIadd\-cache\fP | \fIcache\-mode\fP | \fIremove\fP | \fIremove\-overlay\fP | \fIset\-overlay\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBpg\fP [ \fIdebug\fP | \fIdeep\-scrub\fP | \fIdump\fP | \fIdump_json\fP | \fIdump_pools_json\fP | \fIdump_stuck\fP | \fIgetmap\fP | \fIls\fP | \fIls\-by\-osd\fP | \fIls\-by\-pool\fP | \fIls\-by\-primary\fP | \fImap\fP | \fIrepair\fP | \fIscrub\fP | \fIstat\fP ] ...
.fi
.sp
.nf
\fBceph\fP \fBquorum\fP [ \fIenter\fP | \fIexit\fP ]
.fi
.sp
.nf
\fBceph\fP \fBquorum_status\fP
.fi
.sp
.nf
\fBceph\fP \fBreport\fP { \fI<tags>\fP [ \fI<tags>...\fP ] }
.fi
.sp
.nf
\fBceph\fP \fBscrub\fP
.fi
.sp
.nf
\fBceph\fP \fBstatus\fP
.fi
.sp
.nf
\fBceph\fP \fBsync\fP \fBforce\fP {\-\-yes\-i\-really\-mean\-it} {\-\-i\-know\-what\-i\-am\-doing}
.fi
.sp
.nf
\fBceph\fP \fBtell\fP \fI<name (type.id)> <command> [options...]\fP
.fi
.sp
.nf
\fBceph\fP \fBversion\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBceph\fP is a control utility which is used for manual deployment and maintenance
of a Ceph cluster. It provides a diverse set of commands that allows deployment of
monitors, OSDs, placement groups, MDS and overall maintenance, administration
of the cluster.
.SH COMMANDS
.SS auth
.sp
Manage authentication keys. It is used for adding, removing, exporting
or updating of authentication keys for a particular  entity such as a monitor or
OSD. It uses some additional subcommands.
.sp
Subcommand \fBadd\fP adds authentication info for a particular entity from input
file, or random key if no input is given and/or any caps specified in the command.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth add <entity> {<caps> [<caps>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBcaps\fP updates caps for \fBname\fP from caps specified in the command.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth caps <entity> <caps> [<caps>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdel\fP deletes all caps for \fBname\fP\&.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth del <entity>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBexport\fP writes keyring for requested entity, or master keyring if
none given.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth export {<entity>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\fP writes keyring file with requested key.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth get <entity>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\-key\fP displays requested key.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth get\-key <entity>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\-or\-create\fP adds authentication info for a particular entity
from input file, or random key if no input given and/or any caps specified in the
command.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth get\-or\-create <entity> {<caps> [<caps>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\-or\-create\-key\fP gets or adds key for \fBname\fP from system/caps
pairs specified in the command.  If key already exists, any given caps must match
the existing caps for that key.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth get\-or\-create\-key <entity> {<caps> [<caps>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBimport\fP reads keyring from input file.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth import
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\fP lists authentication state.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth ls
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBprint\-key\fP displays requested key.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth print\-key <entity>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBprint_key\fP displays requested key.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth print_key <entity>
.ft P
.fi
.UNINDENT
.UNINDENT
.SS compact
.sp
Causes compaction of monitor\(aqs leveldb storage.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph compact
.ft P
.fi
.UNINDENT
.UNINDENT
.SS config\-key
.sp
Manage configuration key. It uses some additional subcommands.
.sp
Subcommand \fBdel\fP deletes configuration key.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph config\-key del <key>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBexists\fP checks for configuration keys existence.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph config\-key exists <key>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\fP gets the configuration key.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph config\-key get <key>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBlist\fP lists configuration keys.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph config\-key ls
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump\fP dumps configuration keys and values.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph config\-key dump
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\fP puts configuration key and value.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph config\-key set <key> {<val>}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS daemon
.sp
Submit admin\-socket commands.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph daemon {daemon_name|socket_path} {command} ...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph daemon osd.0 help
.ft P
.fi
.UNINDENT
.UNINDENT
.SS daemonperf
.sp
Watch performance counters from a Ceph daemon.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph daemonperf {daemon_name|socket_path} [{interval} [{count}]]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS df
.sp
Show cluster\(aqs free space status.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph df {detail}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS features
.sp
Show the releases and features of all connected daemons and clients connected
to the cluster, along with the numbers of them in each bucket grouped by the
corresponding features/releases. Each release of Ceph supports a different set
of features, expressed by the features bitmask. New cluster features require
that clients support the feature, or else they are not allowed to connect to
these new features. As new features or capabilities are enabled after an
upgrade, older clients are prevented from connecting.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph features
.ft P
.fi
.UNINDENT
.UNINDENT
.SS fs
.sp
Manage cephfs filesystems. It uses some additional subcommands.
.sp
Subcommand \fBls\fP to list filesystems
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph fs ls
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBnew\fP to make a new filesystem using named pools <metadata> and <data>
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph fs new <fs_name> <metadata> <data>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBreset\fP is used for disaster recovery only: reset to a single\-MDS map
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph fs reset <fs_name> {\-\-yes\-i\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm\fP to disable the named filesystem
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph fs rm <fs_name> {\-\-yes\-i\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS fsid
.sp
Show cluster\(aqs FSID/UUID.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph fsid
.ft P
.fi
.UNINDENT
.UNINDENT
.SS health
.sp
Show cluster\(aqs health.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph health {detail}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS heap
.sp
Show heap usage info (available only if compiled with tcmalloc)
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph heap dump|start_profiler|stop_profiler|release|stats
.ft P
.fi
.UNINDENT
.UNINDENT
.SS injectargs
.sp
Inject configuration arguments into monitor.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph injectargs <injected_args> [<injected_args>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS log
.sp
Log supplied text to the monitor log.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph log <logtext> [<logtext>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.SS mds
.sp
Manage metadata server configuration and administration. It uses some
additional subcommands.
.sp
Subcommand \fBcompat\fP manages compatible features. It uses some additional
subcommands.
.sp
Subcommand \fBrm_compat\fP removes compatible feature.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds compat rm_compat <int[0\-]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm_incompat\fP removes incompatible feature.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds compat rm_incompat <int[0\-]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBshow\fP shows mds compatibility settings.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds compat show
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBfail\fP forces mds to status fail.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds fail <role|gid>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm\fP removes inactive mds.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds rm <int[0\-]> <name> (type.id)>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrmfailed\fP removes failed mds.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds rmfailed <int[0\-]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset_state\fP sets mds state of <gid> to <numeric\-state>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds set_state <int[0\-]> <int[0\-20]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBstat\fP shows MDS status.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds stat
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrepaired\fP mark a damaged MDS rank as no longer damaged.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mds repaired <role>
.ft P
.fi
.UNINDENT
.UNINDENT
.SS mon
.sp
Manage monitor configuration and administration. It uses some additional
subcommands.
.sp
Subcommand \fBadd\fP adds new monitor named <name> at <addr>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mon add <name> <IPaddr[:port]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump\fP dumps formatted monmap (optionally from epoch)
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mon dump {<int[0\-]>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBgetmap\fP gets monmap.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mon getmap {<int[0\-]>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBremove\fP removes monitor named <name>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mon remove <name>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBstat\fP summarizes monitor status.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mon stat
.ft P
.fi
.UNINDENT
.UNINDENT
.SS mon_status
.sp
Reports status of monitors.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mon_status
.ft P
.fi
.UNINDENT
.UNINDENT
.SS mgr
.sp
Ceph manager daemon configuration and management.
.sp
Subcommand \fBdump\fP dumps the latest MgrMap, which describes the active
and standby manager daemons.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mgr dump
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBfail\fP will mark a manager daemon as failed, removing it
from the manager map.  If it is the active manager daemon a standby
will take its place.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mgr fail <name>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmodule ls\fP will list currently enabled manager modules (plugins).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mgr module ls
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmodule enable\fP will enable a manager module.  Available modules are included in MgrMap and visible via \fBmgr dump\fP\&.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mgr module enable <module>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmodule disable\fP will disable an active manager module.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mgr module disable <module>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmetadata\fP will report metadata about all manager daemons or, if the name is specified, a single manager daemon.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mgr metadata [name]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBversions\fP will report a count of running daemon versions.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mgr versions
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBcount\-metadata\fP will report a count of any daemon metadata field.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph mgr count\-metadata <field>
.ft P
.fi
.UNINDENT
.UNINDENT
.SS osd
.sp
Manage OSD configuration and administration. It uses some additional
subcommands.
.sp
Subcommand \fBblacklist\fP manage blacklisted clients. It uses some additional
subcommands.
.sp
Subcommand \fBadd\fP add <addr> to blacklist (optionally until <expire> seconds
from now)
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd blacklist add <EntityAddr> {<float[0.0\-]>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\fP show blacklisted clients
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd blacklist ls
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm\fP remove <addr> from blacklist
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd blacklist rm <EntityAddr>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBblocked\-by\fP prints a histogram of which OSDs are blocking their peers
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd blocked\-by
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBcreate\fP creates new osd (with optional UUID and ID).
.sp
This command is DEPRECATED as of the Luminous release, and will be removed in
a future release.
.sp
Subcommand \fBnew\fP should instead be used.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd create {<uuid>} {<id>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBnew\fP can be used to create a new OSD or to recreate a previously
destroyed OSD with a specific \fIid\fP\&. The new OSD will have the specified \fIuuid\fP,
and the command expects a JSON file containing the base64 cephx key for auth
entity \fIclient.osd.<id>\fP, as well as optional base64 cepx key for dm\-crypt
lockbox access and a dm\-crypt key. Specifying a dm\-crypt requires specifying
the accompanying lockbox cephx key.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd new {<uuid>} {<id>} \-i {<params.json>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The parameters JSON file is optional but if provided, is expected to maintain
a form of the following format:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    "cephx_secret": "AQBWtwhZdBO5ExAAIDyjK2Bh16ZXylmzgYYEjg==",
    "crush_device_class": "myclass"
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Or:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    "cephx_secret": "AQBWtwhZdBO5ExAAIDyjK2Bh16ZXylmzgYYEjg==",
    "cephx_lockbox_secret": "AQDNCglZuaeVCRAAYr76PzR1Anh7A0jswkODIQ==",
    "dmcrypt_key": "<dm\-crypt key>",
    "crush_device_class": "myclass"
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Or:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
{
    "crush_device_class": "myclass"
}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The "crush_device_class" property is optional. If specified, it will set the
initial CRUSH device class for the new OSD.
.sp
Subcommand \fBcrush\fP is used for CRUSH management. It uses some additional
subcommands.
.sp
Subcommand \fBadd\fP adds or updates crushmap position and weight for <name> with
<weight> and location <args>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush add <osdname (id|osd.id)> <float[0.0\-]> <args> [<args>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBadd\-bucket\fP adds no\-parent (probably root) crush bucket <name> of
type <type>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush add\-bucket <name> <type>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBcreate\-or\-move\fP creates entry or moves existing entry for <name>
<weight> at/to location <args>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush create\-or\-move <osdname (id|osd.id)> <float[0.0\-]> <args>
[<args>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump\fP dumps crush map.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush dump
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\-tunable\fP get crush tunable straw_calc_version
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush get\-tunable straw_calc_version
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBlink\fP links existing entry for <name> under location <args>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush link <name> <args> [<args>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmove\fP moves existing entry for <name> to location <args>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush move <name> <args> [<args>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBremove\fP removes <name> from crush map (everywhere, or just at
<ancestor>).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush remove <name> {<ancestor>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrename\-bucket\fP renames bucket <srcname> to <dstname>
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush rename\-bucket <srcname> <dstname>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBreweight\fP change <name>\(aqs weight to <weight> in crush map.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush reweight <name> <float[0.0\-]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBreweight\-all\fP recalculate the weights for the tree to
ensure they sum correctly
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush reweight\-all
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBreweight\-subtree\fP changes all leaf items beneath <name>
to <weight> in crush map
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush reweight\-subtree <name> <weight>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm\fP removes <name> from crush map (everywhere, or just at
<ancestor>).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush rm <name> {<ancestor>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrule\fP is used for creating crush rules. It uses some additional
subcommands.
.sp
Subcommand \fBcreate\-erasure\fP creates crush rule <name> for erasure coded pool
created with <profile> (default default).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush rule create\-erasure <name> {<profile>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBcreate\-simple\fP creates crush rule <name> to start from <root>,
replicate across buckets of type <type>, using a choose mode of <firstn|indep>
(default firstn; indep best for erasure pools).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush rule create\-simple <name> <root> <type> {firstn|indep}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump\fP dumps crush rule <name> (default all).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush rule dump {<name>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\fP lists crush rules.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush rule ls
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm\fP removes crush rule <name>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush rule rm <name>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\fP used alone, sets crush map from input file.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush set
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\fP with osdname/osd.id update crushmap position and weight
for <name> to <weight> with location <args>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush set <osdname (id|osd.id)> <float[0.0\-]> <args> [<args>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\-tunable\fP set crush tunable <tunable> to <value>.  The only
tunable that can be set is straw_calc_version.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush set\-tunable straw_calc_version <value>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBshow\-tunables\fP shows current crush tunables.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush show\-tunables
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBtree\fP shows the crush buckets and items in a tree view.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush tree
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBtunables\fP sets crush tunables values to <profile>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush tunables legacy|argonaut|bobtail|firefly|hammer|optimal|default
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBunlink\fP unlinks <name> from crush map (everywhere, or just at
<ancestor>).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd crush unlink <name> {<ancestor>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdf\fP shows OSD utilization
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd df {plain|tree}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdeep\-scrub\fP initiates deep scrub on specified osd.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd deep\-scrub <who>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdown\fP sets osd(s) <id> [<id>...] down.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd down <ids> [<ids>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump\fP prints summary of OSD map.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd dump {<int[0\-]>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBerasure\-code\-profile\fP is used for managing the erasure code
profiles. It uses some additional subcommands.
.sp
Subcommand \fBget\fP gets erasure code profile <name>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd erasure\-code\-profile get <name>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\fP lists all erasure code profiles.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd erasure\-code\-profile ls
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm\fP removes erasure code profile <name>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd erasure\-code\-profile rm <name>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\fP creates erasure code profile <name> with [<key[=value]> ...]
pairs. Add a \-\-force at the end to override an existing profile (IT IS RISKY).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd erasure\-code\-profile set <name> {<profile> [<profile>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBfind\fP find osd <id> in the CRUSH map and shows its location.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd find <int[0\-]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBgetcrushmap\fP gets CRUSH map.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd getcrushmap {<int[0\-]>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBgetmap\fP gets OSD map.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd getmap {<int[0\-]>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBgetmaxosd\fP shows largest OSD id.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd getmaxosd
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBin\fP sets osd(s) <id> [<id>...] in.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd in <ids> [<ids>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBlost\fP marks osd as permanently lost. THIS DESTROYS DATA IF NO
MORE REPLICAS EXIST, BE CAREFUL.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd lost <int[0\-]> {\-\-yes\-i\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\fP shows all OSD ids.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd ls {<int[0\-]>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBlspools\fP lists pools.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd lspools {<int>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmap\fP finds pg for <object> in <pool>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd map <poolname> <objectname>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmetadata\fP fetches metadata for osd <id>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd metadata {int[0\-]} (default all)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBout\fP sets osd(s) <id> [<id>...] out.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd out <ids> [<ids>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBok\-to\-stop\fP checks whether the list of OSD(s) can be
stopped without immediately making data unavailable.  That is, all
data should remain readable and writeable, although data redundancy
may be reduced as some PGs may end up in a degraded (but active)
state.  It will return a success code if it is okay to stop the
OSD(s), or an error code and informative message if it is not or if no
conclusion can be drawn at the current time.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd ok\-to\-stop <id> [<ids>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBpause\fP pauses osd.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pause
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBperf\fP prints dump of OSD perf summary stats.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd perf
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBpg\-temp\fP set pg_temp mapping pgid:[<id> [<id>...]] (developers
only).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pg\-temp <pgid> {<id> [<id>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBforce\-create\-pg\fP forces creation of pg <pgid>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd force\-create\-pg <pgid>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBpool\fP is used for managing data pools. It uses some additional
subcommands.
.sp
Subcommand \fBcreate\fP creates pool.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool create <poolname> <int[0\-]> {<int[0\-]>} {replicated|erasure}
{<erasure_code_profile>} {<rule>} {<int>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdelete\fP deletes pool.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool delete <poolname> {<poolname>} {\-\-yes\-i\-really\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\fP gets pool parameter <var>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool get <poolname> size|min_size|pg_num|pgp_num|crush_rule|write_fadvise_dontneed
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Only for tiered pools:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool get <poolname> hit_set_type|hit_set_period|hit_set_count|hit_set_fpp|
target_max_objects|target_max_bytes|cache_target_dirty_ratio|cache_target_dirty_high_ratio|
cache_target_full_ratio|cache_min_flush_age|cache_min_evict_age|
min_read_recency_for_promote|hit_set_grade_decay_rate|hit_set_search_last_n
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Only for erasure coded pools:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool get <poolname> erasure_code_profile
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Use \fBall\fP to get all pool parameters that apply to the pool\(aqs type:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool get <poolname> all
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\-quota\fP obtains object or byte limits for pool.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool get\-quota <poolname>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\fP list pools
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool ls {detail}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmksnap\fP makes snapshot <snap> in <pool>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool mksnap <poolname> <snap>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrename\fP renames <srcpool> to <destpool>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool rename <poolname> <poolname>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrmsnap\fP removes snapshot <snap> from <pool>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool rmsnap <poolname> <snap>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\fP sets pool parameter <var> to <val>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool set <poolname> size|min_size|pg_num|
pgp_num|crush_rule|hashpspool|nodelete|nopgchange|nosizechange|
hit_set_type|hit_set_period|hit_set_count|hit_set_fpp|debug_fake_ec_pool|
target_max_bytes|target_max_objects|cache_target_dirty_ratio|
cache_target_dirty_high_ratio|
cache_target_full_ratio|cache_min_flush_age|cache_min_evict_age|
min_read_recency_for_promote|write_fadvise_dontneed|hit_set_grade_decay_rate|
hit_set_search_last_n
<val> {\-\-yes\-i\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\-quota\fP sets object or byte limit on pool.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool set\-quota <poolname> max_objects|max_bytes <val>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBstats\fP obtain stats from all pools, or from specified pool.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool stats {<name>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBapplication\fP is used for adding an annotation to the given
pool. By default, the possible applications are object, block, and file
storage (corresponding app\-names are "rgw", "rbd", and "cephfs"). However,
there might be other applications as well. Based on the application, there
may or may not be some processing conducted.
.sp
Subcommand \fBdisable\fP disables the given application on the given pool.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool application disable <pool\-name> <app> {\-\-yes\-i\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBenable\fP adds an annotation to the given pool for the mentioned
application.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool application enable <pool\-name> <app> {\-\-yes\-i\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBget\fP displays the value for the given key that is assosciated
with the given application of the given pool. Not passing the optional
arguments would display all key\-value pairs for all applications for all
pools.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool application get {<pool\-name>} {<app>} {<key>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm\fP removes the key\-value pair for the given key in the given
application of the given pool.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool application rm <pool\-name> <app> <key>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\fP assosciates or updates, if it already exists, a key\-value
pair with the given application for the given pool.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd pool application set <pool\-name> <app> <key> <value>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBprimary\-affinity\fP adjust osd primary\-affinity from 0.0 <=<weight>
<= 1.0
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd primary\-affinity <osdname (id|osd.id)> <float[0.0\-1.0]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBprimary\-temp\fP sets primary_temp mapping pgid:<id>|\-1 (developers
only).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd primary\-temp <pgid> <id>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrepair\fP initiates repair on a specified osd.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd repair <who>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBreweight\fP reweights osd to 0.0 < <weight> < 1.0.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
osd reweight <int[0\-]> <float[0.0\-1.0]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBreweight\-by\-pg\fP reweight OSDs by PG distribution
[overload\-percentage\-for\-consideration, default 120].
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd reweight\-by\-pg {<int[100\-]>} {<poolname> [<poolname...]}
{\-\-no\-increasing}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBreweight\-by\-utilization\fP reweight OSDs by utilization
[overload\-percentage\-for\-consideration, default 120].
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd reweight\-by\-utilization {<int[100\-]>}
{\-\-no\-increasing}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrm\fP removes osd(s) <id> [<id>...] from the OSD map.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd rm <ids> [<ids>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdestroy\fP marks OSD \fIid\fP as \fIdestroyed\fP, removing its cephx
entity\(aqs keys and all of its dm\-crypt and daemon\-private config key
entries.
.sp
This command will not remove the OSD from crush, nor will it remove the
OSD from the OSD map. Instead, once the command successfully completes,
the OSD will show marked as \fIdestroyed\fP\&.
.sp
In order to mark an OSD as destroyed, the OSD must first be marked as
\fBlost\fP\&.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd destroy <id> {\-\-yes\-i\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBpurge\fP performs a combination of \fBosd destroy\fP,
\fBosd rm\fP and \fBosd crush remove\fP\&.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd purge <id> {\-\-yes\-i\-really\-mean\-it}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBsafe\-to\-destroy\fP checks whether it is safe to remove or
destroy an OSD without reducing overall data redundancy or durability.
It will return a success code if it is definitely safe, or an error
code and informative message if it is not or if no conclusion can be
drawn at the current time.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd safe\-to\-destroy <id> [<ids>...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBscrub\fP initiates scrub on specified osd.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd scrub <who>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\fP sets <key>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd set full|pause|noup|nodown|noout|noin|nobackfill|
norebalance|norecover|noscrub|nodeep\-scrub|notieragent
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBsetcrushmap\fP sets crush map from input file.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd setcrushmap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBsetmaxosd\fP sets new maximum osd value.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd setmaxosd <int[0\-]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\-require\-min\-compat\-client\fP enforces the cluster to be backward
compatible with the specified client version. This subcommand prevents you from
making any changes (e.g., crush tunables, or using new features) that
would violate the current setting. Please note, This subcommand will fail if
any connected daemon or client is not compatible with the features offered by
the given <version>. To see the features and releases of all clients connected
to cluster, please see \fI\%ceph features\fP\&.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd set\-require\-min\-compat\-client <version>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBstat\fP prints summary of OSD map.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd stat
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBtier\fP is used for managing tiers. It uses some additional
subcommands.
.sp
Subcommand \fBadd\fP adds the tier <tierpool> (the second one) to base pool <pool>
(the first one).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd tier add <poolname> <poolname> {\-\-force\-nonempty}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBadd\-cache\fP adds a cache <tierpool> (the second one) of size <size>
to existing pool <pool> (the first one).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd tier add\-cache <poolname> <poolname> <int[0\-]>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBcache\-mode\fP specifies the caching mode for cache tier <pool>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd tier cache\-mode <poolname> none|writeback|forward|readonly|
readforward|readproxy
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBremove\fP removes the tier <tierpool> (the second one) from base pool
<pool> (the first one).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd tier remove <poolname> <poolname>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBremove\-overlay\fP removes the overlay pool for base pool <pool>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd tier remove\-overlay <poolname>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBset\-overlay\fP set the overlay pool for base pool <pool> to be
<overlaypool>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd tier set\-overlay <poolname> <poolname>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBtree\fP prints OSD tree.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd tree {<int[0\-]>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBunpause\fP unpauses osd.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd unpause
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBunset\fP unsets <key>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph osd unset full|pause|noup|nodown|noout|noin|nobackfill|
norebalance|norecover|noscrub|nodeep\-scrub|notieragent
.ft P
.fi
.UNINDENT
.UNINDENT
.SS pg
.sp
It is used for managing the placement groups in OSDs. It uses some
additional subcommands.
.sp
Subcommand \fBdebug\fP shows debug info about pgs.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg debug unfound_objects_exist|degraded_pgs_exist
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdeep\-scrub\fP starts deep\-scrub on <pgid>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg deep\-scrub <pgid>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump\fP shows human\-readable versions of pg map (only \(aqall\(aq valid
with plain).
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg dump {all|summary|sum|delta|pools|osds|pgs|pgs_brief} [{all|summary|sum|delta|pools|osds|pgs|pgs_brief...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump_json\fP shows human\-readable version of pg map in json only.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg dump_json {all|summary|sum|delta|pools|osds|pgs|pgs_brief} [{all|summary|sum|delta|pools|osds|pgs|pgs_brief...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump_pools_json\fP shows pg pools info in json only.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg dump_pools_json
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBdump_stuck\fP shows information about stuck pgs.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg dump_stuck {inactive|unclean|stale|undersized|degraded [inactive|unclean|stale|undersized|degraded...]}
{<int>}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBgetmap\fP gets binary pg map to \-o/stdout.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg getmap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\fP lists pg with specific pool, osd, state
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg ls {<int>} {<pg\-state> [<pg\-state>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\-by\-osd\fP lists pg on osd [osd]
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg ls\-by\-osd <osdname (id|osd.id)> {<int>}
{<pg\-state> [<pg\-state>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\-by\-pool\fP lists pg with pool = [poolname]
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg ls\-by\-pool <poolstr> {<int>} {<pg\-state> [<pg\-state>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBls\-by\-primary\fP lists pg with primary = [osd]
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg ls\-by\-primary <osdname (id|osd.id)> {<int>}
{<pg\-state> [<pg\-state>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBmap\fP shows mapping of pg to osds.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg map <pgid>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBrepair\fP starts repair on <pgid>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg repair <pgid>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBscrub\fP starts scrub on <pgid>.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg scrub <pgid>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Subcommand \fBstat\fP shows placement group status.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph pg stat
.ft P
.fi
.UNINDENT
.UNINDENT
.SS quorum
.sp
Cause MON to enter or exit quorum.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph quorum enter|exit
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Note: this only works on the MON to which the \fBceph\fP command is connected.
If you want a specific MON to enter or exit quorum, use this syntax:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph tell mon.<id> quorum enter|exit
.ft P
.fi
.UNINDENT
.UNINDENT
.SS quorum_status
.sp
Reports status of monitor quorum.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph quorum_status
.ft P
.fi
.UNINDENT
.UNINDENT
.SS report
.sp
Reports full status of cluster, optional title tag strings.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph report {<tags> [<tags>...]}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS scrub
.sp
Scrubs the monitor stores.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph scrub
.ft P
.fi
.UNINDENT
.UNINDENT
.SS status
.sp
Shows cluster status.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph status
.ft P
.fi
.UNINDENT
.UNINDENT
.SS sync force
.sp
Forces sync of and clear monitor store.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph sync force {\-\-yes\-i\-really\-mean\-it} {\-\-i\-know\-what\-i\-am\-doing}
.ft P
.fi
.UNINDENT
.UNINDENT
.SS tell
.sp
Sends a command to a specific daemon.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph tell <name (type.id)> <command> [options...]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
List all available commands.
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph tell <name (type.id)> help
.ft P
.fi
.UNINDENT
.UNINDENT
.SS version
.sp
Show mon daemon version
.sp
Usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ceph version
.ft P
.fi
.UNINDENT
.UNINDENT
.SH OPTIONS
.INDENT 0.0
.TP
.B \-i infile
will specify an input file to be passed along as a payload with the
command to the monitor cluster. This is only used for specific
monitor commands.
.UNINDENT
.INDENT 0.0
.TP
.B \-o outfile
will write any payload returned by the monitor cluster with its
reply to outfile.  Only specific monitor commands (e.g. osd getmap)
return a payload.
.UNINDENT
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use ceph.conf configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-id CLIENT_ID, \-\-user CLIENT_ID
Client id for authentication.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-name CLIENT_NAME, \-n CLIENT_NAME
Client name for authentication.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-cluster CLUSTER
Name of the Ceph cluster.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-admin\-daemon ADMIN_SOCKET, daemon DAEMON_NAME
Submit admin\-socket commands via admin sockets in /var/run/ceph.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-admin\-socket ADMIN_SOCKET_NOPE
You probably mean \-\-admin\-daemon
.UNINDENT
.INDENT 0.0
.TP
.B \-s, \-\-status
Show cluster status.
.UNINDENT
.INDENT 0.0
.TP
.B \-w, \-\-watch
Watch live cluster changes.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-watch\-debug
Watch debug events.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-watch\-info
Watch info events.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-watch\-sec
Watch security events.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-watch\-warn
Watch warning events.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-watch\-error
Watch error events.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-version, \-v
Display version.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-verbose
Make verbose.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-concise
Make less verbose.
.UNINDENT
.INDENT 0.0
.TP
.B \-f {json,json\-pretty,xml,xml\-pretty,plain}, \-\-format
Format of output.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-connect\-timeout CLUSTER_TIMEOUT
Set a timeout for connecting to the cluster.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-no\-increasing
\fB\-\-no\-increasing\fP is off by default. So increasing the osd weight is allowed
using the \fBreweight\-by\-utilization\fP or \fBtest\-reweight\-by\-utilization\fP commands.
If this option is used with these commands, it will help not to increase osd weight
even the osd is under utilized.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-block
block until completion (scrub and deep\-scrub only)
.UNINDENT
.SH AVAILABILITY
.sp
\fBceph\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph\-mon(8),
ceph\-osd(8),
ceph\-mds(8)
.SH SYNOPSIS
.nf
\fBcrushtool\fP ( \-d \fImap\fP | \-c \fImap.txt\fP | \-\-build \-\-num_osds \fInumosds\fP
\fIlayer1\fP \fI\&...\fP | \-\-test ) [ \-o \fIoutfile\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBcrushtool\fP is a utility that lets you create, compile, decompile
and test CRUSH map files.
.sp
CRUSH is a pseudo\-random data distribution algorithm that efficiently
maps input values (which, in the context of Ceph, correspond to Placement
Groups) across a heterogeneous, hierarchically structured device map.
The algorithm was originally described in detail in the following paper
(although it has evolved some since then):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
http://www.ssrc.ucsc.edu/Papers/weil\-sc06.pdf
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The tool has four modes of operation.
.INDENT 0.0
.TP
.B \-\-compile|\-c map.txt
will compile a plaintext map.txt into a binary map file.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-decompile|\-d map
will take the compiled map and decompile it into a plaintext source
file, suitable for editing.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-build \-\-num_osds {num\-osds} layer1 ...
will create map with the given layer structure. See below for a
detailed explanation.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-test
will perform a dry run of a CRUSH mapping for a range of input
values \fB[\-\-min\-x,\-\-max\-x]\fP (default \fB[0,1023]\fP) which can be
thought of as simulated Placement Groups. See below for a more
detailed explanation.
.UNINDENT
.sp
Unlike other Ceph tools, \fBcrushtool\fP does not accept generic options
such as \fB\-\-debug\-crush\fP from the command line. They can, however, be
provided via the CEPH_ARGS environment variable. For instance, to
silence all output from the CRUSH subsystem:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
CEPH_ARGS="\-\-debug\-crush 0" crushtool ...
.ft P
.fi
.UNINDENT
.UNINDENT
.SH RUNNING TESTS WITH --TEST
.sp
The test mode will use the input crush map ( as specified with \fB\-i
map\fP ) and perform a dry run of CRUSH mapping or random placement
(if \fB\-\-simulate\fP is set ). On completion, two kinds of reports can be
created.
1) The \fB\-\-show\-...\fP option outputs human readable information
on stderr.
2) The \fB\-\-output\-csv\fP option creates CSV files that are
documented by the \fB\-\-help\-output\fP option.
.sp
Note: Each Placement Group (PG) has an integer ID which can be obtained
from \fBceph pg dump\fP (for example PG 2.2f means pool id 2, PG id 32).
The pool and PG IDs are combined by a function to get a value which is
given to CRUSH to map it to OSDs. crushtool does not know about PGs or
pools; it only runs simulations by mapping values in the range
\fB[\-\-min\-x,\-\-max\-x]\fP\&.
.INDENT 0.0
.TP
.B \-\-show\-statistics
Displays a summary of the distribution. For instance:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
rule 1 (metadata) num_rep 5 result size == 5:    1024/1024
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
shows that rule \fB1\fP which is named \fBmetadata\fP successfully
mapped \fB1024\fP values to \fBresult size == 5\fP devices when trying
to map them to \fBnum_rep 5\fP replicas. When it fails to provide the
required mapping, presumably because the number of \fBtries\fP must
be increased, a breakdown of the failures is displayed. For instance:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
rule 1 (metadata) num_rep 10 result size == 8:   4/1024
rule 1 (metadata) num_rep 10 result size == 9:   93/1024
rule 1 (metadata) num_rep 10 result size == 10:  927/1024
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
shows that although \fBnum_rep 10\fP replicas were required, \fB4\fP
out of \fB1024\fP values ( \fB4/1024\fP ) were mapped to \fBresult size
== 8\fP devices only.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-show\-mappings
Displays the mapping of each value in the range \fB[\-\-min\-x,\-\-max\-x]\fP\&.
For instance:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
CRUSH rule 1 x 24 [11,6]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
shows that value \fB24\fP is mapped to devices \fB[11,6]\fP by rule
\fB1\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-show\-bad\-mappings
Displays which value failed to be mapped to the required number of
devices. For instance:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
bad mapping rule 1 x 781 num_rep 7 result [8,10,2,11,6,9]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
shows that when rule \fB1\fP was required to map \fB7\fP devices, it
could map only six : \fB[8,10,2,11,6,9]\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-show\-utilization
Displays the expected and actual utilization for each device, for
each number of replicas. For instance:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
device 0: stored : 951      expected : 853.333
device 1: stored : 963      expected : 853.333
\&...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
shows that device \fB0\fP stored \fB951\fP values and was expected to store \fB853\fP\&.
Implies \fB\-\-show\-statistics\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-show\-utilization\-all
Displays the same as \fB\-\-show\-utilization\fP but does not suppress
output when the weight of a device is zero.
Implies \fB\-\-show\-statistics\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-show\-choose\-tries
Displays how many attempts were needed to find a device mapping.
For instance:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
0:     95224
1:      3745
2:      2225
\&..
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
shows that \fB95224\fP mappings succeeded without retries, \fB3745\fP
mappings succeeded with one attempts, etc. There are as many rows
as the value of the \fB\-\-set\-choose\-total\-tries\fP option.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-output\-csv
Creates CSV files (in the current directory) containing information
documented by \fB\-\-help\-output\fP\&. The files are named after the rule
used when collecting the statistics. For instance, if the rule
: \(aqmetadata\(aq is used, the CSV files will be:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
metadata\-absolute_weights.csv
metadata\-device_utilization.csv
\&...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The first line of the file shortly explains the column layout. For
instance:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
metadata\-absolute_weights.csv
Device ID, Absolute Weight
0,1
\&...
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \-\-output\-name NAME
Prepend \fBNAME\fP to the file names generated when \fB\-\-output\-csv\fP
is specified. For instance \fB\-\-output\-name FOO\fP will create
files:
.INDENT 7.0
.INDENT 3.5
.sp
.nf
.ft C
FOO\-metadata\-absolute_weights.csv
FOO\-metadata\-device_utilization.csv
\&...
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.sp
The \fB\-\-set\-...\fP options can be used to modify the tunables of the
input crush map. The input crush map is modified in
memory. For example:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ crushtool \-i mymap \-\-test \-\-show\-bad\-mappings
bad mapping rule 1 x 781 num_rep 7 result [8,10,2,11,6,9]
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
could be fixed by increasing the \fBchoose\-total\-tries\fP as follows:
.INDENT 0.0
.INDENT 3.5
.INDENT 0.0
.TP
.B $ crushtool \-i mymap \-\-test 
\-\-show\-bad\-mappings \-\-set\-choose\-total\-tries 500
.UNINDENT
.UNINDENT
.UNINDENT
.SH BUILDING A MAP WITH --BUILD
.sp
The build mode will generate hierarchical maps. The first argument
specifies the number of devices (leaves) in the CRUSH hierarchy. Each
layer describes how the layer (or devices) preceding it should be
grouped.
.sp
Each layer consists of:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
bucket ( uniform | list | tree | straw ) size
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The \fBbucket\fP is the type of the buckets in the layer
(e.g. "rack"). Each bucket name will be built by appending a unique
number to the \fBbucket\fP string (e.g. "rack0", "rack1"...).
.sp
The second component is the type of bucket: \fBstraw\fP should be used
most of the time.
.sp
The third component is the maximum size of the bucket. A size of zero
means a bucket of infinite capacity.
.SH EXAMPLE
.sp
Suppose we have two rows with two racks each and 20 nodes per rack. Suppose
each node contains 4 storage devices for Ceph OSD Daemons. This configuration
allows us to deploy 320 Ceph OSD Daemons. Lets assume a 42U rack with 2U nodes,
leaving an extra 2U for a rack switch.
.sp
To reflect our hierarchy of devices, nodes, racks and rows, we would execute
the following:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ crushtool \-o crushmap \-\-build \-\-num_osds 320 \e
       node straw 4 \e
       rack straw 20 \e
       row straw 2 \e
       root straw 0
# id        weight  type name       reweight
\-87 320     root root
\-85 160             row row0
\-81 80                      rack rack0
\-1  4                               node node0
0   1                                       osd.0   1
1   1                                       osd.1   1
2   1                                       osd.2   1
3   1                                       osd.3   1
\-2  4                               node node1
4   1                                       osd.4   1
5   1                                       osd.5   1
\&...
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
CRUSH rules are created so the generated crushmap can be
tested. They are the same rules as the ones created by default when
creating a new Ceph cluster. They can be further edited with:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
# decompile
crushtool \-d crushmap \-o map.txt

# edit
emacs map.txt

# recompile
crushtool \-c map.txt \-o crushmap
.ft P
.fi
.UNINDENT
.UNINDENT
.SH RECLASSIFY
.sp
The \fIreclassify\fP function allows users to transition from older maps that
maintain parallel hierarchies for OSDs of different types to a modern CRUSH
map that makes use of the \fIdevice class\fP feature.  For more information,
see \fI\%http://docs.ceph.com/docs/master/rados/operations/crush\-map\-edits/#migrating\-from\-a\-legacy\-ssd\-rule\-to\-device\-classes\fP\&.
.SH EXAMPLE OUTPUT FROM --TEST
.sp
See \fI\%https://github.com/ceph/ceph/blob/master/src/test/cli/crushtool/set\-choose.t\fP
for sample \fBcrushtool \-\-test\fP commands and output produced thereby.
.SH AVAILABILITY
.sp
\fBcrushtool\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please
refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8),
osdmaptool(8),
.SH AUTHORS
.sp
John Wilkins, Sage Weil, Loic Dachary
.SH SYNOPSIS
.nf
\fBlibrados\-config\fP [ \-\-version ] [ \-\-vernum ]
.fi
.sp
.SH DESCRIPTION
.INDENT 0.0
.TP
\fBlibrados\-config\fP is a utility that displays information about the
installed \fBlibrados\fP\&.
.UNINDENT
.SH OPTIONS
.INDENT 0.0
.TP
.B \-\-version
Display \fBlibrados\fP version
.UNINDENT
.INDENT 0.0
.TP
.B \-\-vernum
Display the \fBlibrados\fP version code
.UNINDENT
.SH AVAILABILITY
.sp
\fBlibrados\-config\fP is part of Ceph, a massively scalable, open\-source, distributed storage system.
Please refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for
more information.
.SH SEE ALSO
.sp
ceph(8),
rados(8)
.SH SYNOPSIS
.nf
\fBmonmaptool\fP \fImapfilename\fP [ \-\-clobber ] [ \-\-print ] [ \-\-create ]
[ \-\-add \fIip\fP:\fIport\fP \fI\&...\fP ] [ \-\-rm \fIip\fP:\fIport\fP \fI\&...\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBmonmaptool\fP is a utility to create, view, and modify a monitor
cluster map for the Ceph distributed storage system. The monitor map
specifies the only fixed addresses in the Ceph distributed system.
All other daemons bind to arbitrary addresses and register themselves
with the monitors.
.sp
When creating a map with \-\-create, a new monitor map with a new,
random UUID will be created. It should be followed by one or more
monitor addresses.
.sp
The default Ceph monitor port is 6789.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-\-print
will print a plaintext dump of the map, after any modifications are
made.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-clobber
will allow monmaptool to overwrite mapfilename if changes are made.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-create
will create a new monitor map with a new UUID (and with it, a new,
empty Ceph file system).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-generate
generate a new monmap based on the values on the command line or specified
in the ceph configuration.  This is, in order of preference,
.INDENT 7.0
.INDENT 3.5
.INDENT 0.0
.IP 1. 3
\fB\-\-monmap filename\fP to specify a monmap to load
.IP 2. 3
\fB\-\-mon\-host \(aqhost1,ip2\(aq\fP to specify a list of hosts or ip addresses
.IP 3. 3
\fB[mon.foo]\fP sections containing \fBmon addr\fP settings in the config. Note that this method is not recommended and support will be removed in a future release.
.UNINDENT
.UNINDENT
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \-\-filter\-initial\-members
filter the initial monmap by applying the \fBmon initial members\fP
setting.  Monitors not present in that list will be removed, and
initial members not present in the map will be added with dummy
addresses.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-add name ip:port
will add a monitor with the specified ip:port to the map.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-rm name
will remove the monitor with the specified ip:port from the map.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-fsid uuid
will set the fsid to the given uuid.  If not specified with \-\-create, a random fsid will be generated.
.UNINDENT
.SH EXAMPLE
.sp
To create a new map with three monitors (for a fresh Ceph file system):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
monmaptool  \-\-create  \-\-add  mon.a 192.168.0.10:6789 \-\-add mon.b 192.168.0.11:6789 \e
  \-\-add mon.c 192.168.0.12:6789 \-\-clobber monmap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To display the contents of the map:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
monmaptool \-\-print monmap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To replace one monitor:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
monmaptool \-\-rm mon.a \-\-add mon.a 192.168.0.9:6789 \-\-clobber monmap
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBmonmaptool\fP is part of Ceph, a massively scalable, open\-source, distributed
storage system. Please refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP
for more information.
.SH SEE ALSO
.sp
ceph(8),
crushtool(8),
.SH SYNOPSIS
.nf
\fBmount.ceph\fP \fImonaddr1\fP[,\fImonaddr2\fP,...]:/[\fIsubdir\fP] \fIdir\fP [
\-o \fIoptions\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBmount.ceph\fP is a simple helper for mounting the Ceph file system on
a Linux host. It serves to resolve monitor hostname(s) into IP
addresses and read authentication keys from disk; the Linux kernel
client component does most of the real work. In fact, it is possible
to mount a non\-authenticated Ceph file system without mount.ceph by
specifying monitor address(es) by IP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mount \-t ceph 1.2.3.4:/ mountpoint
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Each monitor address monaddr takes the form host[:port]. If the port
is not specified, the Ceph default of 6789 is assumed.
.sp
Multiple monitor addresses can be separated by commas. Only one
responsible monitor is needed to successfully mount; the client will
learn about all monitors from any responsive monitor. However, it is a
good idea to specify more than one in case one happens to be down at
the time of mount.
.sp
A subdirectory subdir may be specified if a subset of the file system
is to be mounted.
.sp
Mount helper application conventions dictate that the first two
options are device to be mounted and destination path. Options must be
passed only after these fixed arguments.
.SH OPTIONS
.INDENT 0.0
.TP
\fBwsize\fP
int (bytes), max write size. Default: 16777216 (16*1024*1024) (writeback uses smaller of wsize
and stripe unit)
.TP
\fBrsize\fP
int (bytes), max read size. Default: 16777216 (16*1024*1024)
.TP
\fBrasize\fP
int (bytes), max readahead. Default: 8388608 (8192*1024)
.TP
\fBosdtimeout\fP
int (seconds), Default: 60
.TP
\fBosdkeepalive\fP
int, Default: 5
.TP
\fBmount_timeout\fP
int (seconds), Default: 60
.TP
\fBosd_idle_ttl\fP
int (seconds), Default: 60
.TP
\fBcaps_wanted_delay_min\fP
int, cap release delay, Default: 5
.TP
\fBcaps_wanted_delay_max\fP
int, cap release delay, Default: 60
.TP
\fBcap_release_safety\fP
int, Default: calculated
.TP
\fBreaddir_max_entries\fP
int, Default: 1024
.TP
\fBreaddir_max_bytes\fP
int, Default: 524288 (512*1024)
.TP
\fBwrite_congestion_kb\fP
int (kb), max writeback in flight. scale with available
memory. Default: calculated from available memory
.TP
\fBsnapdirname\fP
string, set the name of the hidden snapdir. Default: .snap
.TP
\fBname\fP
RADOS user to authenticate as when using cephx. Default: guest
.TP
\fBsecret\fP
secret key for use with cephx. This option is insecure because it exposes
the secret on the command line. To avoid this, use the secretfile option.
.TP
\fBsecretfile\fP
path to file containing the secret key to use with cephx
.TP
\fBip\fP
my ip
.TP
\fBnoshare\fP
create a new client instance, instead of sharing an existing
instance of a client mounting the same cluster
.TP
\fBdirstat\fP
funky \fIcat dirname\fP for stats, Default: off
.TP
\fBnodirstat\fP
no funky \fIcat dirname\fP for stats
.TP
\fBrbytes\fP
Report the recursive size of the directory contents for st_size on
directories.  Default: off
.TP
\fBnorbytes\fP
Do not report the recursive size of the directory contents for
st_size on directories.
.TP
\fBnocrc\fP
no data crc on writes
.TP
\fBnoasyncreaddir\fP
no dcache readdir
.UNINDENT
.SH EXAMPLES
.sp
Mount the full file system:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mount.ceph monhost:/ /mnt/foo
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
If there are multiple monitors:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mount.ceph monhost1,monhost2,monhost3:/ /mnt/foo
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
If ceph\-mon(8) is running on a non\-standard
port:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mount.ceph monhost1:7000,monhost2:7000,monhost3:7000:/ /mnt/foo
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To mount only part of the namespace:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mount.ceph monhost1:/some/small/thing /mnt/thing
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Assuming mount.ceph(8) is installed properly, it should be
automatically invoked by mount(8) like so:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
mount \-t ceph monhost:/ /mnt/foo
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBmount.ceph\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please
refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph\-fuse(8),
ceph(8)
.SH SYNOPSIS
.nf
\fBosdmaptool\fP \fImapfilename\fP [\-\-print] [\-\-createsimple \fInumosd\fP
[\-\-pgbits \fIbitsperosd\fP ] ] [\-\-clobber]
.fi
.sp
.SH DESCRIPTION
.sp
\fBosdmaptool\fP is a utility that lets you create, view, and manipulate
OSD cluster maps from the Ceph distributed storage system. Notably, it
lets you extract the embedded CRUSH map or import a new CRUSH map.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-\-print
will simply make the tool print a plaintext dump of the map, after
any modifications are made.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-dump <format>
displays the map in plain text when <format> is \(aqplain\(aq, \(aqjson\(aq if specified
format is not supported. This is an alternative to the print option.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-clobber
will allow osdmaptool to overwrite mapfilename if changes are made.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-import\-crush mapfile
will load the CRUSH map from mapfile and embed it in the OSD map.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-export\-crush mapfile
will extract the CRUSH map from the OSD map and write it to
mapfile.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-createsimple numosd [\-\-pg\-bits bitsperosd] [\-\-pgp\-bits bits]
will create a relatively generic OSD map with the numosd devices.
If \-\-pg\-bits is specified, the initial placement group counts will
be set with bitsperosd bits per OSD. That is, the pg_num map
attribute will be set to numosd shifted by bitsperosd.
If \-\-pgp\-bits is specified, then the pgp_num map attribute will
be set to numosd shifted by bits.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-create\-from\-conf
creates an osd map with default configurations.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-test\-map\-pgs [\-\-pool poolid] [\-\-range\-first <first> \-\-range\-last <last>]
will print out the mappings from placement groups to OSDs.
If range is specified, then it iterates from first to last in the directory
specified by argument to osdmaptool.
Eg: \fBosdmaptool \-\-test\-map\-pgs \-\-range\-first 0 \-\-range\-last 2 osdmap_dir\fP\&.
This will iterate through the files named 0,1,2 in osdmap_dir.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-test\-map\-pgs\-dump [\-\-pool poolid] [\-\-range\-first <first> \-\-range\-last <last>]
will print out the summary of all placement groups and the mappings from them to the mapped OSDs.
If range is specified, then it iterates from first to last in the directory
specified by argument to osdmaptool.
Eg: \fBosdmaptool \-\-test\-map\-pgs\-dump \-\-range\-first 0 \-\-range\-last 2 osdmap_dir\fP\&.
This will iterate through the files named 0,1,2 in osdmap_dir.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-test\-map\-pgs\-dump\-all [\-\-pool poolid] [\-\-range\-first <first> \-\-range\-last <last>]
will print out the summary of all placement groups and the mappings
from them to all the OSDs.
If range is specified, then it iterates from first to last in the directory
specified by argument to osdmaptool.
Eg: \fBosdmaptool \-\-test\-map\-pgs\-dump\-all \-\-range\-first 0 \-\-range\-last 2 osdmap_dir\fP\&.
This will iterate through the files named 0,1,2 in osdmap_dir.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-test\-random
does a random mapping of placement groups to the OSDs.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-test\-map\-pg <pgid>
map a particular placement group(specified by pgid) to the OSDs.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-test\-map\-object <objectname> [\-\-pool <poolid>]
map a particular placement group(specified by objectname) to the OSDs.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-test\-crush [\-\-range\-first <first> \-\-range\-last <last>]
map placement groups to acting OSDs.
If range is specified, then it iterates from first to last in the directory
specified by argument to osdmaptool.
Eg: \fBosdmaptool \-\-test\-crush \-\-range\-first 0 \-\-range\-last 2 osdmap_dir\fP\&.
This will iterate through the files named 0,1,2 in osdmap_dir.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-mark\-up\-in
mark osds up and in (but do not persist).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-tree
Displays a hierarchical tree of the map.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-clear\-temp
clears pg_temp and primary_temp variables.
.UNINDENT
.SH EXAMPLE
.sp
To create a simple map with 16 devices:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
osdmaptool \-\-createsimple 16 osdmap \-\-clobber
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To view the result:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
osdmaptool \-\-print osdmap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To view the mappings of placement groups for pool 0:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
osdmaptool \-\-test\-map\-pgs\-dump rbd \-\-pool 0

pool 0 pg_num 8
0.0     [0,2,1] 0
0.1     [2,0,1] 2
0.2     [0,1,2] 0
0.3     [2,0,1] 2
0.4     [0,2,1] 0
0.5     [0,2,1] 0
0.6     [0,1,2] 0
0.7     [1,0,2] 1
#osd    count   first   primary c wt    wt
osd.0   8       5       5       1       1
osd.1   8       1       1       1       1
osd.2   8       2       2       1       1
 in 3
 avg 8 stddev 0 (0x) (expected 2.3094 0.288675x))
 min osd.0 8
 max osd.0 8
size 0  0
size 1  0
size 2  0
size 3  8
.ft P
.fi
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B In which,
.INDENT 7.0
.IP 1. 3
pool 0 has 8 placement groups. And two tables follow:
.IP 2. 3
A table for placement groups. Each row presents a placement group. With columns of:
.INDENT 3.0
.IP \(bu 2
placement group id,
.IP \(bu 2
acting set, and
.IP \(bu 2
primary OSD.
.UNINDENT
.IP 3. 3
A table for all OSDs. Each row presents an OSD. With columns of:
.INDENT 3.0
.IP \(bu 2
count of placement groups being mapped to this OSD,
.IP \(bu 2
count of placement groups where this OSD is the first one in their acting sets,
.IP \(bu 2
count of placement groups where this OSD is the primary of them,
.IP \(bu 2
the CRUSH weight of this OSD, and
.IP \(bu 2
the weight of this OSD.
.UNINDENT
.IP 4. 3
Looking at the number of placement groups held by 3 OSDs. We have
.INDENT 3.0
.IP \(bu 2
avarge, stddev, stddev/average, expected stddev, expected stddev / average
.IP \(bu 2
min and max
.UNINDENT
.IP 5. 3
The number of placement groups mapping to n OSDs. In this case, all 8 placement
groups are mapping to 3 different OSDs.
.UNINDENT
.UNINDENT
.sp
In a less\-balanced cluster, we could have following output for the statistics of
placement group distribution, whose standard deviation is 1.41421:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
#osd    count   first   primary c wt    wt
osd.0   8       5       5       1       1
osd.1   8       1       1       1       1
osd.2   8       2       2       1       1

#osd    count   first    primary c wt    wt
osd.0   33      9        9       0.0145874     1
osd.1   34      14       14      0.0145874     1
osd.2   31      7        7       0.0145874     1
osd.3   31      13       13      0.0145874     1
osd.4   30      14       14      0.0145874     1
osd.5   33      7        7       0.0145874     1
 in 6
 avg 32 stddev 1.41421 (0.0441942x) (expected 5.16398 0.161374x))
 min osd.4 30
 max osd.1 34
size 00
size 10
size 20
size 364
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBosdmaptool\fP is part of Ceph, a massively scalable, open\-source, distributed storage system.  Please
refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for more
information.
.SH SEE ALSO
.sp
ceph(8),
crushtool(8),
.SH SYNOPSIS
.nf
\fBrados\fP [ \fIoptions\fP ] [ \fIcommand\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBrados\fP is a utility for interacting with a Ceph object storage
cluster (RADOS), part of the Ceph distributed storage system.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-p pool, \-\-pool pool
Interact with the given pool. Required by most commands.
.UNINDENT
.INDENT 0.0
.TP
.B \-s snap, \-\-snap snap
Read from the given pool snapshot. Valid for all pool\-specific read operations.
.UNINDENT
.INDENT 0.0
.TP
.B \-i infile
will specify an input file to be passed along as a payload with the
command to the monitor cluster. This is only used for specific
monitor commands.
.UNINDENT
.INDENT 0.0
.TP
.B \-o outfile
will write any payload returned by the monitor cluster with its
reply to outfile. Only specific monitor commands (e.g. osd getmap)
return a payload.
.UNINDENT
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use ceph.conf configuration file instead of the default
/etc/ceph/ceph.conf to determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through ceph.conf).
.UNINDENT
.INDENT 0.0
.TP
.B \-b block_size
Set the block size for put/get/append ops and for write benchmarking.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-striper
Uses the striping API of rados rather than the default one.
Available for stat, stat2, get, put, append, truncate, rm, ls
and all xattr related operation
.UNINDENT
.SH GLOBAL COMMANDS
.INDENT 0.0
.TP
\fBlspools\fP
List object pools
.TP
\fBdf\fP
Show utilization statistics, including disk usage (bytes) and object
counts, over the entire system and broken down by pool.
.TP
\fBlist\-inconsistent\-pg\fP \fIpool\fP
List inconsistent PGs in given pool.
.TP
\fBlist\-inconsistent\-obj\fP \fIpgid\fP
List inconsistent objects in given PG.
.TP
\fBlist\-inconsistent\-snapset\fP \fIpgid\fP
List inconsistent snapsets in given PG.
.UNINDENT
.SH POOL SPECIFIC COMMANDS
.INDENT 0.0
.TP
\fBget\fP \fIname\fP \fIoutfile\fP
Read object name from the cluster and write it to outfile.
.TP
\fBput\fP \fIname\fP \fIinfile\fP [\-\-offset offset]
Write object name with start offset (default:0) to the cluster with contents from infile.
\fBWarning:\fP The put command creates a single RADOS object, sized just as
large as your input file. Unless your objects are of reasonable and consistent sizes, that
is probably not what you want \-\- consider using RGW/S3, CephFS, or RBD instead.
.TP
\fBappend\fP \fIname\fP \fIinfile\fP
Append object name to the cluster with contents from infile.
.TP
\fBrm\fP \fIname\fP
Remove object name.
.TP
\fBlistwatchers\fP \fIname\fP
List the watchers of object name.
.TP
\fBls\fP \fIoutfile\fP
List objects in given pool and write to outfile.
.TP
\fBlssnap\fP
List snapshots for given pool.
.TP
\fBclonedata\fP \fIsrcname\fP \fIdstname\fP \-\-object\-locator \fIkey\fP
Clone object byte data from \fIsrcname\fP to \fIdstname\fP\&.  Both objects must be stored with the locator key \fIkey\fP (usually either \fIsrcname\fP or \fIdstname\fP).  Object attributes and omap keys are not copied or cloned.
.TP
\fBmksnap\fP \fIfoo\fP
Create pool snapshot named \fIfoo\fP\&.
.TP
\fBrmsnap\fP \fIfoo\fP
Remove pool snapshot named \fIfoo\fP\&.
.TP
\fBbench\fP \fIseconds\fP \fImode\fP [ \-b \fIobjsize\fP ] [ \-t \fIthreads\fP ]
Benchmark for \fIseconds\fP\&. The mode can be \fIwrite\fP, \fIseq\fP, or
\fIrand\fP\&. \fIseq\fP and \fIrand\fP are read benchmarks, either
sequential or random. Before running one of the reading benchmarks,
run a write benchmark with the \fI\-\-no\-cleanup\fP option. The default
object size is 4 MB, and the default number of simulated threads
(parallel writes) is 16. The \fI\-\-run\-name <label>\fP option is useful
for benchmarking a workload test from multiple clients. The \fI<label>\fP
is an arbitrary object name. It is "benchmark_last_metadata" by
default, and is used as the underlying object name for "read" and
"write" ops.
Note: \-b \fIobjsize\fP option is valid only in \fIwrite\fP mode.
Note: \fIwrite\fP and \fIseq\fP must be run on the same host otherwise the
objects created by \fIwrite\fP will have names that will fail \fIseq\fP\&.
.TP
\fBcleanup\fP [ \-\-run\-name \fIrun_name\fP ] [ \-\-prefix \fIprefix\fP ]
Clean up a previous benchmark operation.
Note: the default run\-name is "benchmark_last_metadata"
.TP
\fBlistxattr\fP \fIname\fP
List all extended attributes of an object.
.TP
\fBgetxattr\fP \fIname\fP \fIattr\fP
Dump the extended attribute value of \fIattr\fP of an object.
.TP
\fBsetxattr\fP \fIname\fP \fIattr\fP \fIvalue\fP
Set the value of \fIattr\fP in the extended attributes of an object.
.TP
\fBrmxattr\fP \fIname\fP \fIattr\fP
Remove \fIattr\fP from the extended attributes of an object.
.TP
\fBstat\fP \fIname\fP
Get stat (ie. mtime, size) of given object
.TP
\fBstat2\fP \fIname\fP
Get stat (similar to stat, but with high precision time) of given object
.TP
\fBlistomapkeys\fP \fIname\fP
List all the keys stored in the object map of object name.
.TP
\fBlistomapvals\fP \fIname\fP
List all key/value pairs stored in the object map of object name.
The values are dumped in hexadecimal.
.TP
\fBgetomapval\fP [ \-\-omap\-key\-file \fIfile\fP ] \fIname\fP \fIkey\fP [ \fIout\-file\fP ]
Dump the hexadecimal value of key in the object map of object name.
If the optional \fIout\-file\fP argument is not provided, the value will be
written to standard output.
.TP
\fBsetomapval\fP [ \-\-omap\-key\-file \fIfile\fP ] \fIname\fP \fIkey\fP [ \fIvalue\fP ]
Set the value of key in the object map of object name. If the optional
\fIvalue\fP argument is not provided, the value will be read from standard
input.
.TP
\fBrmomapkey\fP [ \-\-omap\-key\-file \fIfile\fP ] \fIname\fP \fIkey\fP
Remove key from the object map of object name.
.TP
\fBgetomapheader\fP \fIname\fP
Dump the hexadecimal value of the object map header of object name.
.TP
\fBsetomapheader\fP \fIname\fP \fIvalue\fP
Set the value of the object map header of object name.
.TP
\fBexport\fP \fIfilename\fP
Serialize pool contents to a file or standard output.n"
.TP
\fBimport\fP [\-\-dry\-run] [\-\-no\-overwrite] < filename | \- >
Load pool contents from a file or standard input
.UNINDENT
.SH EXAMPLES
.sp
To view cluster utilization:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rados df
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To get a list object in pool foo sent to stdout:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rados \-p foo ls \-
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To write an object:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rados \-p foo put myobject blah.txt
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To create a snapshot:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rados \-p foo mksnap mysnap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To delete the object:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rados \-p foo rm myobject
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To read a previously snapshotted version of an object:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rados \-p foo \-s mysnap get myobject blah.txt.old
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To list inconsistent objects in PG 0.6:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rados list\-inconsistent\-obj 0.6 \-\-format=json\-pretty
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBrados\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph(8)
.SH SYNOPSIS
.nf
\fBradosgw\-admin\fP \fIcommand\fP [ \fIoptions\fP \fI\&...\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBradosgw\-admin\fP is a RADOS gateway user administration utility. It
allows creating and modifying users.
.SH COMMANDS
.sp
\fBradosgw\-admin\fP utility uses many commands for administration purpose
which are as follows:
.INDENT 0.0
.TP
\fBuser create\fP
Create a new user.
.TP
\fBuser modify\fP
Modify a user.
.TP
\fBuser info\fP
Display information of a user, and any potentially available
subusers and keys.
.TP
\fBuser rm\fP
Remove a user.
.TP
\fBuser suspend\fP
Suspend a user.
.TP
\fBuser enable\fP
Re\-enable user after suspension.
.TP
\fBuser check\fP
Check user info.
.TP
\fBuser stats\fP
Show user stats as accounted by quota subsystem.
.TP
\fBuser list\fP
List all users.
.TP
\fBcaps add\fP
Add user capabilities.
.TP
\fBcaps rm\fP
Remove user capabilities.
.TP
\fBsubuser create\fP
Create a new subuser (primarily useful for clients using the Swift API).
.TP
\fBsubuser modify\fP
Modify a subuser.
.TP
\fBsubuser rm\fP
Remove a subuser.
.TP
\fBkey create\fP
Create access key.
.TP
\fBkey rm\fP
Remove access key.
.TP
\fBbucket list\fP
List all buckets.
.TP
\fBbucket limit check\fP
Show bucket sharding stats.
.TP
\fBbucket link\fP
Link bucket to specified user.
.TP
\fBbucket unlink\fP
Unlink bucket from specified user.
.TP
\fBbucket stats\fP
Returns bucket statistics.
.TP
\fBbucket rm\fP
Remove a bucket.
.TP
\fBbucket check\fP
Check bucket index.
.TP
\fBbucket rewrite\fP
Rewrite all objects in the specified bucket.
.TP
\fBbucket reshard\fP
Reshard a bucket.
.TP
\fBbucket sync disable\fP
Disable bucket sync.
.TP
\fBbucket sync enable\fP
Enable bucket sync.
.TP
\fBbi get\fP
Retrieve bucket index object entries.
.TP
\fBbi put\fP
Store bucket index object entries.
.TP
\fBbi list\fP
List raw bucket index entries.
.TP
\fBbi purge\fP
Purge bucket index entries.
.TP
\fBobject rm\fP
Remove an object.
.TP
\fBobject stat\fP
Stat an object for its metadata.
.TP
\fBobject unlink\fP
Unlink object from bucket index.
.TP
\fBobject rewrite\fP
Rewrite the specified object.
.TP
\fBobjects expire\fP
Run expired objects cleanup.
.TP
\fBperiod rm\fP
Remove a period.
.TP
\fBperiod get\fP
Get the period info.
.TP
\fBperiod get\-current\fP
Get the current period info.
.TP
\fBperiod pull\fP
Pull a period.
.TP
\fBperiod push\fP
Push a period.
.TP
\fBperiod list\fP
List all periods.
.TP
\fBperiod update\fP
Update the staging period.
.TP
\fBperiod commit\fP
Commit the staging period.
.TP
\fBquota set\fP
Set quota params.
.TP
\fBquota enable\fP
Enable quota.
.TP
\fBquota disable\fP
Disable quota.
.TP
\fBglobal quota get\fP
View global quota parameters.
.TP
\fBglobal quota set\fP
Set global quota parameters.
.TP
\fBglobal quota enable\fP
Enable a global quota.
.TP
\fBglobal quota disable\fP
Disable a global quota.
.TP
\fBrealm create\fP
Create a new realm.
.TP
\fBrealm rm\fP
Remove a realm.
.TP
\fBrealm get\fP
Show the realm info.
.TP
\fBrealm get\-default\fP
Get the default realm name.
.TP
\fBrealm list\fP
List all realms.
.TP
\fBrealm list\-periods\fP
List all realm periods.
.TP
\fBrealm rename\fP
Rename a realm.
.TP
\fBrealm set\fP
Set the realm info (requires infile).
.TP
\fBrealm default\fP
Set the realm as default.
.TP
\fBrealm pull\fP
Pull a realm and its current period.
.TP
\fBzonegroup add\fP
Add a zone to a zonegroup.
.TP
\fBzonegroup create\fP
Create a new zone group info.
.TP
\fBzonegroup default\fP
Set the default zone group.
.TP
\fBzonegroup rm\fP
Remove a zone group info.
.TP
\fBzonegroup get\fP
Show the zone group info.
.TP
\fBzonegroup modify\fP
Modify an existing zonegroup.
.TP
\fBzonegroup set\fP
Set the zone group info (requires infile).
.TP
\fBzonegroup remove\fP
Remove a zone from a zonegroup.
.TP
\fBzonegroup rename\fP
Rename a zone group.
.TP
\fBzonegroup list\fP
List all zone groups set on this cluster.
.TP
\fBzonegroup placement list\fP
List zonegroup\(aqs placement targets.
.TP
\fBzonegroup placement add\fP
Add a placement target id to a zonegroup.
.TP
\fBzonegroup placement modify\fP
Modify a placement target of a specific zonegroup.
.TP
\fBzonegroup placement rm\fP
Remove a placement target from a zonegroup.
.TP
\fBzonegroup placement default\fP
Set a zonegroup\(aqs default placement target.
.TP
\fBzone create\fP
Create a new zone.
.TP
\fBzone rm\fP
Remove a zone.
.TP
\fBzone get\fP
Show zone cluster params.
.TP
\fBzone set\fP
Set zone cluster params (requires infile).
.TP
\fBzone modify\fP
Modify an existing zone.
.TP
\fBzone list\fP
List all zones set on this cluster.
.TP
\fBmetadata sync status\fP
Get metadata sync status.
.TP
\fBmetadata sync init\fP
Init metadata sync.
.TP
\fBmetadata sync run\fP
Run metadata sync.
.TP
\fBdata sync status\fP
Get data sync status of the specified source zone.
.TP
\fBdata sync init\fP
Init data sync for the specified source zone.
.TP
\fBdata sync run\fP
Run data sync for the specified source zone.
.TP
\fBsync error list\fP
list sync error.
.TP
\fBsync error trim\fP
trim sync error.
.TP
\fBzone rename\fP
Rename a zone.
.TP
\fBzone placement list\fP
List zone\(aqs placement targets.
.TP
\fBzone placement add\fP
Add a zone placement target.
.TP
\fBzone placement modify\fP
Modify a zone placement target.
.TP
\fBzone placement rm\fP
Remove a zone placement target.
.TP
\fBpool add\fP
Add an existing pool for data placement.
.TP
\fBpool rm\fP
Remove an existing pool from data placement set.
.TP
\fBpools list\fP
List placement active set.
.TP
\fBpolicy\fP
Display bucket/object policy.
.TP
\fBlog list\fP
List log objects.
.TP
\fBlog show\fP
Dump a log from specific object or (bucket + date + bucket\-id).
(NOTE: required to specify formatting of date to "YYYY\-MM\-DD\-hh")
.TP
\fBlog rm\fP
Remove log object.
.TP
\fBusage show\fP
Show the usage information (with optional user and date range).
.TP
\fBusage trim\fP
Trim usage information (with optional user and date range).
.TP
\fBgc list\fP
Dump expired garbage collection objects (specify \-\-include\-all to list all
entries, including unexpired).
.TP
\fBgc process\fP
Manually process garbage.
.TP
\fBlc list\fP
List all bucket lifecycle progress.
.TP
\fBlc process\fP
Manually process lifecycle.
.TP
\fBmetadata get\fP
Get metadata info.
.TP
\fBmetadata put\fP
Put metadata info.
.TP
\fBmetadata rm\fP
Remove metadata info.
.TP
\fBmetadata list\fP
List metadata info.
.TP
\fBmdlog list\fP
List metadata log.
.TP
\fBmdlog trim\fP
Trim metadata log.
.TP
\fBmdlog status\fP
Read metadata log status.
.TP
\fBbilog list\fP
List bucket index log.
.TP
\fBbilog trim\fP
Trim bucket index log (use start\-marker, end\-marker).
.TP
\fBdatalog list\fP
List data log.
.TP
\fBdatalog trim\fP
Trim data log.
.TP
\fBdatalog status\fP
Read data log status.
.TP
\fBorphans find\fP
Init and run search for leaked rados objects
.TP
\fBorphans finish\fP
Clean up search for leaked rados objects
.TP
\fBorphans list\-jobs\fP
List the current job\-ids for the orphans search.
.TP
\fBrole create\fP
create a new AWS role for use with STS.
.TP
\fBrole rm\fP
Remove a role.
.TP
\fBrole get\fP
Get a role.
.TP
\fBrole list\fP
List the roles with specified path prefix.
.TP
\fBrole modify\fP
Modify the assume role policy of an existing role.
.TP
\fBrole\-policy put\fP
Add/update permission policy to role.
.TP
\fBrole\-policy list\fP
List the policies attached to a role.
.TP
\fBrole\-policy get\fP
Get the specified inline policy document embedded with the given role.
.TP
\fBrole\-policy rm\fP
Remove the policy attached to a role
.TP
\fBreshard add\fP
Schedule a resharding of a bucket
.TP
\fBreshard list\fP
List all bucket resharding or scheduled to be resharded
.TP
\fBreshard process\fP
Process of scheduled reshard jobs
.TP
\fBreshard status\fP
Resharding status of a bucket
.TP
\fBreshard cancel\fP
Cancel resharding a bucket
.UNINDENT
.SH OPTIONS
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fBceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during
startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through ceph.conf).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-tenant=<tenant>
Name of the tenant.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-uid=uid
The radosgw user ID.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-subuser=<name>
Name of the subuser.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-access\-key=<key>
S3 access key.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-email=email
The e\-mail address of the user.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-secret/\-\-secret\-key=<key>
The secret key.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-gen\-access\-key
Generate random access key (for S3).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-gen\-secret
Generate random secret key.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-key\-type=<type>
key type, options are: swift, s3.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-temp\-url\-key[\-2]=<key>
Temporary url key.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-max\-buckets
max number of buckets for a user (0 for no limit, negative value to disable bucket creation).
Default is 1000.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-access=<access>
Set the access permissions for the sub\-user.
Available access permissions are read, write, readwrite and full.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-display\-name=<name>
The display name of the user.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-admin
Set the admin flag on the user.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-system
Set the system flag on the user.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-bucket=bucket
Specify the bucket name.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-pool=<pool>
Specify the pool name.
Also used with \fIorphans find\fP as data pool to scan for leaked rados objects.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-object=object
Specify the object name.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-date=yyyy\-mm\-dd
The date in the format yyyy\-mm\-dd.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-start\-date=yyyy\-mm\-dd
The start date in the format yyyy\-mm\-dd.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-end\-date=yyyy\-mm\-dd
The end date in the format yyyy\-mm\-dd.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-bucket\-id=<bucket\-id>
Specify the bucket id.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-shard\-id=<shard\-id>
Optional for mdlog list, data sync status. Required for \fBmdlog trim\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-max\-entries=<entries>
Optional for listing operations to specify the max entires
.UNINDENT
.INDENT 0.0
.TP
.B \-\-purge\-data
When specified, user removal will also purge all the user data.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-purge\-keys
When specified, subuser removal will also purge all the subuser keys.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-purge\-objects
When specified, the bucket removal will also purge all objects in it.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-metadata\-key=<key>
Key to retrieve metadata from with \fBmetadata get\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-remote=<remote>
Zone or zonegroup id of remote gateway.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-period=<id>
Period id.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-url=<url>
url for pushing/pulling period or realm.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-epoch=<number>
Period epoch.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-commit
Commit the period during \(aqperiod update\(aq.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-staging
Get the staging period info.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-master
Set as master.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-master\-zone=<id>
Master zone id.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-rgw\-realm=<name>
The realm name.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-realm\-id=<id>
The realm id.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-realm\-new\-name=<name>
New name of realm.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-rgw\-zonegroup=<name>
The zonegroup name.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-zonegroup\-id=<id>
The zonegroup id.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-zonegroup\-new\-name=<name>
The new name of the zonegroup.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-rgw\-zone=<zone>
Zone in which radosgw is running.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-zone\-id=<id>
The zone id.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-zone\-new\-name=<name>
The new name of the zone.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-source\-zone
The source zone for data sync.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-default
Set the entity (realm, zonegroup, zone) as default.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-read\-only
Set the zone as read\-only when adding to the zonegroup.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-placement\-id
Placement id for the zonegroup placement commands.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-tags=<list>
The list of tags for zonegroup placement add and modify commands.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-tags\-add=<list>
The list of tags to add for zonegroup placement modify command.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-tags\-rm=<list>
The list of tags to remove for zonegroup placement modify command.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-endpoints=<list>
The zone endpoints.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-index\-pool=<pool>
The placement target index pool.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-data\-pool=<pool>
The placement target data pool.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-data\-extra\-pool=<pool>
The placement target data extra (non\-ec) pool.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-placement\-index\-type=<type>
The placement target index type (normal, indexless, or #id).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-tier\-type=<type>
The zone tier type.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-tier\-config=<k>=<v>[,...]
Set zone tier config keys, values.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-tier\-config\-rm=<k>[,...]
Unset zone tier config keys.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-sync\-from\-all[=false]
Set/reset whether zone syncs from all zonegroup peers.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-sync\-from=[zone\-name][,...]
Set the list of zones to sync from.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-sync\-from\-rm=[zone\-name][,...]
Remove the zones from list of zones to sync from.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-fix
Besides checking bucket index, will also fix it.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-check\-objects
bucket check: Rebuilds bucket index according to actual objects state.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-format=<format>
Specify output format for certain operations. Supported formats: xml, json.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-sync\-stats
Option for \(aquser stats\(aq command. When specified, it will update user stats with
the current stats reported by user\(aqs buckets indexes.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-show\-log\-entries=<flag>
Enable/disable dump of log entries on log show.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-show\-log\-sum=<flag>
Enable/disable dump of log summation on log show.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-skip\-zero\-entries
Log show only dumps entries that don\(aqt have zero value in one of the numeric
field.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-infile
Specify a file to read in when setting data.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-categories=<list>
Comma separated list of categories, used in usage show.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-caps=<caps>
List of caps (e.g., "usage=read, write; user=read".
.UNINDENT
.INDENT 0.0
.TP
.B \-\-compression=<compression\-algorithm>
Placement target compression algorithm (lz4|snappy|zlib|zstd)
.UNINDENT
.INDENT 0.0
.TP
.B \-\-yes\-i\-really\-mean\-it
Required for certain operations.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-min\-rewrite\-size
Specify the min object size for bucket rewrite (default 4M).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-max\-rewrite\-size
Specify the max object size for bucket rewrite (default ULLONG_MAX).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-min\-rewrite\-stripe\-size
Specify the min stripe size for object rewrite (default 0). If the value
is set to 0, then the specified object will always be
rewritten for restriping.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-warnings\-only
When specified with bucket limit check,
list only buckets nearing or over the current max objects per shard value.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-bypass\-gc
When specified with bucket deletion,
triggers object deletions by not involving GC.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-inconsistent\-index
When specified with bucket deletion and bypass\-gc set to true,
ignores bucket index consistency.
.UNINDENT
.SH QUOTA OPTIONS
.INDENT 0.0
.TP
.B \-\-max\-objects
Specify max objects (negative value to disable).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-max\-size
Specify max size (in B/K/M/G/T, negative value to disable).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-quota\-scope
The scope of quota (bucket, user).
.UNINDENT
.SH ORPHANS SEARCH OPTIONS
.INDENT 0.0
.TP
.B \-\-num\-shards
Number of shards to use for keeping the temporary scan info
.UNINDENT
.INDENT 0.0
.TP
.B \-\-orphan\-stale\-secs
Number of seconds to wait before declaring an object to be an orphan.
Default is 86400 (24 hours).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-job\-id
Set the job id (for orphans find)
.UNINDENT
.INDENT 0.0
.TP
.B \-\-max\-concurrent\-ios
Maximum concurrent ios for orphans find.
Default is 32.
.UNINDENT
.SH ORPHANS LIST-JOBS OPTIONS
.INDENT 0.0
.TP
.B \-\-extra\-info
Provide extra info in the job list.
.UNINDENT
.SH ROLE OPTIONS
.INDENT 0.0
.TP
.B \-\-role\-name
The name of the role to create.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-path
The path to the role.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-assume\-role\-policy\-doc
The trust relationship policy document that grants an entity permission to
assume the role.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-policy\-name
The name of the policy document.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-policy\-doc
The permission policy document.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-path\-prefix
The path prefix for filtering the roles.
.UNINDENT
.SH EXAMPLES
.sp
Generate a new user:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin user create \-\-display\-name="johnny rotten" \-\-uid=johnny
{ "user_id": "johnny",
  "rados_uid": 0,
  "display_name": "johnny rotten",
  "email": "",
  "suspended": 0,
  "subusers": [],
  "keys": [
        { "user": "johnny",
          "access_key": "TCICW53D9BQ2VGC46I44",
          "secret_key": "tfm9aHMI8X76L3UdgE+ZQaJag1vJQmE6HDb5Lbrz"}],
  "swift_keys": []}
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Remove a user:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin user rm \-\-uid=johnny
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Remove a user and all associated buckets with their contents:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin user rm \-\-uid=johnny \-\-purge\-data
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Remove a bucket:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin bucket rm \-\-bucket=foo
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Link bucket to specified user:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin bucket link \-\-bucket=foo \-\-bucket_id=<bucket id> \-\-uid=johnny
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Unlink bucket from specified user:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin bucket unlink \-\-bucket=foo \-\-uid=johnny
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Show the logs of a bucket from April 1st, 2012:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin log show \-\-bucket=foo \-\-date=2012\-04\-01\-01 \-\-bucket\-id=default.14193.1
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Show usage information for user from March 1st to (but not including) April 1st, 2012:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin usage show \-\-uid=johnny \e
                \-\-start\-date=2012\-03\-01 \-\-end\-date=2012\-04\-01
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Show only summary of usage information for all users:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin usage show \-\-show\-log\-entries=false
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Trim usage information for user until March 1st, 2012:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
$ radosgw\-admin usage trim \-\-uid=johnny \-\-end\-date=2012\-04\-01
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBradosgw\-admin\fP is part of Ceph, a massively scalable, open\-source,
distributed storage system.  Please refer to the Ceph documentation at
\fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph(8)
radosgw(8)
.SH SYNOPSIS
.nf
\fBradosgw\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBradosgw\fP is an HTTP REST gateway for the RADOS object store, a part
of the Ceph distributed storage system. It is implemented as a FastCGI
module using libfcgi, and can be used in conjunction with any FastCGI
capable web server.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fBceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through \fBceph.conf\fP).
.UNINDENT
.INDENT 0.0
.TP
.B \-i ID, \-\-id ID
Set the ID portion of name for radosgw
.UNINDENT
.INDENT 0.0
.TP
.B \-n TYPE.ID, \-\-name TYPE.ID
Set the rados user name for the gateway (eg. client.radosgw.gateway)
.UNINDENT
.INDENT 0.0
.TP
.B \-\-cluster NAME
Set the cluster name (default: ceph)
.UNINDENT
.INDENT 0.0
.TP
.B \-d
Run in foreground, log to stderr
.UNINDENT
.INDENT 0.0
.TP
.B \-f
Run in foreground, log to usual location
.UNINDENT
.INDENT 0.0
.TP
.B \-\-rgw\-socket\-path=path
Specify a unix domain socket path.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-rgw\-region=region
The region where radosgw runs
.UNINDENT
.INDENT 0.0
.TP
.B \-\-rgw\-zone=zone
The zone where radosgw runs
.UNINDENT
.SH CONFIGURATION
.sp
Earlier RADOS Gateway had to be configured with \fBApache\fP and \fBmod_fastcgi\fP\&.
Now, \fBmod_proxy_fcgi\fP module is used instead of \fBmod_fastcgi\fP\&.
\fBmod_proxy_fcgi\fP works differently than a traditional FastCGI module. This
module requires the service of \fBmod_proxy\fP which provides support for the
FastCGI protocol. So, to be able to handle FastCGI protocol, both \fBmod_proxy\fP
and \fBmod_proxy_fcgi\fP have to be present in the server. Unlike \fBmod_fastcgi\fP,
\fBmod_proxy_fcgi\fP cannot start the application process. Some platforms have
\fBfcgistarter\fP for that purpose. However, external launching of application
or process management may be available in the FastCGI application framework
in use.
.sp
\fBApache\fP can be configured in a way that enables \fBmod_proxy_fcgi\fP to be used
with localhost tcp or through unix domain socket. \fBmod_proxy_fcgi\fP that doesn\(aqt
support unix domain socket such as the ones in Apache 2.2 and earlier versions of
Apache 2.4, needs to be configured for use with localhost tcp. Later versions of
Apache like Apache 2.4.9 or later support unix domain socket and as such they
allow for the configuration with unix domain socket instead of localhost tcp.
.sp
The following steps show the configuration in Ceph\(aqs configuration file i.e,
\fB/etc/ceph/ceph.conf\fP and the gateway configuration file i.e,
\fB/etc/httpd/conf.d/rgw.conf\fP (RPM\-based distros) or
\fB/etc/apache2/conf\-available/rgw.conf\fP (Debian\-based distros) with localhost
tcp and through unix domain socket:
.INDENT 0.0
.IP 1. 3
For distros with Apache 2.2 and early versions of Apache 2.4 that use
localhost TCP and do not support Unix Domain Socket, append the following
contents to \fB/etc/ceph/ceph.conf\fP:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
[client.radosgw.gateway]
host = {hostname}
keyring = /etc/ceph/ceph.client.radosgw.keyring
rgw socket path = ""
log file = /var/log/ceph/client.radosgw.gateway.log
rgw frontends = fastcgi socket_port=9000 socket_host=0.0.0.0
rgw print continue = false
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 2. 3
Add the following content in the gateway configuration file:
.sp
For Debian/Ubuntu add in \fB/etc/apache2/conf\-available/rgw.conf\fP:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
<VirtualHost *:80>
ServerName localhost
DocumentRoot /var/www/html

ErrorLog /var/log/apache2/rgw_error.log
CustomLog /var/log/apache2/rgw_access.log combined

# LogLevel debug

RewriteEngine On

RewriteRule .* \- [E=HTTP_AUTHORIZATION:%{HTTP:Authorization},L]

SetEnv proxy\-nokeepalive 1

ProxyPass / fcgi://localhost:9000/

</VirtualHost>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
For CentOS/RHEL add in \fB/etc/httpd/conf.d/rgw.conf\fP:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
<VirtualHost *:80>
ServerName localhost
DocumentRoot /var/www/html

ErrorLog /var/log/httpd/rgw_error.log
CustomLog /var/log/httpd/rgw_access.log combined

# LogLevel debug

RewriteEngine On

RewriteRule .* \- [E=HTTP_AUTHORIZATION:%{HTTP:Authorization},L]

SetEnv proxy\-nokeepalive 1

ProxyPass / fcgi://localhost:9000/

</VirtualHost>
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 3. 3
For distros with Apache 2.4.9 or later that support Unix Domain Socket,
append the following configuration to \fB/etc/ceph/ceph.conf\fP:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
[client.radosgw.gateway]
host = {hostname}
keyring = /etc/ceph/ceph.client.radosgw.keyring
rgw socket path = /var/run/ceph/ceph.radosgw.gateway.fastcgi.sock
log file = /var/log/ceph/client.radosgw.gateway.log
rgw print continue = false
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 4. 3
Add the following content in the gateway configuration file:
.sp
For CentOS/RHEL add in \fB/etc/httpd/conf.d/rgw.conf\fP:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
<VirtualHost *:80>
ServerName localhost
DocumentRoot /var/www/html

ErrorLog /var/log/httpd/rgw_error.log
CustomLog /var/log/httpd/rgw_access.log combined

# LogLevel debug

RewriteEngine On

RewriteRule .* \- [E=HTTP_AUTHORIZATION:%{HTTP:Authorization},L]

SetEnv proxy\-nokeepalive 1

ProxyPass / unix:///var/run/ceph/ceph.radosgw.gateway.fastcgi.sock|fcgi://localhost:9000/

</VirtualHost>
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Please note, \fBApache 2.4.7\fP does not have Unix Domain Socket support in
it and as such it has to be configured with localhost tcp. The Unix Domain
Socket support is available in \fBApache 2.4.9\fP and later versions.
.IP 5. 3
Generate a key for radosgw to use for authentication with the cluster.
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
ceph\-authtool \-C \-n client.radosgw.gateway \-\-gen\-key /etc/ceph/keyring.radosgw.gateway
ceph\-authtool \-n client.radosgw.gateway \-\-cap mon \(aqallow rw\(aq \-\-cap osd \(aqallow rwx\(aq /etc/ceph/keyring.radosgw.gateway
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 6. 3
Add the key to the auth entries.
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
ceph auth add client.radosgw.gateway \-\-in\-file=keyring.radosgw.gateway
.ft P
.fi
.UNINDENT
.UNINDENT
.IP 7. 3
Start Apache and radosgw.
.sp
Debian/Ubuntu:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
sudo /etc/init.d/apache2 start
sudo /etc/init.d/radosgw start
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
CentOS/RHEL:
.INDENT 3.0
.INDENT 3.5
.sp
.nf
.ft C
sudo apachectl start
sudo /etc/init.d/ceph\-radosgw start
.ft P
.fi
.UNINDENT
.UNINDENT
.UNINDENT
.SH USAGE LOGGING
.sp
\fBradosgw\fP maintains an asynchronous usage log. It accumulates
statistics about user operations and flushes it periodically. The
logs can be accessed and managed through \fBradosgw\-admin\fP\&.
.sp
The information that is being logged contains total data transfer,
total operations, and total successful operations. The data is being
accounted in an hourly resolution under the bucket owner, unless the
operation was done on the service (e.g., when listing a bucket) in
which case it is accounted under the operating user.
.sp
Following is an example configuration:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
[client.radosgw.gateway]
    rgw enable usage log = true
    rgw usage log tick interval = 30
    rgw usage log flush threshold = 1024
    rgw usage max shards = 32
    rgw usage max user shards = 1
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
The total number of shards determines how many total objects hold the
usage log information. The per\-user number of shards specify how many
objects hold usage information for a single user. The tick interval
configures the number of seconds between log flushes, and the flush
threshold specify how many entries can be kept before resorting to
synchronous flush.
.SH AVAILABILITY
.sp
\fBradosgw\fP is part of Ceph, a massively scalable, open\-source, distributed
storage system. Please refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for
more information.
.SH SEE ALSO
.sp
ceph(8)
radosgw\-admin(8)
.SH SYNOPSIS
.nf
\fBrbd\-fuse\fP [ \-p pool ] [\-c conffile] \fImountpoint\fP [ \fIfuse options\fP ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbd\-fuse\fP is a FUSE (File system in USErspace) client for RADOS
block device (rbd) images.  Given a pool containing rbd images,
it will mount a userspace filesystem allowing access to those images
as regular files at \fBmountpoint\fP\&.
.sp
The file system can be unmounted with:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
fusermount \-u mountpoint
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
or by sending \fBSIGINT\fP to the \fBrbd\-fuse\fP process.
.SH OPTIONS
.sp
Any options not recognized by rbd\-fuse will be passed on to libfuse.
.INDENT 0.0
.TP
.B \-c ceph.conf
Use \fIceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-p pool
Use \fIpool\fP as the pool to search for rbd images.  Default is \fBrbd\fP\&.
.UNINDENT
.SH AVAILABILITY
.sp
\fBrbd\-fuse\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
fusermount(8),
rbd(8)
.SH SYNOPSIS
.nf
\fBrbd\-ggate\fP [\-\-read\-only] [\-\-exclusive] [\-\-device \fIggate device\fP] map \fIimage\-spec\fP | \fIsnap\-spec\fP
\fBrbd\-ggate\fP unmap \fIggate device\fP
\fBrbd\-ggate\fP list
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbd\-ggate\fP is a client for RADOS block device (rbd) images. It will
map a rbd image to a ggate (FreeBSD GEOM Gate class) device, allowing
access it as regular local block device.
.SH COMMANDS
.SS map
.sp
Spawn a process responsible for the creation of ggate device and
forwarding I/O requests between the GEOM Gate kernel subsystem and
RADOS.
.SS unmap
.sp
Destroy ggate device and terminate the process responsible for it.
.SS list
.sp
List mapped ggate devices.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-\-device *ggate device*
Specify ggate device path.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-read\-only
Map read\-only.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-exclusive
Forbid writes by other clients.
.UNINDENT
.SH IMAGE AND SNAP SPECS
.nf
\fIimage\-spec\fP is [\fIpool\-name\fP]/\fIimage\-name\fP
\fIsnap\-spec\fP  is [\fIpool\-name\fP]/\fIimage\-name\fP@\fIsnap\-name\fP
.fi
.sp
.sp
The default for \fIpool\-name\fP is "rbd".  If an image name contains a slash
character (\(aq/\(aq), \fIpool\-name\fP is required.
.SH AVAILABILITY
.sp
\fBrbd\-ggate\fP is part of Ceph, a massively scalable, open\-source,
distributed storage system. Please refer to the Ceph documentation at
\fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
rbd(8)
ceph(8)
.SH SYNOPSIS
.nf
\fBrbd\-mirror\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbd\-mirror\fP is a daemon for asynchronous mirroring of RADOS
block device (rbd) images among Ceph clusters. It replays changes to
images in remote clusters in a local cluster, for disaster recovery.
.sp
It connects to remote clusters via the RADOS protocol, relying on
default search paths to find ceph.conf files, monitor addresses and
authentication information for them, i.e. \fB/etc/ceph/$cluster.conf\fP,
\fB/etc/ceph/$cluster.keyring\fP, and
\fB/etc/ceph/$cluster.$name.keyring\fP, where \fB$cluster\fP is the
human\-friendly name of the cluster, and \fB$name\fP is the rados user to
connect as, e.g. \fBclient.rbd\-mirror\fP\&.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf=ceph.conf
Use \fBceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through \fBceph.conf\fP).
.UNINDENT
.INDENT 0.0
.TP
.B \-i ID, \-\-id ID
Set the ID portion of name for rbd\-mirror
.UNINDENT
.INDENT 0.0
.TP
.B \-n TYPE.ID, \-\-name TYPE.ID
Set the rados user name for the gateway (eg. client.rbd\-mirror)
.UNINDENT
.INDENT 0.0
.TP
.B \-\-cluster NAME
Set the cluster name (default: ceph)
.UNINDENT
.INDENT 0.0
.TP
.B \-d
Run in foreground, log to stderr
.UNINDENT
.INDENT 0.0
.TP
.B \-f
Run in foreground, log to usual location
.UNINDENT
.SH AVAILABILITY
.sp
\fBrbd\-mirror\fP is part of Ceph, a massively scalable, open\-source, distributed
storage system. Please refer to the Ceph documentation at \fI\%http://ceph.com/docs\fP for
more information.
.SH SEE ALSO
.sp
rbd(8)
.SH SYNOPSIS
.nf
\fBrbd\-nbd\fP [\-c conf] [\-\-read\-only] [\-\-device \fInbd device\fP] [\-\-nbds_max \fIlimit\fP] [\-\-max_part \fIlimit\fP] [\-\-exclusive] [\-\-timeout \fIseconds\fP] map \fIimage\-spec\fP | \fIsnap\-spec\fP
\fBrbd\-nbd\fP unmap \fInbd device\fP
\fBrbd\-nbd\fP list\-mapped
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbd\-nbd\fP is a client for RADOS block device (rbd) images like rbd kernel module.
It will map a rbd image to a nbd (Network Block Device) device, allowing access it
as regular local block device.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-c ceph.conf
Use \fIceph.conf\fP configuration file instead of the default
\fB/etc/ceph/ceph.conf\fP to determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-read\-only
Map read\-only.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-nbds_max *limit*
Override the parameter of NBD kernel module when modprobe, used to
limit the count of nbd device.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-max_part *limit*
Override for module param nbds_max.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-exclusive
Forbid writes by other clients.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-timeout *seconds*
Override device timeout. Linux kernel will default to a 30 second request timeout.
Allow the user to optionally specify an alternate timeout.
.UNINDENT
.SH IMAGE AND SNAP SPECS
.nf
\fIimage\-spec\fP is [\fIpool\-name\fP]/\fIimage\-name\fP
\fIsnap\-spec\fP  is [\fIpool\-name\fP]/\fIimage\-name\fP@\fIsnap\-name\fP
.fi
.sp
.sp
The default for \fIpool\-name\fP is "rbd".  If an image name contains a slash
character (\(aq/\(aq), \fIpool\-name\fP is required.
.SH AVAILABILITY
.sp
\fBrbd\-nbd\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
rbd(8)
.SH SYNOPSIS
.nf
\fBrbd\-replay\-many\fP [ \fIoptions\fP ] \-\-original\-image \fIname\fP \fIhost1\fP [ \fIhost2\fP [ ... ] ] \-\- \fIrbd_replay_args\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbd\-replay\-many\fP is a utility for replaying a rados block device (RBD) workload on several clients.
Although all clients use the same workload, they replay against separate images.
This matches normal use of librbd, where each original client is a VM with its own image.
.sp
Configuration and replay files are not automatically copied to clients.
Replay images must already exist.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-\-original\-image name
Specifies the name (and snap) of the originally traced image.
Necessary for correct name mapping.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-image\-prefix prefix
Prefix of image names to replay against.
Specifying \-\-image\-prefix=foo results in clients replaying against foo\-0, foo\-1, etc.
Defaults to the original image name.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-exec program
Path to the rbd\-replay executable.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-delay seconds
Delay between starting each client.  Defaults to 0.
.UNINDENT
.SH EXAMPLES
.sp
Typical usage:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd\-replay\-many host\-0 host\-1 \-\-original\-image=image \-\- \-c ceph.conf replay.bin
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This results in the following commands being executed:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
ssh host\-0 \(aqrbd\-replay\(aq \-\-map\-image \(aqimage=image\-0\(aq \-c ceph.conf replay.bin
ssh host\-1 \(aqrbd\-replay\(aq \-\-map\-image \(aqimage=image\-1\(aq \-c ceph.conf replay.bin
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBrbd\-replay\-many\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
rbd\-replay(8),
rbd(8)
.SH SYNOPSIS
.nf
\fBrbd\-replay\-prep\fP [ \-\-window \fIseconds\fP ] [ \-\-anonymize ] \fItrace_dir\fP \fIreplay_file\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbd\-replay\-prep\fP processes raw rados block device (RBD) traces to prepare them for \fBrbd\-replay\fP\&.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-\-window seconds
Requests further apart than \(aqseconds\(aq seconds are assumed to be independent.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-anonymize
Anonymizes image and snap names.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-verbose
Print all processed events to console
.UNINDENT
.SH EXAMPLES
.sp
To prepare workload1\-trace for replay:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd\-replay\-prep workload1\-trace/ust/uid/1000/64\-bit workload1
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBrbd\-replay\-prep\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
rbd\-replay(8),
rbd(8)
.SH SYNOPSIS
.nf
\fBrbd\-replay\fP [ \fIoptions\fP ] \fIreplay_file\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbd\-replay\fP is a utility for replaying rados block device (RBD) workloads.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf ceph.conf
Use ceph.conf configuration file instead of the default /etc/ceph/ceph.conf to
determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-p pool, \-\-pool pool
Interact with the given pool.  Defaults to \(aqrbd\(aq.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-latency\-multiplier
Multiplies inter\-request latencies.  Default: 1.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-read\-only
Only replay non\-destructive requests.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-map\-image rule
Add a rule to map image names in the trace to image names in the replay cluster.
A rule of image1@snap1=image2@snap2 would map snap1 of image1 to snap2 of image2.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-dump\-perf\-counters
\fBExperimental\fP
Dump performance counters to standard out before an image is closed.
Performance counters may be dumped multiple times if multiple images are closed,
or if the same image is opened and closed multiple times.
Performance counters and their meaning may change between versions.
.UNINDENT
.SH EXAMPLES
.sp
To replay workload1 as fast as possible:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd\-replay \-\-latency\-multiplier=0 workload1
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To replay workload1 but use test_image instead of prod_image:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd\-replay \-\-map\-image=prod_image=test_image workload1
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBrbd\-replay\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
rbd\-replay\-prep(8),
rbd(8)
.SH SYNOPSIS
.nf
\fBrbd\fP [ \-c \fIceph.conf\fP ] [ \-m \fImonaddr\fP ] [\-\-cluster \fIcluster\-name\fP]
[ \-p | \-\-pool \fIpool\fP ] [ \fIcommand\fP ... ]
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbd\fP is a utility for manipulating rados block device (RBD) images,
used by the Linux rbd driver and the rbd storage driver for QEMU/KVM.
RBD images are simple block devices that are striped over objects and
stored in a RADOS object store. The size of the objects the image is
striped over must be a power of two.
.SH OPTIONS
.INDENT 0.0
.TP
.B \-c ceph.conf, \-\-conf ceph.conf
Use ceph.conf configuration file instead of the default /etc/ceph/ceph.conf to
determine monitor addresses during startup.
.UNINDENT
.INDENT 0.0
.TP
.B \-m monaddress[:port]
Connect to specified monitor (instead of looking through ceph.conf).
.UNINDENT
.INDENT 0.0
.TP
.B \-\-cluster cluster\-name
Use different cluster name as compared to default cluster name \fIceph\fP\&.
.UNINDENT
.INDENT 0.0
.TP
.B \-p pool\-name, \-\-pool pool\-name
Interact with the given pool. Required by most commands.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-namespace namespace\-name
Use a pre\-defined image namespace within a pool
.UNINDENT
.INDENT 0.0
.TP
.B \-\-no\-progress
Do not output progress information (goes to standard error by
default for some commands).
.UNINDENT
.SH PARAMETERS
.INDENT 0.0
.TP
.B \-\-image\-format format\-id
Specifies which object layout to use. The default is 2.
.INDENT 7.0
.IP \(bu 2
format 1 \- (deprecated) Use the original format for a new rbd image. This
format is understood by all versions of librbd and the kernel rbd module,
but does not support newer features like cloning.
.IP \(bu 2
format 2 \- Use the second rbd format, which is supported by
librbd and kernel since version 3.11 (except for striping). This adds
support for cloning and is more easily extensible to allow more
features in the future.
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \-s size\-in\-M/G/T, \-\-size size\-in\-M/G/T
Specifies the size of the new rbd image or the new size of the existing rbd
image in M/G/T.  If no suffix is given, unit M is assumed.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-object\-size size\-in\-B/K/M
Specifies the object size in B/K/M.  Object size will be rounded up the
nearest power of two; if no suffix is given, unit B is assumed.  The default
object size is 4M, smallest is 4K and maximum is 32M.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-stripe\-unit size\-in\-B/K/M
Specifies the stripe unit size in B/K/M.  If no suffix is given, unit B is
assumed.  See striping section (below) for more details.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-stripe\-count num
Specifies the number of objects to stripe over before looping back
to the first object.  See striping section (below) for more details.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-snap snap
Specifies the snapshot name for the specific operation.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-id username
Specifies the username (without the \fBclient.\fP prefix) to use with the map command.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-keyring filename
Specifies a keyring file containing a secret for the specified user
to use with the map command.  If not specified, the default keyring
locations will be searched.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-keyfile filename
Specifies a file containing the secret key of \fB\-\-id user\fP to use with the map command.
This option is overridden by \fB\-\-keyring\fP if the latter is also specified.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-shared lock\-tag
Option for \fIlock add\fP that allows multiple clients to lock the
same image if they use the same tag. The tag is an arbitrary
string. This is useful for situations where an image must
be open from more than one client at once, like during
live migration of a virtual machine, or for use underneath
a clustered filesystem.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-format format
Specifies output formatting (default: plain, json, xml)
.UNINDENT
.INDENT 0.0
.TP
.B \-\-pretty\-format
Make json or xml formatted output more human\-readable.
.UNINDENT
.INDENT 0.0
.TP
.B \-o krbd\-options, \-\-options krbd\-options
Specifies which options to use when mapping or unmapping an image via the
rbd kernel driver.  krbd\-options is a comma\-separated list of options
(similar to mount(8) mount options).  See kernel rbd (krbd) options section
below for more details.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-read\-only
Map the image read\-only.  Equivalent to \-o ro.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-image\-feature feature\-name
Specifies which RBD format 2 feature should be enabled when creating
an image. Multiple features can be enabled by repeating this option
multiple times. The following features are supported:
.INDENT 7.0
.IP \(bu 2
layering: layering support
.IP \(bu 2
striping: striping v2 support
.IP \(bu 2
exclusive\-lock: exclusive locking support
.IP \(bu 2
object\-map: object map support (requires exclusive\-lock)
.IP \(bu 2
fast\-diff: fast diff calculations (requires object\-map)
.IP \(bu 2
deep\-flatten: snapshot flatten support
.IP \(bu 2
journaling: journaled IO support (requires exclusive\-lock)
.IP \(bu 2
data\-pool: erasure coded pool support
.UNINDENT
.UNINDENT
.INDENT 0.0
.TP
.B \-\-image\-shared
Specifies that the image will be used concurrently by multiple clients.
This will disable features that are dependent upon exclusive ownership
of the image.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-whole\-object
Specifies that the diff should be limited to the extents of a full object
instead of showing intra\-object deltas. When the object map feature is
enabled on an image, limiting the diff to the object extents will
dramatically improve performance since the differences can be computed
by examining the in\-memory object map instead of querying RADOS for each
object within the image.
.UNINDENT
.INDENT 0.0
.TP
.B \-\-limit
Specifies the limit for the number of snapshots permitted.
.UNINDENT
.SH COMMANDS
.INDENT 0.0
.TP
\fBbench\fP \-\-io\-type <read | write | readwrite | rw> [\-\-io\-size \fIsize\-in\-B/K/M/G/T\fP] [\-\-io\-threads \fInum\-ios\-in\-flight\fP] [\-\-io\-total \fIsize\-in\-B/K/M/G/T\fP] [\-\-io\-pattern seq | rand] [\-\-rw\-mix\-read \fIread proportion in readwrite\fP] \fIimage\-spec\fP
Generate a series of IOs to the image and measure the IO throughput and
latency.  If no suffix is given, unit B is assumed for both \-\-io\-size and
\-\-io\-total.  Defaults are: \-\-io\-size 4096, \-\-io\-threads 16, \-\-io\-total 1G,
\-\-io\-pattern seq, \-\-rw\-mix\-read 50.
.TP
\fBchildren\fP \fIsnap\-spec\fP
List the clones of the image at the given snapshot. This checks
every pool, and outputs the resulting poolname/imagename.
.sp
This requires image format 2.
.TP
\fBclone\fP [\-\-object\-size \fIsize\-in\-B/K/M\fP] [\-\-stripe\-unit \fIsize\-in\-B/K/M\fP \-\-stripe\-count \fInum\fP] [\-\-image\-feature \fIfeature\-name\fP] [\-\-image\-shared] \fIparent\-snap\-spec\fP \fIchild\-image\-spec\fP
Will create a clone (copy\-on\-write child) of the parent snapshot.
Object size will be identical to that of the parent image unless
specified. Size will be the same as the parent snapshot. The \-\-stripe\-unit
and \-\-stripe\-count arguments are optional, but must be used together.
.sp
The parent snapshot must be protected (see \fIrbd snap protect\fP).
This requires image format 2.
.TP
\fBconfig global get\fP \fIconfig\-entity\fP \fIkey\fP
Get a global\-level configuration override.
.TP
\fBconfig global list\fP [\-\-format plain | json | xml] [\-\-pretty\-format] \fIconfig\-entity\fP
List global\-level configuration overrides.
.TP
\fBconfig global set\fP \fIconfig\-entity\fP \fIkey\fP \fIvalue\fP
Set a global\-level configuration override.
.TP
\fBconfig global remove\fP \fIconfig\-entity\fP \fIkey\fP
Remove a global\-level configuration override.
.TP
\fBconfig image get\fP \fIimage\-spec\fP \fIkey\fP
Get an image\-level configuration override.
.TP
\fBconfig image list\fP [\-\-format plain | json | xml] [\-\-pretty\-format] \fIimage\-spec\fP
List image\-level configuration overrides.
.TP
\fBconfig image set\fP \fIimage\-spec\fP \fIkey\fP \fIvalue\fP
Set an image\-level configuration override.
.TP
\fBconfig image remove\fP \fIimage\-spec\fP \fIkey\fP
Remove an image\-level configuration override.
.TP
\fBconfig pool get\fP \fIpool\-name\fP \fIkey\fP
Get a pool\-level configuration override.
.TP
\fBconfig pool list\fP [\-\-format plain | json | xml] [\-\-pretty\-format] \fIpool\-name\fP
List pool\-level configuration overrides.
.TP
\fBconfig pool set\fP \fIpool\-name\fP \fIkey\fP \fIvalue\fP
Set a pool\-level configuration override.
.TP
\fBconfig pool remove\fP \fIpool\-name\fP \fIkey\fP
Remove a pool\-level configuration override.
.TP
\fBcp\fP (\fIsrc\-image\-spec\fP | \fIsrc\-snap\-spec\fP) \fIdest\-image\-spec\fP
Copy the content of a src\-image into the newly created dest\-image.
dest\-image will have the same size, object size, and image format as src\-image.
.TP
\fBcreate\fP (\-s | \-\-size \fIsize\-in\-M/G/T\fP) [\-\-image\-format \fIformat\-id\fP] [\-\-object\-size \fIsize\-in\-B/K/M\fP] [\-\-stripe\-unit \fIsize\-in\-B/K/M\fP \-\-stripe\-count \fInum\fP] [\-\-thick\-provision] [\-\-no\-progress] [\-\-image\-feature \fIfeature\-name\fP]... [\-\-image\-shared] \fIimage\-spec\fP
Will create a new rbd image. You must also specify the size via \-\-size.  The
\-\-stripe\-unit and \-\-stripe\-count arguments are optional, but must be used together.
If the \-\-thick\-provision is enabled, it will fully allocate storage for
the image at creation time. It will take a long time to do.
Note: thick provisioning requires zeroing the contents of the entire image.
.TP
\fBdeep cp\fP (\fIsrc\-image\-spec\fP | \fIsrc\-snap\-spec\fP) \fIdest\-image\-spec\fP
Deep copy the content of a src\-image into the newly created dest\-image.
Dest\-image will have the same size, object size, image format, and snapshots as src\-image.
.TP
\fBdevice list\fP [\-t | \-\-device\-type \fIdevice\-type\fP] [\-\-format plain | json | xml] \-\-pretty\-format
Show the rbd images that are mapped via the rbd kernel module
(default) or other supported device.
.TP
\fBdevice map\fP [\-t | \-\-device\-type \fIdevice\-type\fP] [\-\-read\-only] [\-\-exclusive] [\-o | \-\-options \fIdevice\-options\fP] \fIimage\-spec\fP | \fIsnap\-spec\fP
Map the specified image to a block device via the rbd kernel module
(default) or other supported device (\fInbd\fP on Linux or \fIggate\fP on
FreeBSD).
.sp
The \-\-options argument is a comma separated list of device type
specific options (opt1,opt2=val,...).
.TP
\fBdevice unmap\fP [\-t | \-\-device\-type \fIdevice\-type\fP] [\-o | \-\-options \fIdevice\-options\fP] \fIimage\-spec\fP | \fIsnap\-spec\fP | \fIdevice\-path\fP
Unmap the block device that was mapped via the rbd kernel module
(default) or other supported device.
.sp
The \-\-options argument is a comma separated list of device type
specific options (opt1,opt2=val,...).
.TP
\fBdiff\fP [\-\-from\-snap \fIsnap\-name\fP] [\-\-whole\-object] \fIimage\-spec\fP | \fIsnap\-spec\fP
Dump a list of byte extents in the image that have changed since the specified start
snapshot, or since the image was created.  Each output line includes the starting offset
(in bytes), the length of the region (in bytes), and either \(aqzero\(aq or \(aqdata\(aq to indicate
whether the region is known to be zeros or may contain other data.
.TP
\fBdu\fP [\-p | \-\-pool \fIpool\-name\fP] [\fIimage\-spec\fP | \fIsnap\-spec\fP]
Will calculate the provisioned and actual disk usage of all images and
associated snapshots within the specified pool.  It can also be used against
individual images and snapshots.
.sp
If the RBD fast\-diff feature is not enabled on images, this operation will
require querying the OSDs for every potential object within the image.
.TP
\fBexport\fP [\-\-export\-format \fIformat (1 or 2)\fP] (\fIimage\-spec\fP | \fIsnap\-spec\fP) [\fIdest\-path\fP]
Export image to dest path (use \- for stdout).
The \-\-export\-format accepts \(aq1\(aq or \(aq2\(aq currently. Format 2 allow us to export not only the content
of image, but also the snapshots and other properties, such as image_order, features.
.TP
\fBexport\-diff\fP [\-\-from\-snap \fIsnap\-name\fP] [\-\-whole\-object] (\fIimage\-spec\fP | \fIsnap\-spec\fP) \fIdest\-path\fP
Export an incremental diff for an image to dest path (use \- for stdout).  If
an initial snapshot is specified, only changes since that snapshot are included; otherwise,
any regions of the image that contain data are included.  The end snapshot is specified
using the standard \-\-snap option or @snap syntax (see below).  The image diff format includes
metadata about image size changes, and the start and end snapshots.  It efficiently represents
discarded or \(aqzero\(aq regions of the image.
.TP
\fBfeature disable\fP \fIimage\-spec\fP \fIfeature\-name\fP\&...
Disable the specified feature on the specified image. Multiple features can
be specified.
.TP
\fBfeature enable\fP \fIimage\-spec\fP \fIfeature\-name\fP\&...
Enable the specified feature on the specified image. Multiple features can
be specified.
.TP
\fBflatten\fP \fIimage\-spec\fP
If image is a clone, copy all shared blocks from the parent snapshot and
make the child independent of the parent, severing the link between
parent snap and child.  The parent snapshot can be unprotected and
deleted if it has no further dependent clones.
.sp
This requires image format 2.
.TP
\fBgroup create\fP \fIgroup\-spec\fP
Create a group.
.TP
\fBgroup image add\fP \fIgroup\-spec\fP \fIimage\-spec\fP
Add an image to a group.
.TP
\fBgroup image list\fP \fIgroup\-spec\fP
List images in a group.
.TP
\fBgroup image remove\fP \fIgroup\-spec\fP \fIimage\-spec\fP
Remove an image from a group.
.TP
\fBgroup ls\fP [\-p | \-\-pool \fIpool\-name\fP]
List rbd groups.
.TP
\fBgroup rename\fP \fIsrc\-group\-spec\fP \fIdest\-group\-spec\fP
Rename a group.  Note: rename across pools is not supported.
.TP
\fBgroup rm\fP \fIgroup\-spec\fP
Delete a group.
.TP
\fBgroup snap create\fP \fIgroup\-snap\-spec\fP
Make a snapshot of a group.
.TP
\fBgroup snap list\fP \fIgroup\-spec\fP
List snapshots of a group.
.TP
\fBgroup snap rm\fP \fIgroup\-snap\-spec\fP
Remove a snapshot from a group.
.TP
\fBgroup snap rename\fP \fIgroup\-snap\-spec\fP \fIsnap\-name\fP
Rename group\(aqs snapshot.
.TP
\fBgroup snap rollback\fP \fIgroup\-snap\-spec\fP
Rollback group to snapshot.
.TP
\fBimage\-meta get\fP \fIimage\-spec\fP \fIkey\fP
Get metadata value with the key.
.TP
\fBimage\-meta list\fP \fIimage\-spec\fP
Show metadata held on the image. The first column is the key
and the second column is the value.
.TP
\fBimage\-meta remove\fP \fIimage\-spec\fP \fIkey\fP
Remove metadata key with the value.
.TP
\fBimage\-meta set\fP \fIimage\-spec\fP \fIkey\fP \fIvalue\fP
Set metadata key with the value. They will displayed in \fIimage\-meta list\fP\&.
.TP
\fBimport\fP [\-\-export\-format \fIformat (1 or 2)\fP] [\-\-image\-format \fIformat\-id\fP] [\-\-object\-size \fIsize\-in\-B/K/M\fP] [\-\-stripe\-unit \fIsize\-in\-B/K/M\fP \-\-stripe\-count \fInum\fP] [\-\-image\-feature \fIfeature\-name\fP]... [\-\-image\-shared] \fIsrc\-path\fP [\fIimage\-spec\fP]
Create a new image and imports its data from path (use \- for
stdin).  The import operation will try to create sparse rbd images
if possible.  For import from stdin, the sparsification unit is
the data block size of the destination image (object size).
.sp
The \-\-stripe\-unit and \-\-stripe\-count arguments are optional, but must be
used together.
.sp
The \-\-export\-format accepts \(aq1\(aq or \(aq2\(aq currently. Format 2 allow us to import not only the content
of image, but also the snapshots and other properties, such as image_order, features.
.TP
\fBimport\-diff\fP \fIsrc\-path\fP \fIimage\-spec\fP
Import an incremental diff of an image and applies it to the current image.  If the diff
was generated relative to a start snapshot, we verify that snapshot already exists before
continuing.  If there was an end snapshot we verify it does not already exist before
applying the changes, and create the snapshot when we are done.
.TP
\fBinfo\fP \fIimage\-spec\fP | \fIsnap\-spec\fP
Will dump information (such as size and object size) about a specific rbd image.
If image is a clone, information about its parent is also displayed.
If a snapshot is specified, whether it is protected is shown as well.
.TP
\fBjournal client disconnect\fP \fIjournal\-spec\fP
Flag image journal client as disconnected.
.TP
\fBjournal export\fP [\-\-verbose] [\-\-no\-error] \fIsrc\-journal\-spec\fP \fIpath\-name\fP
Export image journal to path (use \- for stdout). It can be make a backup
of the image journal especially before attempting dangerous operations.
.sp
Note that this command may not always work if the journal is badly corrupted.
.TP
\fBjournal import\fP [\-\-verbose] [\-\-no\-error] \fIpath\-name\fP \fIdest\-journal\-spec\fP
Import image journal from path (use \- for stdin).
.TP
\fBjournal info\fP \fIjournal\-spec\fP
Show information about image journal.
.TP
\fBjournal inspect\fP [\-\-verbose] \fIjournal\-spec\fP
Inspect and report image journal for structural errors.
.TP
\fBjournal reset\fP \fIjournal\-spec\fP
Reset image journal.
.TP
\fBjournal status\fP \fIjournal\-spec\fP
Show status of image journal.
.TP
\fBlock add\fP [\-\-shared \fIlock\-tag\fP] \fIimage\-spec\fP \fIlock\-id\fP
Lock an image. The lock\-id is an arbitrary name for the user\(aqs
convenience. By default, this is an exclusive lock, meaning it
will fail if the image is already locked. The \-\-shared option
changes this behavior. Note that locking does not affect
any operation other than adding a lock. It does not
protect an image from being deleted.
.TP
\fBlock ls\fP \fIimage\-spec\fP
Show locks held on the image. The first column is the locker
to use with the \fIlock remove\fP command.
.TP
\fBlock rm\fP \fIimage\-spec\fP \fIlock\-id\fP \fIlocker\fP
Release a lock on an image. The lock id and locker are
as output by lock ls.
.TP
\fBls\fP [\-l | \-\-long] [\fIpool\-name\fP]
Will list all rbd images listed in the rbd_directory object.  With
\-l, also show snapshots, and use longer\-format output including
size, parent (if clone), format, etc.
.TP
\fBmerge\-diff\fP \fIfirst\-diff\-path\fP \fIsecond\-diff\-path\fP \fImerged\-diff\-path\fP
Merge two continuous incremental diffs of an image into one single diff. The
first diff\(aqs end snapshot must be equal with the second diff\(aqs start snapshot.
The first diff could be \- for stdin, and merged diff could be \- for stdout, which
enables multiple diff files to be merged using something like
\(aqrbd merge\-diff first second \- | rbd merge\-diff \- third result\(aq. Note this command
currently only support the source incremental diff with stripe_count == 1
.TP
\fBmigration abort\fP \fIimage\-spec\fP
Cancel image migration. This step may be run after successful or
failed migration prepare or migration execute steps and returns the
image to its initial (before migration) state. All modifications to
the destination image are lost.
.TP
\fBmigration commit\fP \fIimage\-spec\fP
Commit image migration. This step is run after a successful migration
prepare and migration execute steps and removes the source image data.
.TP
\fBmigration execute\fP \fIimage\-spec\fP
Execute image migration. This step is run after a successful migration
prepare step and copies image data to the destination.
.TP
\fBmigration prepare\fP [\-\-order \fIorder\fP] [\-\-object\-size \fIobject\-size\fP] [\-\-image\-feature \fIimage\-feature\fP] [\-\-image\-shared] [\-\-stripe\-unit \fIstripe\-unit\fP] [\-\-stripe\-count \fIstripe\-count\fP] [\-\-data\-pool \fIdata\-pool\fP] \fIsrc\-image\-spec\fP [\fIdest\-image\-spec\fP]
Prepare image migration. This is the first step when migrating an
image, i.e. changing the image location, format or other
parameters that can\(aqt be changed dynamically. The destination can
match the source, and in this case \fIdest\-image\-spec\fP can be omitted.
After this step the source image is set as a parent of the
destination image, and the image is accessible in copy\-on\-write mode
by its destination spec.
.TP
\fBmirror image demote\fP \fIimage\-spec\fP
Demote a primary image to non\-primary for RBD mirroring.
.TP
\fBmirror image disable\fP [\-\-force] \fIimage\-spec\fP
Disable RBD mirroring for an image. If the mirroring is
configured in \fBimage\fP mode for the image\(aqs pool, then it
can be explicitly disabled mirroring for each image within
the pool.
.TP
\fBmirror image enable\fP \fIimage\-spec\fP
Enable RBD mirroring for an image. If the mirroring is
configured in \fBimage\fP mode for the image\(aqs pool, then it
can be explicitly enabled mirroring for each image within
the pool.
.sp
This requires the RBD journaling feature is enabled.
.TP
\fBmirror image promote\fP [\-\-force] \fIimage\-spec\fP
Promote a non\-primary image to primary for RBD mirroring.
.TP
\fBmirror image resync\fP \fIimage\-spec\fP
Force resync to primary image for RBD mirroring.
.TP
\fBmirror image status\fP \fIimage\-spec\fP
Show RBD mirroring status for an image.
.TP
\fBmirror pool demote\fP [\fIpool\-name\fP]
Demote all primary images within a pool to non\-primary.
Every mirroring enabled image will demoted in the pool.
.TP
\fBmirror pool disable\fP [\fIpool\-name\fP]
Disable RBD mirroring by default within a pool. When mirroring
is disabled on a pool in this way, mirroring will also be
disabled on any images (within the pool) for which mirroring
was enabled explicitly.
.TP
\fBmirror pool enable\fP [\fIpool\-name\fP] \fImode\fP
Enable RBD mirroring by default within a pool.
The mirroring mode can either be \fBpool\fP or \fBimage\fP\&.
If configured in \fBpool\fP mode, all images in the pool
with the journaling feature enabled are mirrored.
If configured in \fBimage\fP mode, mirroring needs to be
explicitly enabled (by \fBmirror image enable\fP command)
on each image.
.TP
\fBmirror pool info\fP [\fIpool\-name\fP]
Show information about the pool mirroring configuration.
It includes mirroring mode, peer UUID, remote cluster name,
and remote client name.
.TP
\fBmirror pool peer add\fP [\fIpool\-name\fP] \fIremote\-cluster\-spec\fP
Add a mirroring peer to a pool.
\fIremote\-cluster\-spec\fP is [\fIremote client name\fP@]\fIremote cluster name\fP\&.
.sp
The default for \fIremote client name\fP is "client.admin".
.sp
This requires mirroring mode is enabled.
.TP
\fBmirror pool peer remove\fP [\fIpool\-name\fP] \fIuuid\fP
Remove a mirroring peer from a pool. The peer uuid is available
from \fBmirror pool info\fP command.
.TP
\fBmirror pool peer set\fP [\fIpool\-name\fP] \fIuuid\fP \fIkey\fP \fIvalue\fP
Update mirroring peer settings.
The key can be either \fBclient\fP or \fBcluster\fP, and the value
is corresponding to remote client name or remote cluster name.
.TP
\fBmirror pool promote\fP [\-\-force] [\fIpool\-name\fP]
Promote all non\-primary images within a pool to primary.
Every mirroring enabled image will promoted in the pool.
.TP
\fBmirror pool status\fP [\-\-verbose] [\fIpool\-name\fP]
Show status for all mirrored images in the pool.
With \-\-verbose, also show additionally output status
details for every mirroring image in the pool.
.TP
\fBmv\fP \fIsrc\-image\-spec\fP \fIdest\-image\-spec\fP
Rename an image.  Note: rename across pools is not supported.
.TP
\fBnamespace create\fP \fIpool\-name\fP \fInamespace\-name\fP
Create a new image namespace within the pool.
.TP
\fBnamespace list\fP \fIpool\-name\fP
List image namespaces defined within the pool.
.TP
\fBnamespace remove\fP \fIpool\-name\fP \fInamespace\-name\fP
Remove an empty image namespace from the pool.
.TP
\fBobject\-map check\fP \fIimage\-spec\fP | \fIsnap\-spec\fP
Verify the object map is correct.
.TP
\fBobject\-map rebuild\fP \fIimage\-spec\fP | \fIsnap\-spec\fP
Rebuild an invalid object map for the specified image. An image snapshot can be
specified to rebuild an invalid object map for a snapshot.
.TP
\fBpool init\fP [\fIpool\-name\fP] [\-\-force]
Initialize pool for use by RBD. Newly created pools must initialized
prior to use.
.TP
\fBresize\fP (\-s | \-\-size \fIsize\-in\-M/G/T\fP) [\-\-allow\-shrink] \fIimage\-spec\fP
Resize rbd image. The size parameter also needs to be specified.
The \-\-allow\-shrink option lets the size be reduced.
.TP
\fBrm\fP \fIimage\-spec\fP
Delete an rbd image (including all data blocks). If the image has
snapshots, this fails and nothing is deleted.
.TP
\fBsnap create\fP \fIsnap\-spec\fP
Create a new snapshot. Requires the snapshot name parameter specified.
.TP
\fBsnap limit clear\fP \fIimage\-spec\fP
Remove any previously set limit on the number of snapshots allowed on
an image.
.TP
\fBsnap limit set\fP [\-\-limit] \fIlimit\fP \fIimage\-spec\fP
Set a limit for the number of snapshots allowed on an image.
.TP
\fBsnap ls\fP \fIimage\-spec\fP
Dump the list of snapshots inside a specific image.
.TP
\fBsnap protect\fP \fIsnap\-spec\fP
Protect a snapshot from deletion, so that clones can be made of it
(see \fIrbd clone\fP).  Snapshots must be protected before clones are made;
protection implies that there exist dependent cloned children that
refer to this snapshot.  \fIrbd clone\fP will fail on a nonprotected
snapshot.
.sp
This requires image format 2.
.TP
\fBsnap purge\fP \fIimage\-spec\fP
Remove all unprotected snapshots from an image.
.TP
\fBsnap rename\fP \fIsrc\-snap\-spec\fP \fIdest\-snap\-spec\fP
Rename a snapshot. Note: rename across pools and images is not supported.
.TP
\fBsnap rm\fP [\-\-force] \fIsnap\-spec\fP
Remove the specified snapshot.
.TP
\fBsnap rollback\fP \fIsnap\-spec\fP
Rollback image content to snapshot. This will iterate through the entire blocks
array and update the data head content to the snapshotted version.
.TP
\fBsnap unprotect\fP \fIsnap\-spec\fP
Unprotect a snapshot from deletion (undo \fIsnap protect\fP).  If cloned
children remain, \fIsnap unprotect\fP fails.  (Note that clones may exist
in different pools than the parent snapshot.)
.sp
This requires image format 2.
.TP
\fBstatus\fP \fIimage\-spec\fP
Show the status of the image, including which clients have it open.
.TP
\fBtrash ls\fP [\fIpool\-name\fP]
List all entries from trash.
.TP
\fBtrash mv\fP \fIimage\-spec\fP
Move an image to the trash. Images, even ones actively in\-use by
clones, can be moved to the trash and deleted at a later time.
.TP
\fBtrash purge\fP [\fIpool\-name\fP]
Remove all expired images from trash.
.TP
\fBtrash restore\fP \fIimage\-id\fP
Restore an image from trash.
.TP
\fBtrash rm\fP \fIimage\-id\fP
Delete an image from trash. If image deferment time has not expired
you can not removed it unless use force. But an actively in\-use by clones
or has snapshots can not be removed.
.TP
\fBwatch\fP \fIimage\-spec\fP
Watch events on image.
.UNINDENT
.SH IMAGE, SNAP, GROUP AND JOURNAL SPECS
.nf
\fIimage\-spec\fP      is [\fIpool\-name\fP/[\fInamespace\-name\fP/]]\fIimage\-name\fP
\fIsnap\-spec\fP       is [\fIpool\-name\fP/[\fInamespace\-name\fP/]]\fIimage\-name\fP@\fIsnap\-name\fP
\fIgroup\-spec\fP      is [\fIpool\-name\fP/[\fInamespace\-name\fP/]]\fIgroup\-name\fP
\fIgroup\-snap\-spec\fP is [\fIpool\-name\fP/[\fInamespace\-name\fP/]]\fIgroup\-name\fP@\fIsnap\-name\fP
\fIjournal\-spec\fP    is [\fIpool\-name\fP/[\fInamespace\-name\fP/]]\fIjournal\-name\fP
.fi
.sp
.sp
The default for \fIpool\-name\fP is "rbd" and \fInamespace\-name\fP is "". If an image
name contains a slash character (\(aq/\(aq), \fIpool\-name\fP is required.
.sp
The \fIjournal\-name\fP is \fIimage\-id\fP\&.
.sp
You may specify each name individually, using \-\-pool, \-\-namespace, \-\-image, and
\-\-snap options, but this is discouraged in favor of the above spec syntax.
.SH STRIPING
.sp
RBD images are striped over many objects, which are then stored by the
Ceph distributed object store (RADOS).  As a result, read and write
requests for the image are distributed across many nodes in the
cluster, generally preventing any single node from becoming a
bottleneck when individual images get large or busy.
.sp
The striping is controlled by three parameters:
.INDENT 0.0
.TP
.B object\-size
The size of objects we stripe over is a power of two. It will be rounded up the nearest power of two.
The default object size is 4 MB, smallest is 4K and maximum is 32M.
.UNINDENT
.INDENT 0.0
.TP
.B stripe_unit
Each [\fIstripe_unit\fP] contiguous bytes are stored adjacently in the same object, before we move on
to the next object.
.UNINDENT
.INDENT 0.0
.TP
.B stripe_count
After we write [\fIstripe_unit\fP] bytes to [\fIstripe_count\fP] objects, we loop back to the initial object
and write another stripe, until the object reaches its maximum size.  At that point,
we move on to the next [\fIstripe_count\fP] objects.
.UNINDENT
.sp
By default, [\fIstripe_unit\fP] is the same as the object size and [\fIstripe_count\fP] is 1.  Specifying a different
[\fIstripe_unit\fP] requires that the STRIPINGV2 feature be supported (added in Ceph v0.53) and format 2 images be
used.
.SH KERNEL RBD (KRBD) OPTIONS
.sp
Most of these options are useful mainly for debugging and benchmarking.  The
default values are set in the kernel and may therefore depend on the version of
the running kernel.
.sp
Per client instance \fIrbd device map\fP options:
.INDENT 0.0
.IP \(bu 2
fsid=aaaaaaaa\-bbbb\-cccc\-dddd\-eeeeeeeeeeee \- FSID that should be assumed by
the client.
.IP \(bu 2
ip=a.b.c.d[:p] \- IP and, optionally, port the client should use.
.IP \(bu 2
share \- Enable sharing of client instances with other mappings (default).
.IP \(bu 2
noshare \- Disable sharing of client instances with other mappings.
.IP \(bu 2
crc \- Enable CRC32C checksumming for data writes (default).
.IP \(bu 2
nocrc \- Disable CRC32C checksumming for data writes.
.IP \(bu 2
cephx_require_signatures \- Require cephx message signing (since 3.19,
default).
.IP \(bu 2
nocephx_require_signatures \- Don\(aqt require cephx message signing (since
3.19).
.IP \(bu 2
tcp_nodelay \- Disable Nagle\(aqs algorithm on client sockets (since 4.0,
default).
.IP \(bu 2
notcp_nodelay \- Enable Nagle\(aqs algorithm on client sockets (since 4.0).
.IP \(bu 2
cephx_sign_messages \- Enable message signing (since 4.4, default).
.IP \(bu 2
nocephx_sign_messages \- Disable message signing (since 4.4).
.IP \(bu 2
mount_timeout=x \- A timeout on various steps in \fIrbd device map\fP and
\fIrbd device unmap\fP sequences (default is 60 seconds).  In particular,
since 4.2 this can be used to ensure that \fIrbd device unmap\fP eventually
times out when there is no network connection to a cluster.
.IP \(bu 2
osdkeepalive=x \- OSD keepalive timeout (default is 5 seconds).
.IP \(bu 2
osd_idle_ttl=x \- OSD idle TTL (default is 60 seconds).
.UNINDENT
.sp
Per mapping (block device) \fIrbd device map\fP options:
.INDENT 0.0
.IP \(bu 2
rw \- Map the image read\-write (default).
.IP \(bu 2
ro \- Map the image read\-only.  Equivalent to \-\-read\-only.
.IP \(bu 2
queue_depth=x \- queue depth (since 4.2, default is 128 requests).
.IP \(bu 2
lock_on_read \- Acquire exclusive lock on reads, in addition to writes and
discards (since 4.9).
.IP \(bu 2
exclusive \- Disable automatic exclusive lock transitions (since 4.12).
.IP \(bu 2
lock_timeout=x \- A timeout on waiting for the acquisition of exclusive lock
(since 4.17, default is 0 seconds, meaning no timeout).
.IP \(bu 2
notrim \- Turn off discard and write zeroes offload support to avoid
deprovisioning a fully provisioned image (since 4.17). When enabled, discard
requests will fail with \-EOPNOTSUPP, write zeroes requests will fall back to
manually zeroing.
.IP \(bu 2
abort_on_full \- Fail write requests with \-ENOSPC when the cluster is full or
the data pool reaches its quota (since 5.0).  The default behaviour is to
block until the full condition is cleared.
.IP \(bu 2
alloc_size \- Minimum allocation unit of the underlying OSD object store
backend (since 5.1, default is 64K bytes).  This is used to round off and
drop discards that are too small.  For bluestore, the recommended setting is
bluestore_min_alloc_size (typically 64K for hard disk drives and 16K for
solid\-state drives).  For filestore with filestore_punch_hole = false, the
recommended setting is image object size (typically 4M).
.UNINDENT
.sp
\fIrbd device unmap\fP options:
.INDENT 0.0
.IP \(bu 2
force \- Force the unmapping of a block device that is open (since 4.9).  The
driver will wait for running requests to complete and then unmap; requests
sent to the driver after initiating the unmap will be failed.
.UNINDENT
.SH EXAMPLES
.sp
To create a new rbd image that is 100 GB:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd create mypool/myimage \-\-size 102400
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To use a non\-default object size (8 MB):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd create mypool/myimage \-\-size 102400 \-\-object\-size 8M
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To delete an rbd image (be careful!):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd rm mypool/myimage
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To create a new snapshot:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd snap create mypool/myimage@mysnap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To create a copy\-on\-write clone of a protected snapshot:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd clone mypool/myimage@mysnap otherpool/cloneimage
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To see which clones of a snapshot exist:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd children mypool/myimage@mysnap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To delete a snapshot:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd snap rm mypool/myimage@mysnap
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To map an image via the kernel with cephx enabled:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd device map mypool/myimage \-\-id admin \-\-keyfile secretfile
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To map an image via the kernel with different cluster name other than default \fIceph\fP:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd device map mypool/myimage \-\-cluster cluster\-name
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To unmap an image:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd device unmap /dev/rbd0
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To create an image and a clone from it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd import \-\-image\-format 2 image mypool/parent
rbd snap create mypool/parent@snap
rbd snap protect mypool/parent@snap
rbd clone mypool/parent@snap otherpool/child
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To create an image with a smaller stripe_unit (to better distribute small writes in some workloads):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd create mypool/myimage \-\-size 102400 \-\-stripe\-unit 65536B \-\-stripe\-count 16
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To change an image from one image format to another, export it and then
import it as the desired image format:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd export mypool/myimage@snap /tmp/img
rbd import \-\-image\-format 2 /tmp/img mypool/myimage2
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To lock an image for exclusive use:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd lock add mypool/myimage mylockid
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To release a lock:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd lock remove mypool/myimage mylockid client.2485
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To list images from trash:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd trash ls mypool
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To defer delete an image (use \fI\-\-expires\-at\fP to set expiration time, default is now):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd trash mv mypool/myimage \-\-expires\-at "tomorrow"
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To delete an image from trash (be careful!):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd trash rm mypool/myimage\-id
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To force delete an image from trash (be careful!):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd trash rm mypool/myimage\-id  \-\-force
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To restore an image from trash:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd trash restore mypool/myimage\-id
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
To restore an image from trash and rename it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd trash restore mypool/myimage\-id \-\-image mynewimage
.ft P
.fi
.UNINDENT
.UNINDENT
.SH AVAILABILITY
.sp
\fBrbd\fP is part of Ceph, a massively scalable, open\-source, distributed storage system. Please refer to
the Ceph documentation at \fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
ceph(8),
rados(8)
.SH SYNOPSIS
.nf
\fBrbdmap map\fP
\fBrbdmap unmap\fP
.fi
.sp
.SH DESCRIPTION
.sp
\fBrbdmap\fP is a shell script that automates \fBrbd map\fP and \fBrbd unmap\fP
operations on one or more RBD (RADOS Block Device) images. While the script can be
run manually by the system administrator at any time, the principal use case is
automatic mapping/mounting of RBD images at boot time (and unmounting/unmapping
at shutdown), as triggered by the init system (a systemd unit file,
\fBrbdmap.service\fP is included with the ceph\-common package for this purpose).
.sp
The script takes a single argument, which can be either "map" or "unmap".
In either case, the script parses a configuration file (defaults to \fB/etc/ceph/rbdmap\fP,
but can be overridden via an environment variable \fBRBDMAPFILE\fP). Each line
of the configuration file corresponds to an RBD image which is to be mapped, or
unmapped.
.sp
The configuration file format is:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
IMAGESPEC RBDOPTS
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
where \fBIMAGESPEC\fP should be specified as \fBPOOLNAME/IMAGENAME\fP (the pool
name, a forward slash, and the image name), or merely \fBIMAGENAME\fP, in which
case the \fBPOOLNAME\fP defaults to "rbd". \fBRBDOPTS\fP is an optional list of
parameters to be passed to the underlying \fBrbd map\fP command. These parameters
and their values should be specified as a comma\-separated string:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
PARAM1=VAL1,PARAM2=VAL2,...,PARAMN=VALN
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
This will cause the script to issue an \fBrbd map\fP command like the following:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd map POOLNAME/IMAGENAME \-\-PARAM1 VAL1 \-\-PARAM2 VAL2
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
(See the \fBrbd\fP manpage for a full list of possible options.)
For parameters and values which contain commas or equality signs, a simple
apostrophe can be used to prevent replacing them.
.sp
When run as \fBrbdmap map\fP, the script parses the configuration file, and for
each RBD image specified attempts to first map the image (using the \fBrbd map\fP
command) and, second, to mount the image.
.sp
When run as \fBrbdmap unmap\fP, images listed in the configuration file will
be unmounted and unmapped.
.sp
\fBrbdmap unmap\-all\fP attempts to unmount and subsequently unmap all currently
mapped RBD images, regardless of whether or not they are listed in the
configuration file.
.sp
If successful, the \fBrbd map\fP operation maps the image to a \fB/dev/rbdX\fP
device, at which point a udev rule is triggered to create a friendly device
name symlink, \fB/dev/rbd/POOLNAME/IMAGENAME\fP, pointing to the real mapped
device.
.sp
In order for mounting/unmounting to succeed, the friendly device name must
have a corresponding entry in \fB/etc/fstab\fP\&.
.sp
When writing \fB/etc/fstab\fP entries for RBD images, it\(aqs a good idea to specify
the "noauto" (or "nofail") mount option. This prevents the init system from
trying to mount the device too early \- before the device in question even
exists. (Since \fBrbdmap.service\fP
executes a shell script, it is typically triggered quite late in the boot
sequence.)
.SH EXAMPLES
.sp
Example \fB/etc/ceph/rbdmap\fP for three RBD images called "bar1", "bar2" and "bar3",
which are in pool "foopool":
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
foopool/bar1    id=admin,keyring=/etc/ceph/ceph.client.admin.keyring
foopool/bar2    id=admin,keyring=/etc/ceph/ceph.client.admin.keyring
foopool/bar3    id=admin,keyring=/etc/ceph/ceph.client.admin.keyring,options=\(aqlock_on_read,queue_depth=1024\(aq
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
Each line in the file contains two strings: the image spec and the options to
be passed to \fBrbd map\fP\&. These two lines get transformed into the following
commands:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
rbd map foopool/bar1 \-\-id admin \-\-keyring /etc/ceph/ceph.client.admin.keyring
rbd map foopool/bar2 \-\-id admin \-\-keyring /etc/ceph/ceph.client.admin.keyring
rbd map foopool/bar2 \-\-id admin \-\-keyring /etc/ceph/ceph.client.admin.keyring \-\-options lock_on_read,queue_depth=1024
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
If the images had XFS filesystems on them, the corresponding \fB/etc/fstab\fP
entries might look like this:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
/dev/rbd/foopool/bar1 /mnt/bar1 xfs noauto 0 0
/dev/rbd/foopool/bar2 /mnt/bar2 xfs noauto 0 0
/dev/rbd/foopool/bar3 /mnt/bar3 xfs noauto 0 0
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
After creating the images and populating the \fB/etc/ceph/rbdmap\fP file, making
the images get automatically mapped and mounted at boot is just a matter of
enabling that unit:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
systemctl enable rbdmap.service
.ft P
.fi
.UNINDENT
.UNINDENT
.SH OPTIONS
.sp
None
.SH AVAILABILITY
.sp
\fBrbdmap\fP is part of Ceph, a massively scalable, open\-source, distributed
storage system. Please refer to the Ceph documentation at
\fI\%http://ceph.com/docs\fP for more information.
.SH SEE ALSO
.sp
rbd(8),
.SH COPYRIGHT
2016, Red Hat, Inc, and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0)
.\" Generated by docutils manpage writer.
.
