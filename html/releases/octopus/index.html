
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>v15.1.0 Octopus &#8212; Ceph Documentation</title>
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/js/ceph.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="v14.2.8 Nautilus" href="../nautilus/" />
    <link rel="prev" title="Ceph 版本（索引）" href="../" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../nautilus/" title="v14.2.8 Nautilus"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../" title="Ceph 版本（索引）"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../">Ceph Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../" accesskey="U">Ceph 版本（索引）</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            

<div id="dev-warning" class="admonition note" style="display:none;">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>

<div id="eol-warning" class="admonition warning" style="display:none;">
  <p class="first admonition-title">Warning</p>
  <p class="last">This document is for an unsupported version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="display:none; padding: 15px; font-weight: bold;">
    <a id="edit-on-github" href="https://github.com/ceph/ceph/edit/master/doc/releases/octopus.rst" rel="nofollow">Edit on GitHub</a> | <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <div class="section" id="v15-1-0-octopus">
<h1>v15.1.0 Octopus<a class="headerlink" href="#v15-1-0-octopus" title="Permalink to this headline">¶</a></h1>
<p>These are draft notes for the upcoming Octopus release.</p>
<div class="section" id="major-changes-from-nautilus">
<h2>Major Changes from Nautilus<a class="headerlink" href="#major-changes-from-nautilus" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p><em>General</em>:</p>
<ul class="simple">
<li><p>A new deployment tool called <strong>cephadm</strong> has been introduced that
integrates Ceph daemon deployment and management via containers
into the orchestration layer.  For more information see
<span class="xref std std-ref">cephadm-bootstrap</span>.</p></li>
<li><p>Health alerts can now be muted, either temporarily or permanently.</p></li>
<li><p>A simple ‘alerts’ capability has been introduced to send email
health alerts for clusters deployed without the benefit of an
existing external monitoring infrastructure.</p></li>
<li><p>Health alerts are now raised for recent Ceph daemons crashes.</p></li>
</ul>
</li>
<li><p><em>Dashboard</em>:</p>
<p>The <a class="reference internal" href="../../mgr/dashboard/#mgr-dashboard"><span class="std std-ref">Ceph 仪表盘</span></a> has gained a lot of new features and functionality:</p>
<ul class="simple">
<li><p>UI Enhancements</p>
<ul>
<li><p>New vertical navigation bar</p></li>
<li><p>New unified sidebar: better background task and events notification</p></li>
<li><p>Shows all progress mgr module notifications</p></li>
<li><p>Multi-select on tables to perform bulk operations</p></li>
</ul>
</li>
<li><p>Dashboard user account security enhancements</p>
<ul>
<li><p>Disabling/enabling existing user accounts</p></li>
<li><p>Clone an existing user role</p></li>
<li><p>Users can change their own password</p></li>
<li><p>Configurable password policies: Minimum password complexity/length
requirements</p></li>
<li><p>Configurable password expiration</p></li>
<li><p>Change password after first login</p></li>
</ul>
</li>
</ul>
<p>New and enhanced management of Ceph features/services:</p>
<ul class="simple">
<li><p>OSD/device management</p>
<ul>
<li><p>List all disks associated with an OSD</p></li>
<li><p>Add support for blinking enclosure LEDs via the orchestrator</p></li>
<li><p>List all hosts known by the orchestrator</p></li>
<li><p>List all disks and their properties attached to a node</p></li>
<li><p>Display disk health information (health prediction and SMART data)</p></li>
<li><p>Deploy new OSDs on new disks/hosts</p></li>
<li><p>Display and allow sorting by an OSD’s default device class in the OSD
table</p></li>
<li><p>Explicitly set/change the device class of an OSD, display and sort OSDs by
device class</p></li>
</ul>
</li>
<li><p>Pool management</p>
<ul>
<li><p>Viewing and setting pool quotas</p></li>
<li><p>Define and change per-pool PG autoscaling mode</p></li>
</ul>
</li>
<li><p>RGW management enhancements</p>
<ul>
<li><p>Enable bucket versioning</p></li>
<li><p>Enable MFA support</p></li>
<li><p>Select placement target on bucket creation</p></li>
</ul>
</li>
<li><p>CephFS management enhancements</p>
<ul>
<li><p>CephFS client eviction</p></li>
<li><p>CephFS snapshot management</p></li>
<li><p>CephFS quota management</p></li>
<li><p>Browse CephFS directory</p></li>
</ul>
</li>
<li><p>iSCSI management enhancements</p>
<ul>
<li><p>Show iSCSI GW status on landing page</p></li>
<li><p>Prevent deletion of IQNs with open sessions</p></li>
<li><p>Display iSCSI “logged in” info</p></li>
</ul>
</li>
<li><p>Prometheus alert management</p>
<ul>
<li><p>List configured Prometheus alerts</p></li>
</ul>
</li>
</ul>
</li>
<li><p><em>RADOS</em>:</p>
<ul class="simple">
<li><p>Objects can now be brought in sync during recovery by copying only
the modified portion of the object, reducing tail latencies during
recovery.</p></li>
<li><p>The PG autoscaler feature introduced in Nautilus is enabled for
new pools by default, allowing new clusters to autotune <em>pg num</em>
without any user intervention.  The default values for new pools
and RGW/CephFS metadata pools have also been adjusted to perform
well for most users.</p></li>
<li><p>BlueStore has received several improvements and performance
updates, including improved accounting for “omap” (key/value)
object data by pool, improved cache memory management, and a
reduced allocation unit size for SSD devices.  (Note that by
default, the first time each OSD starts after upgrading to octopus
it will trigger a conversion that may take from a few minutes to a
few hours, depending on the amount of stored “omap” data.)</p></li>
<li><p>Snapshot trimming metadata is now managed in a more efficient and
scalable fashion.</p></li>
</ul>
</li>
<li><p><em>RBD</em> block storage:</p>
<ul class="simple">
<li><p>Clone operations now preserve the sparseness of the underlying RBD image.</p></li>
<li><p>The trash feature has been improved to (optionally) automatically
move old parent images to the trash when their children are all
deleted or flattened.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">rbd-nbd</span></code> tool has been improved to use more modern kernel interfaces.</p></li>
<li><p>Caching has been improved to be more efficient and performant.</p></li>
</ul>
</li>
<li><p><em>RGW</em> object storage:</p>
<ul class="simple">
<li><p>Multi-site replication can now be managed on a per-bucket basis (EXPERIMENTAL).</p></li>
<li><p>WORM?</p></li>
<li><p>bucket tagging?</p></li>
</ul>
</li>
<li><p><em>CephFS</em> distributed file system:</p>
<ul class="simple">
<li><p>Inline data support in CephFS has been deprecated and will likely be
removed in a future release.</p></li>
<li><p>MDS daemons can now be assigned to manage a particular file system via the
new <code class="docutils literal notranslate"><span class="pre">mds_join_fs</span></code> option.</p></li>
<li><p>MDS now aggressively asks idle clients to trim caps which improves stability
when file system load changes.</p></li>
<li><p>The mgr volumes plugin has received numerous improvements to support CephFS
via CSI, including snapshots and cloning.</p></li>
<li><p>cephfs-shell has had numerous incremental improvements and bug fixes.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="upgrading-from-mimic-or-nautilus">
<h2>Upgrading from Mimic or Nautilus<a class="headerlink" href="#upgrading-from-mimic-or-nautilus" title="Permalink to this headline">¶</a></h2>
<div class="section" id="notes">
<h3>Notes<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>You can monitor the progress of your upgrade at each stage with the
<code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">versions</span></code> command, which will tell you what ceph version(s) are
running for each type of daemon.</p></li>
</ul>
</div>
<div class="section" id="instructions">
<h3>Instructions<a class="headerlink" href="#instructions" title="Permalink to this headline">¶</a></h3>
<ol class="arabic">
<li><p>Make sure your cluster is stable and healthy (no down or
recovering OSDs).  (Optional, but recommended.)</p></li>
<li><p>Set the <code class="docutils literal notranslate"><span class="pre">noout</span></code> flag for the duration of the upgrade. (Optional,
but recommended.):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph osd set noout</span>
</pre></div>
</div>
</li>
<li><p>Upgrade monitors by installing the new packages and restarting the
monitor daemons.  For example, on each monitor host,:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># systemctl restart ceph-mon.target</span>
</pre></div>
</div>
<p>Once all monitors are up, verify that the monitor upgrade is
complete by looking for the <code class="docutils literal notranslate"><span class="pre">octopus</span></code> string in the mon
map.  The command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph mon dump | grep min_mon_release</span>
</pre></div>
</div>
<p>should report:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">min_mon_release</span> <span class="mi">15</span> <span class="p">(</span><span class="n">nautilus</span><span class="p">)</span>
</pre></div>
</div>
<p>If it doesn’t, that implies that one or more monitors hasn’t been
upgraded and restarted and/or the quorum does not include all monitors.</p>
</li>
<li><p>Upgrade <code class="docutils literal notranslate"><span class="pre">ceph-mgr</span></code> daemons by installing the new packages and
restarting all manager daemons.  For example, on each manager host,:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># systemctl restart ceph-mgr.target</span>
</pre></div>
</div>
<p>Verify the <code class="docutils literal notranslate"><span class="pre">ceph-mgr</span></code> daemons are running by checking <code class="docutils literal notranslate"><span class="pre">ceph</span>
<span class="pre">-s</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph -s</span>

<span class="o">...</span>
  <span class="n">services</span><span class="p">:</span>
   <span class="n">mon</span><span class="p">:</span> <span class="mi">3</span> <span class="n">daemons</span><span class="p">,</span> <span class="n">quorum</span> <span class="n">foo</span><span class="p">,</span><span class="n">bar</span><span class="p">,</span><span class="n">baz</span>
   <span class="n">mgr</span><span class="p">:</span> <span class="n">foo</span><span class="p">(</span><span class="n">active</span><span class="p">),</span> <span class="n">standbys</span><span class="p">:</span> <span class="n">bar</span><span class="p">,</span> <span class="n">baz</span>
<span class="o">...</span>
</pre></div>
</div>
</li>
<li><p>Upgrade all OSDs by installing the new packages and restarting the
ceph-osd daemons on all OSD hosts:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># systemctl restart ceph-osd.target</span>
</pre></div>
</div>
<p>Note that the first time each OSD starts, it will do a format
conversion to improve the accounting for “omap” data.  This may
take a few minutes to as much as a few hours (for an HDD with lots
of omap data).  You can disable this automatic conversion with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph config set osd bluestore_fsck_quick_fix_on_mount false</span>
</pre></div>
</div>
<p>You can monitor the progress of the OSD upgrades with the
<code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">versions</span></code> or <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">versions</span></code> commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph osd versions</span>
<span class="p">{</span>
   <span class="s2">&quot;ceph version 13.2.5 (...) mimic (stable)&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>
   <span class="s2">&quot;ceph version 15.2.0 (...) octopus (stable)&quot;</span><span class="p">:</span> <span class="mi">22</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
<li><p>Upgrade all CephFS MDS daemons.  For each CephFS file system,</p>
<ol class="arabic">
<li><p>Reduce the number of ranks to 1.  (Make note of the original
number of MDS daemons first if you plan to restore it later.):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph status</span>
<span class="c1"># ceph fs set &lt;fs_name&gt; max_mds 1</span>
</pre></div>
</div>
</li>
<li><p>Wait for the cluster to deactivate any non-zero ranks by
periodically checking the status:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph status</span>
</pre></div>
</div>
</li>
<li><p>Take all standby MDS daemons offline on the appropriate hosts with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># systemctl stop ceph-mds@&lt;daemon_name&gt;</span>
</pre></div>
</div>
</li>
<li><p>Confirm that only one MDS is online and is rank 0 for your FS:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph status</span>
</pre></div>
</div>
</li>
<li><p>Upgrade the last remaining MDS daemon by installing the new
packages and restarting the daemon:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># systemctl restart ceph-mds.target</span>
</pre></div>
</div>
</li>
<li><p>Restart all standby MDS daemons that were taken offline:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># systemctl start ceph-mds.target</span>
</pre></div>
</div>
</li>
<li><p>Restore the original value of <code class="docutils literal notranslate"><span class="pre">max_mds</span></code> for the volume:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph fs set &lt;fs_name&gt; max_mds &lt;original_max_mds&gt;</span>
</pre></div>
</div>
</li>
</ol>
</li>
<li><p>Upgrade all radosgw daemons by upgrading packages and restarting
daemons on all hosts:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># systemctl restart ceph-radosgw.target</span>
</pre></div>
</div>
</li>
<li><p>Complete the upgrade by disallowing pre-Octopus OSDs and enabling
all new Octopus-only functionality:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph osd require-osd-release octopus</span>
</pre></div>
</div>
</li>
<li><p>If you set <code class="docutils literal notranslate"><span class="pre">noout</span></code> at the beginning, be sure to clear it with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ceph osd unset noout</span>
</pre></div>
</div>
</li>
<li><p>Verify the cluster is healthy with <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">health</span></code>.</p>
<p>If your CRUSH tunables are older than Hammer, Ceph will now issue a
health warning.  If you see a health alert to that effect, you can
revert this change with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">config</span> <span class="nb">set</span> <span class="n">mon</span> <span class="n">mon_crush_min_required_version</span> <span class="n">firefly</span>
</pre></div>
</div>
<p>If Ceph does not complain, however, then we recommend you also
switch any existing CRUSH buckets to straw2, which was added back
in the Hammer release.  If you have any ‘straw’ buckets, this will
result in a modest amount of data movement, but generally nothing
too severe.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">getcrushmap</span> <span class="o">-</span><span class="n">o</span> <span class="n">backup</span><span class="o">-</span><span class="n">crushmap</span>
<span class="n">ceph</span> <span class="n">osd</span> <span class="n">crush</span> <span class="nb">set</span><span class="o">-</span><span class="nb">all</span><span class="o">-</span><span class="n">straw</span><span class="o">-</span><span class="n">buckets</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">straw2</span>
</pre></div>
</div>
<p>If there are problems, you can easily revert with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">setcrushmap</span> <span class="o">-</span><span class="n">i</span> <span class="n">backup</span><span class="o">-</span><span class="n">crushmap</span>
</pre></div>
</div>
<p>Moving to ‘straw2’ buckets will unlock a few recent features, like
the <cite>crush-compat</cite> <a class="reference internal" href="../../rados/operations/balancer/#balancer"><span class="std std-ref">balancer</span></a> mode added back in Luminous.</p>
</li>
<li><p>If you are upgrading from Mimic, or did not already do so when you
upgraded to Nautlius, we recommened you enable the new <a class="reference internal" href="../../rados/configuration/msgr2/#msgr2"><span class="std std-ref">v2
network protocol</span></a>, issue the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">mon</span> <span class="n">enable</span><span class="o">-</span><span class="n">msgr2</span>
</pre></div>
</div>
<p>This will instruct all monitors that bind to the old default port
6789 for the legacy v1 protocol to also bind to the new 3300 v2
protocol port.  To see if all monitors have been updated,:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">mon</span> <span class="n">dump</span>
</pre></div>
</div>
<p>and verify that each monitor has both a <code class="docutils literal notranslate"><span class="pre">v2:</span></code> and <code class="docutils literal notranslate"><span class="pre">v1:</span></code> address
listed.</p>
</li>
<li><p>Consider enabling the <a class="reference internal" href="../../mgr/telemetry/#telemetry"><span class="std std-ref">telemetry module</span></a> to send
anonymized usage statistics and crash information to the Ceph
upstream developers.  To see what would be reported (without actually
sending any information to anyone),:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">mgr</span> <span class="n">module</span> <span class="n">enable</span> <span class="n">telemetry</span>
<span class="n">ceph</span> <span class="n">telemetry</span> <span class="n">show</span>
</pre></div>
</div>
<p>If you are comfortable with the data that is reported, you can opt-in to
automatically report the high-level cluster metadata with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">telemetry</span> <span class="n">on</span>
</pre></div>
</div>
<p>For more information about the telemetry module, see <a class="reference internal" href="../../mgr/telemetry/#telemetry"><span class="std std-ref">the
documentation</span></a>.</p>
</li>
</ol>
</div>
</div>
<div class="section" id="upgrading-from-pre-mimic-releases-like-luminous">
<h2>Upgrading from pre-Mimic releases (like Luminous)<a class="headerlink" href="#upgrading-from-pre-mimic-releases-like-luminous" title="Permalink to this headline">¶</a></h2>
<p>You <em>must</em> first upgrade to Mimic (13.2.z) or Nautilus (14.2.z) before
upgrading to Octopus.</p>
</div>
<div class="section" id="upgrade-compatibility-notes">
<h2>Upgrade compatibility notes<a class="headerlink" href="#upgrade-compatibility-notes" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>The RGW “num_rados_handles” has been removed.
If you were using a value of “num_rados_handles” greater than 1
multiply your current “objecter_inflight_ops” and
“objecter_inflight_op_bytes” paramaeters by the old
“num_rados_handles” to get the same throttle behavior.</p></li>
<li><p>Ceph now packages python bindings for python3.6 instead of
python3.4, because python3 in EL7/EL8 is now using python3.6
as the native python3. see the <cite>announcement &lt;https://lists.fedoraproject.org/archives/list/epel-announce&#64;lists.fedoraproject.org/message/EGUMKAIMPK2UD5VSHXM53BH2MBDGDWMO/&gt;_</cite>
for more details on the background of this change.</p></li>
<li><p>librbd now uses a write-around cache policy be default,
replacing the previous write-back cache policy default.
This cache policy allows librbd to immediately complete
write IOs while they are still in-flight to the OSDs.
Subsequent flush requests will ensure all in-flight
write IOs are completed prior to completing. The
librbd cache policy can be controlled via a new
“rbd_cache_policy” configuration option.</p></li>
<li><p>librbd now includes a simple IO scheduler which attempts to
batch together multiple IOs against the same backing RBD
data block object. The librbd IO scheduler policy can be
controlled via a new “rbd_io_scheduler” configuration
option.</p></li>
<li><p>RGW: radosgw-admin introduces two subcommands that allow the
managing of expire-stale objects that might be left behind after a
bucket reshard in earlier versions of RGW. One subcommand lists such
objects and the other deletes them. Read the troubleshooting section
of the dynamic resharding docs for details.</p></li>
<li><p>RGW: Bucket naming restrictions have changed and likely to cause
InvalidBucketName errors. We recommend to set <code class="docutils literal notranslate"><span class="pre">rgw_relaxed_s3_bucket_names</span></code>
option to true as a workaround.</p></li>
<li><p>In the Zabbix Mgr Module there was a typo in the key being send
to Zabbix for PGs in backfill_wait state. The key that was sent
was ‘wait_backfill’ and the correct name is ‘backfill_wait’.
Update your Zabbix template accordingly so that it accepts the
new key being send to Zabbix.</p></li>
<li><p>zabbix plugin for ceph manager now includes osd and pool
discovery. Update of zabbix_template.xml is needed
to receive per-pool (read/write throughput, diskspace usage)
and per-osd (latency, status, pgs) statistics</p></li>
<li><p>The format of all date + time stamps has been modified to fully
conform to ISO 8601.  The old format (<code class="docutils literal notranslate"><span class="pre">YYYY-MM-DD</span>
<span class="pre">HH:MM:SS.ssssss</span></code>) excluded the <code class="docutils literal notranslate"><span class="pre">T</span></code> separator between the date and
time and was rendered using the local time zone without any explicit
indication.  The new format includes the separator as well as a
<code class="docutils literal notranslate"><span class="pre">+nnnn</span></code> or <code class="docutils literal notranslate"><span class="pre">-nnnn</span></code> suffix to indicate the time zone, or a <code class="docutils literal notranslate"><span class="pre">Z</span></code>
suffix if the time is UTC.  For example,
<code class="docutils literal notranslate"><span class="pre">2019-04-26T18:40:06.225953+0100</span></code>.</p>
<p>Any code or scripts that was previously parsing date and/or time
values from the JSON or XML structure CLI output should be checked
to ensure it can handle ISO 8601 conformant values.  Any code
parsing date or time values from the unstructured human-readable
output should be modified to parse the structured output instead, as
the human-readable output may change without notice.</p>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">bluestore_no_per_pool_stats_tolerance</span></code> config option has been
replaced with <code class="docutils literal notranslate"><span class="pre">bluestore_fsck_error_on_no_per_pool_stats</span></code>
(default: false).  The overall default behavior has not changed:
fsck will warn but not fail on legacy stores, and repair will
convert to per-pool stats.</p></li>
<li><p>The disaster-recovery related ‘ceph mon sync force’ command has been
replaced with ‘ceph daemon &lt;…&gt; sync_force’.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">osd_recovery_max_active</span></code> option now has
<code class="docutils literal notranslate"><span class="pre">osd_recovery_max_active_hdd</span></code> and <code class="docutils literal notranslate"><span class="pre">osd_recovery_max_active_ssd</span></code>
variants, each with different default values for HDD and SSD-backed
OSDs, respectively.  By default <code class="docutils literal notranslate"><span class="pre">osd_recovery_max_active</span></code> now
defaults to zero, which means that the OSD will conditionally use
the HDD or SSD option values.  Administrators who have customized
this value may want to consider whether they have set this to a
value similar to the new defaults (3 for HDDs and 10 for SSDs) and,
if so, remove the option from their configuration entirely.</p></li>
<li><p>monitors now have a <cite>ceph osd info</cite> command that will provide information
on all osds, or provided osds, thus simplifying the process of having to
parse <cite>osd dump</cite> for the same information.</p></li>
<li><p>The structured output of <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">status</span></code> or <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">-s</span></code> is now more
concise, particularly the <cite>mgrmap</cite> and <cite>monmap</cite> sections, and the
structure of the <cite>osdmap</cite> section has been cleaned up.</p></li>
<li><p>A health warning is now generated if the average osd heartbeat ping
time exceeds a configurable threshold for any of the intervals
computed.  The OSD computes 1 minute, 5 minute and 15 minute
intervals with average, minimum and maximum values.  New
configuration option <code class="docutils literal notranslate"><span class="pre">mon_warn_on_slow_ping_ratio</span></code> specifies a
percentage of <code class="docutils literal notranslate"><span class="pre">osd_heartbeat_grace</span></code> to determine the threshold.  A
value of zero disables the warning.  New configuration option
<code class="docutils literal notranslate"><span class="pre">mon_warn_on_slow_ping_time</span></code> specified in milliseconds over-rides
the computed value, causes a warning when OSD heartbeat pings take
longer than the specified amount.  New admin command <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">daemon</span>
<span class="pre">mgr.#</span> <span class="pre">dump_osd_network</span> <span class="pre">[threshold]</span></code> command will list all
connections with a ping time longer than the specified threshold or
value determined by the config options, for the average for any of
the 3 intervals.  New admin command <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">daemon</span> <span class="pre">osd.#</span>
<span class="pre">dump_osd_network</span> <span class="pre">[threshold]</span></code> will do the same but only including
heartbeats initiated by the specified OSD.</p></li>
<li><p>Inline data support for CephFS has been deprecated. When setting the flag,
users will see a warning to that effect, and enabling it now requires the
<code class="docutils literal notranslate"><span class="pre">--yes-i-really-really-mean-it</span></code> flag. If the MDS is started on a
filesystem that has it enabled, a health warning is generated. Support for
this feature will be removed in a future release.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">{set,unset}</span> <span class="pre">full</span></code> is not supported anymore. We have been using
<code class="docutils literal notranslate"><span class="pre">full</span></code> and <code class="docutils literal notranslate"><span class="pre">nearfull</span></code> flags in OSD map for tracking the fullness status
of a cluster back since the Hammer release, if the OSD map is marked <code class="docutils literal notranslate"><span class="pre">full</span></code>
all write operations will be blocked until this flag is removed. In the
Infernalis release and Linux kernel 4.7 client, we introduced the per-pool
full/nearfull flags to track the status for a finer-grained control, so the
clients will hold the write operations if either the cluster-wide <code class="docutils literal notranslate"><span class="pre">full</span></code>
flag or the per-pool <code class="docutils literal notranslate"><span class="pre">full</span></code> flag is set. This was a compromise, as we
needed to support the cluster with and without per-pool <code class="docutils literal notranslate"><span class="pre">full</span></code> flags
support. But this practically defeated the purpose of introducing the
per-pool flags. So, in the Mimic release, the new flags finally took the
place of their cluster-wide counterparts, as the monitor started removing
these two flags from OSD map. So the clients of Infernalis and up can benefit
from this change, as they won’t be blocked by the full pools which they are
not writing to. In this release, <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">{set,unset}</span> <span class="pre">full</span></code> is now considered
as an invalid command. And the clients will continue honoring both the
cluster-wide and per-pool flags to be backward comaptible with pre-infernalis
clusters.</p></li>
<li><p>The telemetry module now reports more information.</p>
<p>First, there is a new ‘device’ channel, enabled by default, that
will report anonymized hard disk and SSD health metrics to
telemetry.ceph.com in order to build and improve device failure
prediction algorithms.  If you are not comfortable sharing device
metrics, you can disable that channel first before re-opting-in:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">config</span> <span class="nb">set</span> <span class="n">mgr</span> <span class="n">mgr</span><span class="o">/</span><span class="n">telemetry</span><span class="o">/</span><span class="n">channel_device</span> <span class="n">false</span>
</pre></div>
</div>
<p>Second, we now report more information about CephFS file systems,
including:</p>
<blockquote>
<div><ul class="simple">
<li><p>how many MDS daemons (in total and per file system)</p></li>
<li><p>which features are (or have been) enabled</p></li>
<li><p>how many data pools</p></li>
<li><p>approximate file system age (year + month of creation)</p></li>
<li><p>how many files, bytes, and snapshots</p></li>
<li><p>how much metadata is being cached</p></li>
</ul>
</div></blockquote>
<p>We have also added:</p>
<blockquote>
<div><ul class="simple">
<li><p>which Ceph release the monitors are running</p></li>
<li><p>whether msgr v1 or v2 addresses are used for the monitors</p></li>
<li><p>whether IPv4 or IPv6 addresses are used for the monitors</p></li>
<li><p>whether RADOS cache tiering is enabled (and which mode)</p></li>
<li><p>whether pools are replicated or erasure coded, and
which erasure code profile plugin and parameters are in use</p></li>
<li><p>how many hosts are in the cluster, and how many hosts have each type of daemon</p></li>
<li><p>whether a separate OSD cluster network is being used</p></li>
<li><p>how many RBD pools and images are in the cluster, and how many pools have RBD mirroring enabled</p></li>
<li><p>how many RGW daemons, zones, and zonegroups are present; which RGW frontends are in use</p></li>
<li><p>aggregate stats about the CRUSH map, like which algorithms are used, how
big buckets are, how many rules are defined, and what tunables are in
use</p></li>
</ul>
</div></blockquote>
<p>If you had telemetry enabled, you will need to re-opt-in with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">telemetry</span> <span class="n">on</span>
</pre></div>
</div>
<p>You can view exactly what information will be reported first with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">telemetry</span> <span class="n">show</span>        <span class="c1"># see everything</span>
<span class="n">ceph</span> <span class="n">telemetry</span> <span class="n">show</span> <span class="n">basic</span>  <span class="c1"># basic cluster info (including all of the new info)</span>
</pre></div>
</div>
</li>
<li><p>Following invalid settings now are not tolerated anymore
for the command <cite>ceph osd erasure-code-profile set xxx</cite>.
* invalid <cite>m</cite> for “reed_sol_r6_op” erasure technique
* invalid <cite>m</cite> and invalid <cite>w</cite> for “liber8tion” erasure technique</p></li>
<li><p>New OSD daemon command dump_recovery_reservations which reveals the
recovery locks held (in_progress) and waiting in priority queues.</p></li>
<li><p>New OSD daemon command dump_scrub_reservations which reveals the
scrub reservations that are held for local (primary) and remote (replica) PGs.</p></li>
<li><p>Previously, <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">tell</span> <span class="pre">mgr</span> <span class="pre">...</span></code> could be used to call commands
implemented by mgr modules.  This is no longer supported.  Since
luminous, using <code class="docutils literal notranslate"><span class="pre">tell</span></code> has not been necessary: those same commands
are also accessible without the <code class="docutils literal notranslate"><span class="pre">tell</span> <span class="pre">mgr</span></code> portion (e.g., <code class="docutils literal notranslate"><span class="pre">ceph</span>
<span class="pre">tell</span> <span class="pre">mgr</span> <span class="pre">influx</span> <span class="pre">foo</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">influx</span> <span class="pre">foo</span></code>.  <code class="docutils literal notranslate"><span class="pre">ceph</span>
<span class="pre">tell</span> <span class="pre">mgr</span> <span class="pre">...</span></code> will now call admin commands–the same set of
commands accessible via <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">daemon</span> <span class="pre">...</span></code> when you are logged into
the appropriate host.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">tell</span></code> and <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">daemon</span></code> commands have been unified,
such that all such commands are accessible via either interface.
Note that ceph-mgr tell commands are accessible via either <code class="docutils literal notranslate"><span class="pre">ceph</span>
<span class="pre">tell</span> <span class="pre">mgr</span> <span class="pre">...</span></code> or <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">tell</span> <span class="pre">mgr.&lt;id&gt;</span> <span class="pre">...</span></code>, and it is only
possible to send tell commands to the active daemon (the standbys do
not accept incoming connections over the network).</p></li>
<li><p>Ceph will now issue a health warning if a RADOS pool as a <code class="docutils literal notranslate"><span class="pre">pg_num</span></code>
value that is not a power of two.  This can be fixed by adjusting
the pool to a nearby power of two:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="nb">set</span> <span class="o">&lt;</span><span class="n">pool</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="n">pg_num</span> <span class="o">&lt;</span><span class="n">new</span><span class="o">-</span><span class="n">pg</span><span class="o">-</span><span class="n">num</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Alternatively, the warning can be silenced with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">config</span> <span class="nb">set</span> <span class="k">global</span> <span class="n">mon_warn_on_pool_pg_num_not_power_of_two</span> <span class="n">false</span>
</pre></div>
</div>
</li>
<li><p>The format of MDSs in <cite>ceph fs dump</cite> has changed.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">mds_cache_size</span></code> config option is completely removed. Since luminous,
the <code class="docutils literal notranslate"><span class="pre">mds_cache_memory_limit</span></code> config option has been preferred to configure
the MDS’s cache limits.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">pg_autoscale_mode</span></code> is now set to <code class="docutils literal notranslate"><span class="pre">on</span></code> by default for newly
created pools, which means that Ceph will automatically manage the
number of PGs.  To change this behavior, or to learn more about PG
autoscaling, see <a class="reference internal" href="../../rados/operations/placement-groups/#pg-autoscaler"><span class="std std-ref">自伸缩归置组</span></a>.  Note that existing pools in
upgraded clusters will still be set to <code class="docutils literal notranslate"><span class="pre">warn</span></code> by default.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">upmap_max_iterations</span></code> config option of mgr/balancer has been
renamed to <code class="docutils literal notranslate"><span class="pre">upmap_max_optimizations</span></code> to better match its behaviour.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mClockClientQueue</span></code> and <code class="docutils literal notranslate"><span class="pre">mClockClassQueue</span></code> OpQueue
implementations have been removed in favor of of a single
<code class="docutils literal notranslate"><span class="pre">mClockScheduler</span></code> implementation of a simpler OSD interface.
Accordingly, the <code class="docutils literal notranslate"><span class="pre">osd_op_queue_mclock*</span></code> family of config options
has been removed in favor of the <code class="docutils literal notranslate"><span class="pre">osd_mclock_scheduler*</span></code> family
of options.</p></li>
<li><p>The config subsystem now searches dot (‘.’) delineated prefixes for
options.  That means for an entity like <code class="docutils literal notranslate"><span class="pre">client.foo.bar</span></code>, it’s
overall configuration will be a combination of the global options,
<code class="docutils literal notranslate"><span class="pre">client</span></code>, <code class="docutils literal notranslate"><span class="pre">client.foo</span></code>, and <code class="docutils literal notranslate"><span class="pre">client.foo.bar</span></code>.  Previously,
only global, <code class="docutils literal notranslate"><span class="pre">client</span></code>, and <code class="docutils literal notranslate"><span class="pre">client.foo.bar</span></code> options would apply.
This change may affect the configuration for clients that include a
<code class="docutils literal notranslate"><span class="pre">.</span></code> in their name.</p>
<p>Note that this only applies to configuration options in the</p>
</li>
</ul>
</div>
</div>



          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../">
              <img class="logo" src="../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../start/">安装（ ceph-deploy ）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">安装（手动）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../start/kube-helm/">安装（ Kubernetes + Helm ）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/developer_guide/">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../">Ceph 版本（索引）</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../#id1">待发布版本</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Octopus</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#major-changes-from-nautilus">Major Changes from Nautilus</a></li>
<li class="toctree-l4"><a class="reference internal" href="#upgrading-from-mimic-or-nautilus">Upgrading from Mimic or Nautilus</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#notes">Notes</a></li>
<li class="toctree-l5"><a class="reference internal" href="#instructions">Instructions</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#upgrading-from-pre-mimic-releases-like-luminous">Upgrading from pre-Mimic releases (like Luminous)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#upgrade-compatibility-notes">Upgrade compatibility notes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../#id2">活跃版本</a></li>
<li class="toctree-l2"><a class="reference internal" href="../#id3">归档版本</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../translation_cn/">中文版翻译资源</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../nautilus/" title="v14.2.8 Nautilus"
             >next</a> |</li>
        <li class="right" >
          <a href="../" title="Ceph 版本（索引）"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../">Ceph Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../" >Ceph 版本（索引）</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).
    </div>
  </body>
</html>