

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Squid &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="Reef" href="../reef/" />
    <link rel="prev" title="Ceph 版本（索引）" href="../" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../">Ceph 版本（索引）</a> &raquo;</li>
      <li>Squid</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/releases/squid.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../">
          

          
            
            <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/developer_guide/">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../">Ceph 版本（索引）</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../#active-releases">活跃版本</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Squid (v19.2.*)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#v19-2-0-squid">v19.2.0 Squid</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../reef/">Reef (v18.2.*)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quincy/">Quincy (v17.2.*)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../#id2">归档版本</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hardware-monitoring/">硬件监控</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="squid">
<h1>Squid<a class="headerlink" href="#squid" title="Permalink to this heading"></a></h1>
<p>Squid is the 19th stable release of Ceph.</p>
<section id="v19-2-0-squid">
<h2>v19.2.0 Squid<a class="headerlink" href="#v19-2-0-squid" title="Permalink to this heading"></a></h2>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>iSCSI users are advised that the upstream developers of Ceph encountered a
bug during an upgrade from Ceph 19.1.1 to Ceph 19.2.0. Read <a class="reference external" href="https://tracker.ceph.com/issues/68215">Tracker Issue
68215</a> before attempting an upgrade
to 19.2.0.</p>
</div>
<section id="highlights">
<h3>Highlights<a class="headerlink" href="#highlights" title="Permalink to this heading"></a></h3>
<p>RADOS</p>
<ul class="simple">
<li><p>BlueStore has been optimized for better performance in snapshot-intensive workloads.</p></li>
<li><p>BlueStore RocksDB LZ4 compression is now enabled by default to improve average performance
and “fast device” space usage.</p></li>
<li><p>Other improvements include more flexible EC configurations, an OpTracker to help debug mgr
module issues, and better scrub scheduling.</p></li>
</ul>
<p>Dashboard</p>
<ul class="simple">
<li><p>Improved navigation layout</p></li>
<li><p>Support for managing CephFS snapshots and clones, as well as snapshot schedule management</p></li>
<li><p>Manage authorization capabilities for CephFS resources</p></li>
<li><p>Helpers on mounting a CephFS volume</p></li>
</ul>
<p>RBD</p>
<ul class="simple">
<li><p>diff-iterate can now execute locally, bringing a dramatic performance improvement for QEMU
live disk synchronization and backup use cases.</p></li>
<li><p>Support for cloning from non-user type snapshots is added.</p></li>
<li><p>rbd-wnbd driver has gained the ability to multiplex image mappings.</p></li>
</ul>
<p>RGW</p>
<ul class="simple">
<li><p>The User Accounts feature unlocks several new AWS-compatible IAM APIs for the self-service
management of users, keys, groups, roles, policy and more.</p></li>
</ul>
<p>Crimson/Seastore</p>
<ul class="simple">
<li><p>Crimson’s first tech preview release! Supporting RBD workloads on Replicated pools. For more
information please visit: <a class="reference external" href="https://ceph.io/en/news/crimson">https://ceph.io/en/news/crimson</a></p></li>
</ul>
</section>
<section id="ceph">
<h3>Ceph<a class="headerlink" href="#ceph" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>ceph: a new <cite>--daemon-output-file</cite> switch is available for <cite>ceph tell</cite>
commands to dump output to a file local to the daemon. For commands which
produce large amounts of output, this avoids a potential spike in memory
usage on the daemon, allows for faster streaming writes to a file local to
the daemon, and reduces time holding any locks required to execute the
command. For analysis, it is necessary to manually retrieve the file from the host
running the daemon. Currently, only <code class="docutils literal notranslate"><span class="pre">--format=json|json-pretty</span></code>
are supported.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cls_cxx_gather</span></code> is marked as deprecated.</p></li>
<li><p>Tracing: The blkin tracing feature (see
<a class="reference external" href="https://docs.ceph.com/en/reef/dev/blkin/">https://docs.ceph.com/en/reef/dev/blkin/</a>) is now deprecated in favor of
Opentracing
(<a class="reference external" href="https://docs.ceph.com/en/reef/dev/developer_guide/jaegertracing/">https://docs.ceph.com/en/reef/dev/developer_guide/jaegertracing/</a>) and will
be removed in a later release.</p></li>
<li><p>PG dump: The default output of <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">pg</span> <span class="pre">dump</span> <span class="pre">--format</span> <span class="pre">json</span></code> has changed.
The default JSON format produces a rather massive output in large clusters
and isn’t scalable, so we have removed the ‘network_ping_times’ section from
the output. Details in the tracker: <a class="reference external" href="https://tracker.ceph.com/issues/57460">https://tracker.ceph.com/issues/57460</a></p></li>
</ul>
</section>
<section id="cephfs">
<h3>CephFS<a class="headerlink" href="#cephfs" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>CephFS: it is now possible to pause write I/O and metadata mutations on a
tree in the file system using a new suite of subvolume quiesce commands.
This is implemented to support crash-consistent snapshots for distributed
applications. Please see the relevant section in the documentation on CephFS
subvolumes for more information.</p></li>
<li><p>CephFS: MDS evicts clients which are not advancing their request tids which
causes a large buildup of session metadata resulting in the MDS going
read-only due to the RADOS operation exceeding the size threshold.
<cite>mds_session_metadata_threshold</cite> config controls the maximum size that a
(encoded) session metadata can grow.</p></li>
<li><p>CephFS: A new “mds last-seen” command is available for querying the last time
an MDS was in the FSMap, subject to a pruning threshold.</p></li>
<li><p>CephFS: For clusters with multiple CephFS file systems, all the snap-schedule
commands now expect the ‘--fs’ argument.</p></li>
<li><p>CephFS: The period specifier <code class="docutils literal notranslate"><span class="pre">m</span></code> now implies minutes and the period
specifier <code class="docutils literal notranslate"><span class="pre">M</span></code> now implies months. This has been made consistent with the
rest of the system.</p></li>
<li><p>CephFS: Running the command “ceph fs authorize” for an existing entity now
upgrades the entity’s capabilities instead of printing an error. It can now
also change read/write permissions in a capability that the entity already
holds. If the capability passed by user is same as one of the capabilities
that the entity already holds, idempotency is maintained.</p></li>
<li><p>CephFS: Two FS names can now be swapped, optionally along with their IDs,
using “ceph fs swap” command. The function of this API is to facilitate
file system swaps for disaster recovery. In particular, it avoids situations
where a named file system is temporarily missing which would prompt a higher
level storage operator (like Rook) to recreate the missing file system.
See <a class="reference external" href="https://docs.ceph.com/en/latest/cephfs/administration/#file-systems">https://docs.ceph.com/en/latest/cephfs/administration/#file-systems</a>
docs for more information.</p></li>
<li><p>CephFS: Before running the command “ceph fs rename”, the filesystem to be
renamed must be offline and the config “refuse_client_session” must be set
for it. The config “refuse_client_session” can be removed/unset and
filesystem can be online after the rename operation is complete.</p></li>
<li><p>CephFS: Disallow delegating preallocated inode ranges to clients. Config
<cite>mds_client_delegate_inos_pct</cite> defaults to 0 which disables async dirops
in the kclient.</p></li>
<li><p>CephFS: MDS log trimming is now driven by a separate thread which tries to
trim the log every second (<cite>mds_log_trim_upkeep_interval</cite> config). Also, a
couple of configs govern how much time the MDS spends in trimming its logs.
These configs are <cite>mds_log_trim_threshold</cite> and <cite>mds_log_trim_decay_rate</cite>.</p></li>
<li><p>CephFS: Full support for subvolumes and subvolume groups is now available</p></li>
<li><p>CephFS: The <cite>subvolume snapshot clone</cite> command now depends on the config
option <cite>snapshot_clone_no_wait</cite> which is used to reject the clone operation
when all the cloner threads are busy. This config option is enabled by
default which means that if no cloner threads are free, the clone request
errors out with EAGAIN.  The value of the config option can be fetched by
using: <cite>ceph config get mgr mgr/volumes/snapshot_clone_no_wait</cite> and it can be
disabled by using: <cite>ceph config set mgr mgr/volumes/snapshot_clone_no_wait
false</cite>
for snap_schedule Manager module.</p></li>
<li><p>CephFS: Commands <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">mds</span> <span class="pre">fail</span></code> and <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">fs</span> <span class="pre">fail</span></code> now require a
confirmation flag when some MDSs exhibit health warning MDS_TRIM or
MDS_CACHE_OVERSIZED. This is to prevent accidental MDS failover causing
further delays in recovery.</p></li>
<li><p>CephFS: fixes to the implementation of the <code class="docutils literal notranslate"><span class="pre">root_squash</span></code> mechanism enabled
via cephx <code class="docutils literal notranslate"><span class="pre">mds</span></code> caps on a client credential require a new client feature
bit, <code class="docutils literal notranslate"><span class="pre">client_mds_auth_caps</span></code>. Clients using credentials with <code class="docutils literal notranslate"><span class="pre">root_squash</span></code>
without this feature will trigger the MDS to raise a HEALTH_ERR on the
cluster, MDS_CLIENTS_BROKEN_ROOTSQUASH. See the documentation on this warning
and the new feature bit for more information.</p></li>
<li><p>CephFS: Expanded removexattr support for cephfs virtual extended attributes.
Previously one had to use setxattr to restore the default in order to
“remove”.  You may now properly use removexattr to remove. You can also now
remove layout on root inode, which then will restore layout to default
layout.</p></li>
<li><p>CephFS: cephfs-journal-tool is guarded against running on an online file
system.  The ‘cephfs-journal-tool --rank &lt;fs_name&gt;:&lt;mds_rank&gt; journal reset’
and ‘cephfs-journal-tool --rank &lt;fs_name&gt;:&lt;mds_rank&gt; journal reset --force’
commands require ‘--yes-i-really-really-mean-it’.</p></li>
<li><p>CephFS: “ceph fs clone status” command will now print statistics about clone
progress in terms of how much data has been cloned (in both percentage as
well as bytes) and how many files have been cloned.</p></li>
<li><p>CephFS: “ceph status” command will now print a progress bar when cloning is
ongoing. If clone jobs are more than the cloner threads, it will print one
more progress bar that shows total amount of progress made by both ongoing
as well as pending clones. Both progress are accompanied by messages that
show number of clone jobs in the respective categories and the amount of
progress made by each of them.</p></li>
<li><p>cephfs-shell: The cephfs-shell utility is now packaged for RHEL 9 / CentOS 9
as required python dependencies are now available in EPEL9.</p></li>
<li><p>The CephFS automatic metadata load (sometimes called “default”) balancer is
now disabled by default. The new file system flag <cite>balance_automate</cite>
can be used to toggle it on or off. It can be enabled or disabled via
<cite>ceph fs set &lt;fs_name&gt; balance_automate &lt;bool&gt;</cite>.</p></li>
</ul>
</section>
<section id="cephx">
<h3>CephX<a class="headerlink" href="#cephx" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>cephx: key rotation is now possible using <cite>ceph auth rotate</cite>. Previously,
this was only possible by deleting and then recreating the key.</p></li>
</ul>
</section>
<section id="dashboard">
<h3>Dashboard<a class="headerlink" href="#dashboard" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Dashboard: Rearranged Navigation Layout: The navigation layout has been reorganized for improved usability and easier access to key features.</p></li>
<li><p>Dashboard: CephFS Improvments
* Support for managing CephFS snapshots and clones, as well as snapshot schedule management
* Manage authorization capabilities for CephFS resources
* Helpers on mounting a CephFS volume</p></li>
<li><p>Dashboard: RGW Improvements
* Support for managing bucket policies
* Add/Remove bucket tags
* ACL Management
* Several UI/UX Improvements to the bucket form</p></li>
</ul>
</section>
<section id="mgr">
<h3>MGR<a class="headerlink" href="#mgr" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>MGR/REST: The REST manager module will trim requests based on the
‘max_requests’ option.  Without this feature, and in the absence of manual
deletion of old requests, the accumulation of requests in the array can lead
to Out Of Memory (OOM) issues, resulting in the Manager crashing.</p></li>
<li><p>MGR: An OpTracker to help debug mgr module issues is now available.</p></li>
</ul>
</section>
<section id="monitoring">
<h3>Monitoring<a class="headerlink" href="#monitoring" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Monitoring: Grafana dashboards are now loaded into the container at runtime
rather than building a grafana image with the grafana dashboards. Official
Ceph grafana images can be found in quay.io/ceph/grafana</p></li>
<li><p>Monitoring: RGW S3 Analytics: A new Grafana dashboard is now available,
enabling you to visualize per bucket and user analytics data, including total
GETs, PUTs, Deletes, Copies, and list metrics.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">mon_cluster_log_file_level</span></code> and <code class="docutils literal notranslate"><span class="pre">mon_cluster_log_to_syslog_level</span></code>
options have been removed. Henceforth, users should use the new generic
option <code class="docutils literal notranslate"><span class="pre">mon_cluster_log_level</span></code> to control the cluster log level verbosity
for the cluster log file as well as for all external entities.</p></li>
</ul>
</section>
<section id="rados">
<h3>RADOS<a class="headerlink" href="#rados" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>RADOS: <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">POOL_APP_NOT_ENABLED</span></code> health warning will now be reported if the
application is not enabled for the pool irrespective of whether the pool is
in use or not. Always tag a pool with an application using <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">pool</span>
<span class="pre">application</span> <span class="pre">enable</span></code> command to avoid reporting of POOL_APP_NOT_ENABLED
health warning for that pool. The user might temporarily mute this warning
using <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">health</span> <span class="pre">mute</span> <span class="pre">POOL_APP_NOT_ENABLED</span></code>.</p></li>
<li><p>RADOS: <cite>get_pool_is_selfmanaged_snaps_mode</cite> C++ API has been deprecated due
to being prone to false negative results.  Its safer replacement is
<cite>pool_is_in_selfmanaged_snaps_mode</cite>.</p></li>
<li><p>RADOS: For bug 62338 (<a class="reference external" href="https://tracker.ceph.com/issues/62338">https://tracker.ceph.com/issues/62338</a>), we did not
choose to condition the fix on a server flag in order to simplify
backporting.  As a result, in rare cases it may be possible for a PG to flip
between two acting sets while an upgrade to a version with the fix is in
progress.  If you observe this behavior, you should be able to work around it
by completing the upgrade or by disabling async recovery by setting
osd_async_recovery_min_cost to a very large value on all OSDs until the
upgrade is complete: <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">config</span> <span class="pre">set</span> <span class="pre">osd</span> <span class="pre">osd_async_recovery_min_cost</span>
<span class="pre">1099511627776</span></code></p></li>
<li><p>RADOS: A detailed version of the <cite>balancer status</cite> CLI command in the
balancer module is now available. Users may run <cite>ceph balancer status detail</cite>
to see more details about which PGs were updated in the balancer’s last
optimization.  See <a class="reference external" href="https://docs.ceph.com/en/latest/rados/operations/balancer/">https://docs.ceph.com/en/latest/rados/operations/balancer/</a>
for more information.</p></li>
<li><p>RADOS: Read balancing may now be managed automatically via the balancer
manager module. Users may choose between two new modes: <code class="docutils literal notranslate"><span class="pre">upmap-read</span></code>, which
offers upmap and read optimization simultaneously, or <code class="docutils literal notranslate"><span class="pre">read</span></code>, which may be
used to only optimize reads. For more detailed information see
<a class="reference external" href="https://docs.ceph.com/en/latest/rados/operations/read-balancer/#online-optimization">https://docs.ceph.com/en/latest/rados/operations/read-balancer/#online-optimization</a>.</p></li>
<li><p>RADOS: BlueStore has been optimized for better performance in snapshot-intensive workloads.</p></li>
<li><p>RADOS: BlueStore RocksDB LZ4 compression is now enabled by default to improve average
performance and “fast device” space usage.</p></li>
<li><p>RADOS: A new CRUSH rule type, MSR (Multi-Step Retry), allows for more flexible EC
configurations.</p></li>
<li><p>RADOS: Scrub scheduling behavior has been improved.</p></li>
</ul>
</section>
<section id="crimson-seastore">
<h3>Crimson/Seastore<a class="headerlink" href="#crimson-seastore" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Crimson’s first tech preview release!
Supporting RBD workloads on Replicated pools.
For more information please visit: <a class="reference external" href="https://ceph.io/en/news/crimson">https://ceph.io/en/news/crimson</a></p></li>
</ul>
</section>
<section id="rbd">
<h3>RBD<a class="headerlink" href="#rbd" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>RBD: When diffing against the beginning of time (<cite>fromsnapname == NULL</cite>) in
fast-diff mode (<cite>whole_object == true</cite> with <code class="docutils literal notranslate"><span class="pre">fast-diff</span></code> image feature enabled
and valid), diff-iterate is now guaranteed to execute locally if exclusive
lock is available.  This brings a dramatic performance improvement for QEMU
live disk synchronization and backup use cases.</p></li>
<li><p>RBD: The <code class="docutils literal notranslate"><span class="pre">try-netlink</span></code> mapping option for rbd-nbd has become the default
and is now deprecated. If the NBD netlink interface is not supported by the
kernel, then the mapping is retried using the legacy ioctl interface.</p></li>
<li><p>RBD: The option <code class="docutils literal notranslate"><span class="pre">--image-id</span></code> has been added to <cite>rbd children</cite> CLI command,
so it can be run for images in the trash.</p></li>
<li><p>RBD: <cite>Image::access_timestamp</cite> and <cite>Image::modify_timestamp</cite> Python APIs now
return timestamps in UTC.</p></li>
<li><p>RBD: Support for cloning from non-user type snapshots is added.  This is
intended primarily as a building block for cloning new groups from group
snapshots created with <cite>rbd group snap create</cite> command, but has also been
exposed via the new <cite>--snap-id</cite> option for <cite>rbd clone</cite> command.</p></li>
<li><p>RBD: The output of <cite>rbd snap ls --all</cite> command now includes the original
type for trashed snapshots.</p></li>
<li><p>RBD: <cite>RBD_IMAGE_OPTION_CLONE_FORMAT</cite> option has been exposed in Python
bindings via <cite>clone_format</cite> optional parameter to <cite>clone</cite>, <cite>deep_copy</cite> and
<cite>migration_prepare</cite> methods.</p></li>
<li><p>RBD: <cite>RBD_IMAGE_OPTION_FLATTEN</cite> option has been exposed in Python bindings
via <cite>flatten</cite> optional parameter to <cite>deep_copy</cite> and <cite>migration_prepare</cite>
methods.</p></li>
<li><p>RBD: <cite>rbd-wnbd</cite> driver has gained the ability to multiplex image mappings.
Previously, each image mapping spawned its own <cite>rbd-wnbd</cite> daemon, which lead
to an excessive amount of TCP sessions and other resources being consumed,
eventually exceeding Windows limits.  With this change, a single <cite>rbd-wnbd</cite>
daemon is spawned per host and most OS resources are shared between image
mappings.  Additionally, <cite>ceph-rbd</cite> service starts much faster.</p></li>
</ul>
</section>
<section id="rgw">
<h3>RGW<a class="headerlink" href="#rgw" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>RGW: GetObject and HeadObject requests now return a x-rgw-replicated-at
header for replicated objects. This timestamp can be compared against the
Last-Modified header to determine how long the object took to replicate.</p></li>
<li><p>RGW: S3 multipart uploads using Server-Side Encryption now replicate
correctly in multi-site. Previously, the replicas of such objects were
corrupted on decryption.  A new tool, <code class="docutils literal notranslate"><span class="pre">radosgw-admin</span> <span class="pre">bucket</span> <span class="pre">resync</span> <span class="pre">encrypted</span>
<span class="pre">multipart</span></code>, can be used to identify these original multipart uploads. The
<code class="docutils literal notranslate"><span class="pre">LastModified</span></code> timestamp of any identified object is incremented by 1ns to
cause peer zones to replicate it again.  For multi-site deployments that make
any use of Server-Side Encryption, we recommended running this command
against every bucket in every zone after all zones have upgraded.</p></li>
<li><p>RGW: Introducing a new data layout for the Topic metadata associated with S3
Bucket Notifications, where each Topic is stored as a separate RADOS object
and the bucket notification configuration is stored in a bucket attribute.
This new representation supports multisite replication via metadata sync and
can scale to many topics. This is on by default for new deployments, but is
not enabled by default on upgrade. Once all radosgws have upgraded (on all
zones in a multisite configuration), the <code class="docutils literal notranslate"><span class="pre">notification_v2</span></code> zone feature can
be enabled to migrate to the new format. See
<a class="reference external" href="https://docs.ceph.com/en/squid/radosgw/zone-features">https://docs.ceph.com/en/squid/radosgw/zone-features</a> for details. The “v1”
format is now considered deprecated and may be removed after 2 major releases.</p></li>
<li><p>RGW: New tools have been added to radosgw-admin for identifying and
correcting issues with versioned bucket indexes. Historical bugs with the
versioned bucket index transaction workflow made it possible for the index
to accumulate extraneous “book-keeping” olh entries and plain placeholder
entries. In some specific scenarios where clients made concurrent requests
referencing the same object key, it was likely that a lot of extra index
entries would accumulate. When a significant number of these entries are
present in a single bucket index shard, they can cause high bucket listing
latencies and lifecycle processing failures. To check whether a versioned
bucket has unnecessary olh entries, users can now run <code class="docutils literal notranslate"><span class="pre">radosgw-admin</span>
<span class="pre">bucket</span> <span class="pre">check</span> <span class="pre">olh</span></code>. If the <code class="docutils literal notranslate"><span class="pre">--fix</span></code> flag is used, the extra entries will
be safely removed. A distinct issue from the one described thus far, it is
also possible that some versioned buckets are maintaining extra unlinked
objects that are not listable from the S3/ Swift APIs. These extra objects
are typically a result of PUT requests that exited abnormally, in the middle
of a bucket index transaction - so the client would not have received a
successful response. Bugs in prior releases made these unlinked objects easy
to reproduce with any PUT request that was made on a bucket that was actively
resharding. Besides the extra space that these hidden, unlinked objects
consume, there can be another side effect in certain scenarios, caused by
the nature of the failure mode that produced them, where a client of a bucket
that was a victim of this bug may find the object associated with the key to
be in an inconsistent state. To check whether a versioned bucket has unlinked
entries, users can now run <code class="docutils literal notranslate"><span class="pre">radosgw-admin</span> <span class="pre">bucket</span> <span class="pre">check</span> <span class="pre">unlinked</span></code>. If the
<code class="docutils literal notranslate"><span class="pre">--fix</span></code> flag is used, the unlinked objects will be safely removed. Finally,
a third issue made it possible for versioned bucket index stats to be
accounted inaccurately. The tooling for recalculating versioned bucket stats
also had a bug, and was not previously capable of fixing these inaccuracies.
This release resolves those issues and users can now expect that the existing
<code class="docutils literal notranslate"><span class="pre">radosgw-admin</span> <span class="pre">bucket</span> <span class="pre">check</span></code> command will produce correct results. We
recommend that users with versioned buckets, especially those that existed
on prior releases, use these new tools to check whether their buckets are
affected and to clean them up accordingly.</p></li>
<li><p>RGW: The User Accounts feature unlocks several new AWS-compatible IAM APIs
for the self-service management of users, keys, groups, roles, policy and
more. Existing users can be adopted into new accounts. This process is
optional but irreversible. See <a class="reference external" href="https://docs.ceph.com/en/squid/radosgw/account">https://docs.ceph.com/en/squid/radosgw/account</a>
and <a class="reference external" href="https://docs.ceph.com/en/squid/radosgw/iam">https://docs.ceph.com/en/squid/radosgw/iam</a> for details.</p></li>
<li><p>RGW: On startup, radosgw and radosgw-admin now validate the <code class="docutils literal notranslate"><span class="pre">rgw_realm</span></code>
config option. Previously, they would ignore invalid or missing realms and go
on to load a zone/zonegroup in a different realm. If startup fails with a
“failed to load realm” error, fix or remove the <code class="docutils literal notranslate"><span class="pre">rgw_realm</span></code> option.</p></li>
<li><p>RGW: The radosgw-admin commands <code class="docutils literal notranslate"><span class="pre">realm</span> <span class="pre">create</span></code> and <code class="docutils literal notranslate"><span class="pre">realm</span> <span class="pre">pull</span></code> no longer
set the default realm without <code class="docutils literal notranslate"><span class="pre">--default</span></code>.</p></li>
<li><p>RGW: Fixed an S3 Object Lock bug with PutObjectRetention requests that
specify a RetainUntilDate after the year 2106. This date was truncated to 32
bits when stored, so a much earlier date was used for object lock
enforcement.  This does not effect PutBucketObjectLockConfiguration where a
duration is given in Days.  The RetainUntilDate encoding is fixed for new
PutObjectRetention requests, but cannot repair the dates of existing object
locks. Such objects can be identified with a HeadObject request based on the
x-amz-object-lock-retain-until-date response header.</p></li>
<li><p>S3 <code class="docutils literal notranslate"><span class="pre">Get/HeadObject</span></code> now supports the query parameter <code class="docutils literal notranslate"><span class="pre">partNumber</span></code> to read
a specific part of a completed multipart upload.</p></li>
<li><p>RGW: The SNS CreateTopic API now enforces the same topic naming requirements
as AWS: Topic names must be made up of only uppercase and lowercase ASCII
letters, numbers, underscores, and hyphens, and must be between 1 and 256
characters long.</p></li>
<li><p>RGW: Notification topics are now owned by the user that created them.  By
default, only the owner can read/write their topics. Topic policy documents
are now supported to grant these permissions to other users. Preexisting
topics are treated as if they have no owner, and any user can read/write them
using the SNS API.  If such a topic is recreated with CreateTopic, the
issuing user becomes the new owner.  For backward compatibility, all users
still have permission to publish bucket notifications to topics owned by
other users. A new configuration parameter,
<code class="docutils literal notranslate"><span class="pre">rgw_topic_require_publish_policy</span></code>, can be enabled to deny <code class="docutils literal notranslate"><span class="pre">sns:Publish</span></code>
permissions unless explicitly granted by topic policy.</p></li>
<li><p>RGW: Fix issue with persistent notifications where the changes to topic param
that were modified while persistent notifications were in the queue will be
reflected in notifications.  So if the user sets up topic with incorrect config
(password/ssl) causing failure while delivering the notifications to broker,
can now modify the incorrect topic attribute and on retry attempt to delivery
the notifications, new configs will be used.</p></li>
<li><p>RGW: in bucket notifications, the <code class="docutils literal notranslate"><span class="pre">principalId</span></code> inside <code class="docutils literal notranslate"><span class="pre">ownerIdentity</span></code>
now contains the complete user ID, prefixed with the tenant ID.</p></li>
</ul>
</section>
<section id="telemetry">
<h3>Telemetry<a class="headerlink" href="#telemetry" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">basic</span></code> channel in telemetry now captures pool flags that allows us to
better understand feature adoption, such as Crimson.
To opt in to telemetry, run <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">telemetry</span> <span class="pre">on</span></code>.</p></li>
</ul>
<section id="upgrading-from-quincy-or-reef">
<h4>Upgrading from Quincy or Reef<a class="headerlink" href="#upgrading-from-quincy-or-reef" title="Permalink to this heading"></a></h4>
<p>Before starting, make sure your cluster is stable and healthy (no down or recovering OSDs).
(This is optional, but recommended.) You can disable the autoscaler for all pools during the
upgrade using the noautoscale flag.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can monitor the progress of your upgrade at each stage with the <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">versions</span></code> command, which will tell you what ceph version(s) are running for each type of daemon.</p>
</div>
</section>
</section>
<section id="upgrading-cephadm-clusters">
<h3>Upgrading cephadm clusters<a class="headerlink" href="#upgrading-cephadm-clusters" title="Permalink to this heading"></a></h3>
<p>If your cluster is deployed with cephadm (first introduced in Octopus), then the upgrade process is entirely automated. To initiate the upgrade,</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "# ";
}
</style><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>upgrade<span class="w"> </span>start<span class="w"> </span>--image<span class="w"> </span>quay.io/ceph/ceph:v19.2.0</span>
</pre></div></div></div></blockquote>
<p>The same process is used to upgrade to future minor releases.</p>
<p>Upgrade progress can be monitored with</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>upgrade<span class="w"> </span>status</span>
</pre></div></div></div></blockquote>
<p>Upgrade progress can also be monitored with <cite>ceph -s</cite> (which provides a simple progress bar) or more verbosely with</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>-W<span class="w"> </span>cephadm</span>
</pre></div></div></div></blockquote>
<p>The upgrade can be paused or resumed with</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>upgrade<span class="w"> </span>pause<span class="w">  </span><span class="c1"># to pause</span></span>
<span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>upgrade<span class="w"> </span>resume<span class="w"> </span><span class="c1"># to resume</span></span>
</pre></div></div></div></blockquote>
<p>or canceled with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>upgrade<span class="w"> </span>stop</span>
</pre></div></div><p>Note that canceling the upgrade simply stops the process; there is no ability to downgrade back to Quincy or Reef.</p>
</section>
<section id="upgrading-non-cephadm-clusters">
<h3>Upgrading non-cephadm clusters<a class="headerlink" href="#upgrading-non-cephadm-clusters" title="Permalink to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic">
<li><p>If your cluster is running Quincy (17.2.x) or later, you might choose to first convert it to use cephadm so that the upgrade to Squid is automated (see above).
For more information, see <a class="reference external" href="https://docs.ceph.com/en/squid/cephadm/adoption/">https://docs.ceph.com/en/squid/cephadm/adoption/</a>.</p></li>
<li><p>If your cluster is running Quincy (17.2.x) or later, systemd unit file names have changed to include the cluster fsid. To find the correct systemd unit file name for your cluster, run following command:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">systemctl</span> <span class="pre">-l</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">&lt;daemon</span> <span class="pre">type&gt;</span>
<span class="pre">`</span></code></p>
<p>Example:</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">$</span> <span class="pre">systemctl</span> <span class="pre">-l</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">mon</span> <span class="pre">|</span> <span class="pre">grep</span> <span class="pre">active</span>
<span class="pre">ceph-6ce0347c-314a-11ee-9b52-000af7995d6c&#64;mon.f28-h21-000-r630.service</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; <span class="pre">loaded</span> <span class="pre">active</span> <span class="pre">running</span>&#160;&#160; <span class="pre">Ceph</span> <span class="pre">mon.f28-h21-000-r630</span> <span class="pre">for</span> <span class="pre">6ce0347c-314a-11ee-9b52-000af7995d6c</span>
<span class="pre">`</span></code></p>
</li>
</ol>
</div>
<ol class="arabic">
<li><p>Set the <cite>noout</cite> flag for the duration of the upgrade. (Optional, but recommended.)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span><span class="nb">set</span><span class="w"> </span>noout</span>
</pre></div></div></li>
<li><p>Upgrade monitors by installing the new packages and restarting the monitor daemons. For example, on each monitor host</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">systemctl<span class="w"> </span>restart<span class="w"> </span>ceph-mon.target</span>
</pre></div></div><p>Once all monitors are up, verify that the monitor upgrade is complete by looking for the <cite>squid</cite> string in the mon map. The command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>mon<span class="w"> </span>dump<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>min_mon_release</span>
</pre></div></div><p>should report:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">min_mon_release<span class="w"> </span><span class="m">19</span><span class="w"> </span><span class="o">(</span>squid<span class="o">)</span></span>
</pre></div></div><p>If it does not, that implies that one or more monitors hasn’t been upgraded and restarted and/or the quorum does not include all monitors.</p>
</li>
<li><p>Upgrade <cite>ceph-mgr</cite> daemons by installing the new packages and restarting all manager daemons. For example, on each manager host,</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">systemctl<span class="w"> </span>restart<span class="w"> </span>ceph-mgr.target</span>
</pre></div></div><p>Verify the <cite>ceph-mgr</cite> daemons are running by checking <cite>ceph -s</cite>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>-s</span>
</pre></div></div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
  <span class="n">services</span><span class="p">:</span>
   <span class="n">mon</span><span class="p">:</span> <span class="mi">3</span> <span class="n">daemons</span><span class="p">,</span> <span class="n">quorum</span> <span class="n">foo</span><span class="p">,</span><span class="n">bar</span><span class="p">,</span><span class="n">baz</span>
   <span class="n">mgr</span><span class="p">:</span> <span class="n">foo</span><span class="p">(</span><span class="n">active</span><span class="p">),</span> <span class="n">standbys</span><span class="p">:</span> <span class="n">bar</span><span class="p">,</span> <span class="n">baz</span>
<span class="o">...</span>
</pre></div>
</div>
</li>
<li><p>Upgrade all OSDs by installing the new packages and restarting the ceph-osd daemons on all OSD hosts</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">systemctl<span class="w"> </span>restart<span class="w"> </span>ceph-osd.target</span>
</pre></div></div></li>
<li><p>Upgrade all CephFS MDS daemons. For each CephFS file system,</p>
<ol class="arabic">
<li><p>Disable standby_replay:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>fs<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;fs_name&gt;<span class="w"> </span>allow_standby_replay<span class="w"> </span><span class="nb">false</span></span>
</pre></div></div></div></blockquote>
</li>
<li><p>Reduce the number of ranks to 1. (Make note of the original number of MDS daemons first if you plan to restore it later.)</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>status<span class="w"> </span><span class="c1"># ceph fs set &lt;fs_name&gt; max_mds 1</span></span>
</pre></div></div></li>
<li><p>Wait for the cluster to deactivate any non-zero ranks by periodically checking the status</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>status</span>
</pre></div></div></li>
<li><p>Take all standby MDS daemons offline on the appropriate hosts with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">systemctl<span class="w"> </span>stop<span class="w"> </span>ceph-mds@&lt;daemon_name&gt;</span>
</pre></div></div></li>
<li><p>Confirm that only one MDS is online and is rank 0 for your FS</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>status</span>
</pre></div></div></li>
<li><p>Upgrade the last remaining MDS daemon by installing the new packages and restarting the daemon</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">systemctl<span class="w"> </span>restart<span class="w"> </span>ceph-mds.target</span>
</pre></div></div></li>
<li><p>Restart all standby MDS daemons that were taken offline</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">systemctl<span class="w"> </span>start<span class="w"> </span>ceph-mds.target</span>
</pre></div></div></li>
<li><p>Restore the original value of <cite>max_mds</cite> for the volume</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>fs<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;fs_name&gt;<span class="w"> </span>max_mds<span class="w"> </span>&lt;original_max_mds&gt;</span>
</pre></div></div></li>
</ol>
</li>
<li><p>Upgrade all radosgw daemons by upgrading packages and restarting daemons on all hosts</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">systemctl<span class="w"> </span>restart<span class="w"> </span>ceph-radosgw.target</span>
</pre></div></div></li>
<li><p>Complete the upgrade by disallowing pre-Squid OSDs and enabling all new Squid-only functionality</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>require-osd-release<span class="w"> </span>squid</span>
</pre></div></div></li>
<li><p>If you set <cite>noout</cite> at the beginning, be sure to clear it with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span><span class="nb">unset</span><span class="w"> </span>noout</span>
</pre></div></div></li>
<li><p>Consider transitioning your cluster to use the cephadm deployment and orchestration framework to simplify
cluster management and future upgrades. For more information on converting an existing cluster to cephadm,
see <a class="reference external" href="https://docs.ceph.com/en/squid/cephadm/adoption/">https://docs.ceph.com/en/squid/cephadm/adoption/</a>.</p></li>
</ol>
</section>
<section id="post-upgrade">
<h3>Post-upgrade<a class="headerlink" href="#post-upgrade" title="Permalink to this heading"></a></h3>
<ol class="arabic">
<li><p>Verify the cluster is healthy with <cite>ceph health</cite>. If your cluster is running Filestore, and you are upgrading directly from Quincy to Squid, a deprecation warning is expected. This warning can be temporarily muted using the following command</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>health<span class="w"> </span>mute<span class="w"> </span>OSD_FILESTORE</span>
</pre></div></div></li>
<li><p>Consider enabling the <a class="reference external" href="https://docs.ceph.com/en/squid/mgr/telemetry/">telemetry module</a> to send anonymized usage statistics and crash information to the Ceph upstream developers. To see what would be reported (without actually sending any information to anyone),</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>telemetry<span class="w"> </span>preview-all</span>
</pre></div></div><p>If you are comfortable with the data that is reported, you can opt-in to automatically report the high-level cluster metadata with</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>telemetry<span class="w"> </span>on</span>
</pre></div></div><p>The public dashboard that aggregates Ceph telemetry can be found at <a class="reference external" href="https://telemetry-public.ceph.com/">https://telemetry-public.ceph.com/</a>.</p>
</li>
</ol>
</section>
<section id="upgrading-from-pre-quincy-releases-like-pacific">
<h3>Upgrading from pre-Quincy releases (like Pacific)<a class="headerlink" href="#upgrading-from-pre-quincy-releases-like-pacific" title="Permalink to this heading"></a></h3>
<p>You <strong>must</strong> first upgrade to Quincy (17.2.z) or Reef (18.2.z) before upgrading to Squid.</p>
</section>
</section>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../" class="btn btn-neutral float-left" title="Ceph 版本（索引）" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../reef/" class="btn btn-neutral float-right" title="Reef" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>