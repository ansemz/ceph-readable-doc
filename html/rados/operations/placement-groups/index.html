

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>归置组 &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" />
    <link rel="next" title="归置组状态" href="../pg-states/" />
    <link rel="prev" title="分级缓存" href="../cache-tiering/" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../">Ceph 存储集群</a></li>
          <li class="breadcrumb-item"><a href="../">集群运维</a></li>
      <li class="breadcrumb-item active">归置组</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/rados/operations/placement-groups.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../../" class="icon icon-home"> Ceph
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephadm/">Cephadm</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../">Ceph 存储集群</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../configuration/">配置</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../">运维</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../operating/">操纵集群</a></li>
<li class="toctree-l3"><a class="reference internal" href="../health-checks/">健康检查</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitoring/">监控集群</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitoring-osd-pg/">监控 OSD 和归置组</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user-management/">用户管理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pgcalc/">PG Calc</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data-placement/">数据归置概览</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pools/">存储池</a></li>
<li class="toctree-l3"><a class="reference internal" href="../erasure-code/">纠删码</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cache-tiering/">分级缓存</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">归置组</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pg-autoscaler">自动伸缩归置组</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pg-num">预定义 pg_num</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">归置组的使用</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">指定 pg_num 时的相关因素</a></li>
<li class="toctree-l4"><a class="reference internal" href="#choosing-number-of-placement-groups">确定 PG 数量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setting-the-number-of-placement-groups">设置归置组数量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rados-ops-pgs-get-pg-num">获取归置组数量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id15">获取集群的 PG 统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id16">获取卡住的归置组统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id17">获取一归置组运行图</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id18">查看一个 PG 的统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id19">洗刷归置组</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">改变归置组的回填/恢复优先级</a></li>
<li class="toctree-l4"><a class="reference internal" href="#rados">恢复丢失的 RADOS 对象</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../upmap/">使用 pg-upmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="../read-balancer/">Operating the Read (Primary) Balancer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../balancer/">均衡器模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../crush-map/">CRUSH 图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../crush-map-edits/">手动编辑一个 CRUSH 图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../stretch-mode/">Stretch Clusters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../change-mon-elections/">Configuring Monitor Election Strategies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../add-or-rm-osds/">增加/删除 OSD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../add-or-rm-mons/">增加/删除监视器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../devices/">设备管理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bluestore-migration/">迁移到 BlueStore</a></li>
<li class="toctree-l3"><a class="reference internal" href="../control/">命令参考</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/community/">Ceph 社区</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-mon/">监视器故障排除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-osd/">OSD 故障排除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-pg/">归置组排障</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/log-and-debug/">日志记录和调试</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/cpu-profiling/">CPU 剖析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/memory-profiling/">内存剖析</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../man/">    手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../troubleshooting/">故障排除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/">APIs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/developer_guide/">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hardware-monitoring/">硬件监控</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="id1">
<span id="id2"></span><h1>归置组<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h1>
<p>归置组（ PG ）是每个逻辑 Ceph 存储池的子集。
归置组的功能是将对象们（作为一个组）归置进 OSD 。
Ceph 内部以归置组粒度管理数据：
这比管理单个 RADOS 对象的扩展性更好。
归置组较多（例如，每个 OSD 有 150 个归置组）的集群比拥有较少归置组的同等集群的平衡性更好。</p>
<p>Ceph 内部的 RADOS 对象各自都会映射到一个特定的归置组，
每个归置组只能属于一个 Ceph 存储池。</p>
<p>请参阅 Sage Weil 的博文： <a class="reference external" href="https://ceph.io/en/news/blog/2019/new-in-nautilus-pg-merging-and-autotuning/">Nautilus 新功能： PG 合并和自动调整</a>
了解归置组与存储池和对象之间的关系。</p>
<section id="pg-autoscaler">
<span id="id3"></span><h2>自动伸缩归置组<a class="headerlink" href="#pg-autoscaler" title="Permalink to this heading"></a></h2>
<p>归置组（ PG ）是 Ceph 如何分配数据的一个具体的内部实现。
自动伸缩（ autoscaling ）提供了一种管理 PG ，
尤其是管理不同存储池中 PG 数量的方法。
启用 <em>pg-autoscaling</em> 后，集群就能根据集群利用率和预期的存储池利用率对每个存储池的 PG 数量（ <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> ）
提出建议或自动调整。</p>
<p>每个存储池都有一个 <code class="docutils literal notranslate"><span class="pre">pg_autoscale_mode</span></code> 属性，可以设置为
<code class="docutils literal notranslate"><span class="pre">off</span></code> 、 <code class="docutils literal notranslate"><span class="pre">on</span></code> 、或 <code class="docutils literal notranslate"><span class="pre">warn</span></code> ：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">off</span></code>: 此存储池禁用自动伸缩。
由管理员为各个存储池确定合适的 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 。
详情参考 <a class="reference internal" href="#choosing-number-of-placement-groups"><span class="std std-ref">确定 PG 数量</span></a> 。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">on</span></code>: 在指定存储池上启用 PG 数的自动调整。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">warn</span></code>: PG 数应该调整时发出健康报警。</p></li>
</ul>
<p>给现有存储池设置自动伸缩模式，
执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "# ";
}
</style><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>pg_autoscale_mode<span class="w"> </span>&lt;mode&gt;</span>
</pre></div></div><p>例如，要在 <code class="docutils literal notranslate"><span class="pre">foo</span></code> 存储池上启用自动伸缩，执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>foo<span class="w"> </span>pg_autoscale_mode<span class="w"> </span>on</span>
</pre></div></div><p>你也可以配置默认的 <code class="docutils literal notranslate"><span class="pre">pg_autoscale_mode</span></code> ，
它将应用于之后创建的所有存储池。用下列命令更改此配置：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>global<span class="w"> </span>osd_pool_default_pg_autoscale_mode<span class="w"> </span>&lt;mode&gt;</span>
</pre></div></div><p>您可以使用 <code class="docutils literal notranslate"><span class="pre">noautoscale</span></code> 标志禁用或启用所有存储池的自动伸缩功能。
默认情况下，该标记设置为关闭 <code class="docutils literal notranslate"><span class="pre">off</span></code> ，
但可以通过运行以下命令将其设置为开启 <code class="docutils literal notranslate"><span class="pre">on</span></code> ：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>noautoscale</span>
</pre></div></div><p>要把 <code class="docutils literal notranslate"><span class="pre">noautoscale</span></code> 标志设置为 <code class="docutils literal notranslate"><span class="pre">off</span></code> ，执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">unset</span><span class="w"> </span>noautoscale</span>
</pre></div></div><p>要查看这个标志现在的样子，执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>get<span class="w"> </span>noautoscale</span>
</pre></div></div><section id="pg">
<h3>查看 PG 伸缩建议<a class="headerlink" href="#pg" title="Permalink to this heading"></a></h3>
<p>查看各个存储池，其相对利用率、以及 PG 数建议的更改数值，执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>autoscale-status</span>
</pre></div></div><p>命令输出形似如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">POOL</span>    <span class="n">SIZE</span>  <span class="n">TARGET</span> <span class="n">SIZE</span>  <span class="n">RATE</span>  <span class="n">RAW</span> <span class="n">CAPACITY</span>   <span class="n">RATIO</span>  <span class="n">TARGET</span> <span class="n">RATIO</span>  <span class="n">EFFECTIVE</span> <span class="n">RATIO</span> <span class="n">BIAS</span> <span class="n">PG_NUM</span>  <span class="n">NEW</span> <span class="n">PG_NUM</span>  <span class="n">AUTOSCALE</span> <span class="n">BULK</span>
<span class="n">a</span>     <span class="mi">12900</span><span class="n">M</span>                <span class="mf">3.0</span>        <span class="mi">82431</span><span class="n">M</span>  <span class="mf">0.4695</span>                                          <span class="mi">8</span>         <span class="mi">128</span>  <span class="n">warn</span>      <span class="kc">True</span>
<span class="n">c</span>         <span class="mi">0</span>                 <span class="mf">3.0</span>        <span class="mi">82431</span><span class="n">M</span>  <span class="mf">0.0000</span>        <span class="mf">0.2000</span>           <span class="mf">0.9884</span>  <span class="mf">1.0</span>      <span class="mi">1</span>          <span class="mi">64</span>  <span class="n">warn</span>      <span class="kc">True</span>
<span class="n">b</span>         <span class="mi">0</span>        <span class="mf">953.6</span><span class="n">M</span>   <span class="mf">3.0</span>        <span class="mi">82431</span><span class="n">M</span>  <span class="mf">0.0347</span>                                          <span class="mi">8</span>              <span class="n">warn</span>      <span class="kc">False</span>
</pre></div>
</div>
<ul>
<li><p><strong>POOL</strong> 是存储池的名字</p></li>
<li><p><strong>SIZE</strong> 是这个存储池内的数据量</p></li>
<li><p><strong>TARGET SIZE</strong> （如果存在）是此存储池预计要存储的数据量，
由管理员指定。系统会使用两个值中较大的一个进行计算。</p></li>
<li><p><strong>RATE</strong> 是存储池的倍率，能决定消耗多少原始存储容量。
例如，一个三副本存储池的倍率为 3.0 ，
一个 <code class="docutils literal notranslate"><span class="pre">k=4</span> <span class="pre">m=2</span></code> 的纠删码存储池的倍率为 1.5 。</p></li>
<li><p><strong>RAW CAPACITY</strong> 是负责存储池数据
（也许还有其他存储池的数据）
的特定 OSD 上原始存储容量的总量。</p></li>
<li><p><strong>RATIO</strong> 是比率， (1) 存储池消耗的存储空间与
(2) 原始存储总容量的比率。换句话说，
RATIO 的定义是 (SIZE * RATE) / RAW CAPACITY 。</p></li>
<li><p><strong>TARGET RATIO</strong> （如果存在）是此存储池的预期存储量
（即管理员指定的，此存储池预计会消耗的存储量）
与所有其他已设置目标比率的存储池的总预期存储量之比。
如果同时指定 <code class="docutils literal notranslate"><span class="pre">target_size_bytes</span></code> 和 <code class="docutils literal notranslate"><span class="pre">target_size_ratio</span></code> ，
则 <code class="docutils literal notranslate"><span class="pre">target_size_ratio</span></code> 优先。</p></li>
<li><p><strong>EFFECTIVE RATIO</strong> 是对目标比率进行两次调整的结果：</p>
<ol class="arabic simple">
<li><p>减去已设置目标尺寸的存储池预计使用的所有容量。</p></li>
<li><p>对于已设置目标比率的存储池，
把它们的目标比率进行归一化处理，
使它们统称为目标集群容量。例如，
四个 target_ratio 为 1.0 的存储池的有效比率为 0.25 。</p></li>
</ol>
<p>系统在计算时会使用这两个比率
（即目标比率和有效比率）中较大的一个。</p>
</li>
<li><p><strong>BIAS</strong> 是用于手动调整存储池 PG 数的一个乘数，
根据先前的信息，预估某个指定存储池应该有多少个 PG 。</p></li>
<li><p><strong>PG_NUM</strong> 是与存储池相关联的当前 PG 数量，
或者，如果正在进行 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 变更，
则是该存储池正要实现的当前 PG 数量。</p></li>
<li><p><strong>NEW PG_NUM</strong> （如果存在）是系统建议的存储池 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 值。
它总是 2 的幂次，
只有当建议值与当前值的差异超过默认系数 3 时才会出现。
要调整这个倍数（在下面的示例中，
它被改为 2），执行以下命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>threshold<span class="w"> </span><span class="m">2</span>.0</span>
</pre></div></div></li>
<li><p><strong>AUTOSCALE</strong> 是存储池的 <code class="docutils literal notranslate"><span class="pre">pg_autoscale_mode</span></code> ，可设置为
<code class="docutils literal notranslate"><span class="pre">on</span></code> 、 <code class="docutils literal notranslate"><span class="pre">off</span></code> 或 <code class="docutils literal notranslate"><span class="pre">warn</span></code> 。</p></li>
<li><p><strong>BULK</strong> 指明此存储池是否为巨型的（ <code class="docutils literal notranslate"><span class="pre">bulk</span></code> ），
它的值为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 或 <code class="docutils literal notranslate"><span class="pre">False</span></code> 。巨型池（ bulk pool ）预计规模会很大，
而且应该一开始就拥有大量 PG ，这样性能才不会受到影响。
另一方面，非巨型存储池的规模应该较小
（例如 <code class="docutils literal notranslate"><span class="pre">.mgr</span></code> 存储池或元数据存储池）。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>如果 <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">autoscale-status</span></code> 命令什么也没有输出，
那么有可能至少有一个存储池跨越了多个 CRUSH 根节点。
这种 “跨越存储池” 的问题可能出现在以下情形：
当新部署的集群在 CRUSH 的 <code class="docutils literal notranslate"><span class="pre">default</span></code> 根上自动创建 <code class="docutils literal notranslate"><span class="pre">.mgr</span></code> 池时，
后续创建存储池时，给它们分配的规则会限制在特定的 CRUSH 影子树上。
例如，假设创建了一个 RBD 元数据存储池，
并分配 <code class="docutils literal notranslate"><span class="pre">deviceclass</span> <span class="pre">=</span> <span class="pre">ssd</span></code> ，
还有个 RBD 数据存储池分配了 <code class="docutils literal notranslate"><span class="pre">deviceclass</span>&#160; <span class="pre">=</span> <span class="pre">hdd</span></code> ，
就会遇到这个问题。
要解决这个问题，请将存储池限制为只有一个设备类别。
在上述场景下，可能有一个 <code class="docutils literal notranslate"><span class="pre">replicated-ssd</span></code> CRUSH 规则正在生效，
可以通过运行以下命令将 <code class="docutils literal notranslate"><span class="pre">.mgr</span></code> 存储池分配给 <code class="docutils literal notranslate"><span class="pre">ssd</span></code> 设备：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>.mgr<span class="w"> </span>crush_rule<span class="w"> </span>replicated-ssd</span>
</pre></div></div><p>这种干预会导致少量回填，
但通常这种流量会很快完成。</p>
</div>
</section>
<section id="id4">
<h3>自动化的伸缩<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h3>
<p>在自动伸缩的方法中，最简单的可以允许集群根据使用情况自动伸缩 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 。
Ceph 会考虑整个系统的总空闲空间和 PG 的目标数量、
考虑每个存储池中存储了多少数据，并相应地分配 PG 。
系统的做法比较保守，只有在当前 PG 数量（ <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> ）与建议数量相差超过 3 倍时，才会对存储池进行更改。</p>
<p>每个 OSD 的目标 PG 数由 <code class="docutils literal notranslate"><span class="pre">mon_target_pg_per_osd</span></code>
参数（默认值： 100 ）决定，
可执行以下命令进行调整：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>global<span class="w"> </span>mon_target_pg_per_osd<span class="w"> </span><span class="m">100</span></span>
</pre></div></div><p>自动伸缩器会分析存储池，并根据每个子树进行调整。
由于每个存储池可能映射到不同的 CRUSH 规则，
而每个规则可能会把数据摊派到不同的设备上，
因此 Ceph 将独立考虑分级结构中每个子树的利用率。例如，
映射到 <code class="docutils literal notranslate"><span class="pre">ssd</span></code> 类 OSD 的存储池和映射到 <code class="docutils literal notranslate"><span class="pre">hdd</span></code> 类 OSD 的存储池各自的最佳 PG 数量将取决于这两种不同类型设备的数量。</p>
<p>如果存储池使用了两个或更多 CRUSH 根下的 OSD
（例如，同时使用 <code class="docutils literal notranslate"><span class="pre">ssd</span></code> 和 <code class="docutils literal notranslate"><span class="pre">hdd</span></code> 设备的影子树），
自动伸缩器会在管理器日志中向用户发出警告。
警告中会指出存储池的名称和哪些根相互重叠了。
自动伸缩器不会伸缩任何有重叠根的存储池，
因为这种情况会导致伸缩过程出现问题。
我们建议把每个存储池都限定在一个根内（即一个 OSD 类别），
以消除警告并确保缩放过程成功。</p>
<section id="bulk">
<span id="managing-bulk-flagged-pools"></span><h4>管理带有 <code class="docutils literal notranslate"><span class="pre">bulk</span></code> 标志的存储池<a class="headerlink" href="#bulk" title="Permalink to this heading"></a></h4>
<p>如果一个存储池打了 <code class="docutils literal notranslate"><span class="pre">bulk</span></code> 标志，那么自动伸缩器在启动存储池时就会创建所有 PG ，只有当整个存储池的使用率不均衡时，
才会缩减 PG 的数量。但是，如果一个存储池没有打 <code class="docutils literal notranslate"><span class="pre">bulk</span></code> 标志，
那么自动伸缩器会以最少的 PG 启动这个存储池，
只有当存储池的使用量增加时，才会另外创建 PG 。</p>
<p>要创建一个带 <code class="docutils literal notranslate"><span class="pre">bulk</span></code> 标志的存储池，执行以下命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>create<span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>--bulk</span>
</pre></div></div><p>要设置或者取消现有存储池的 <code class="docutils literal notranslate"><span class="pre">bulk</span></code> 标志，执行以下命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>bulk<span class="w"> </span>&lt;true/false/1/0&gt;</span>
</pre></div></div><p>要查看现有存储池的 <code class="docutils literal notranslate"><span class="pre">bulk</span></code> 标志，执行以下命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>get<span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>bulk</span>
</pre></div></div></section>
</section>
<section id="specifying-pool-target-size">
<span id="id5"></span><h3>配置期望的存储池尺寸<a class="headerlink" href="#specifying-pool-target-size" title="Permalink to this heading"></a></h3>
<p>集群或存储池刚创建时，只消耗了集群总容量的一小部分，
在系统看来应该只需要少量的 PG 。
但在某些情况下，集群管理员知道长期运行后，
哪些存储池可能会消耗系统的大部分容量。
如果向 Ceph 提供了这些信息，可以从一开始就分配更合适的 PG 数量，
从而避免随后更改 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 以及随之而来的重新归位数据带来的开销。</p>
<p>存储池的<em>目标大小（ target size ）</em>有两种指定方式：
一种是设置存储池的绝对大小（以字节为单位），
一种是相对于所有其他设置了 <code class="docutils literal notranslate"><span class="pre">target_size_ratio</span></code> 的存储池的权重。</p>
<p>例如，告诉系统， <code class="docutils literal notranslate"><span class="pre">mypool</span></code> 预计会消耗 100 TB，
执行以下命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>mypool<span class="w"> </span>target_size_bytes<span class="w"> </span>100T</span>
</pre></div></div><p>或者告诉系统，相对于其他设置了 <code class="docutils literal notranslate"><span class="pre">target_size_ratio</span></code> 的存储池，
<code class="docutils literal notranslate"><span class="pre">mypool</span></code> 存储池预计消耗比率为 1.0 ，
要调整 <code class="docutils literal notranslate"><span class="pre">mypool</span></code> 的 <code class="docutils literal notranslate"><span class="pre">target_size_ratio</span></code> 选项的值，
执行以下命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>mypool<span class="w"> </span>target_size_ratio<span class="w"> </span><span class="m">1</span>.0</span>
</pre></div></div><p>如果 <code class="docutils literal notranslate"><span class="pre">mypool</span></code> 是集群中唯一的存储池，那么预计它会用掉集群总容量的 100% 。
但是，如果集群中包含第二个存储池，
它的 <code class="docutils literal notranslate"><span class="pre">target_size_ratio</span></code> 设置成了 1.0 ，
那么预计两个存储池各自将使用集群总容量的 50% 。</p>
<p>命令 <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">create</span></code> 有两个命令行选项，
可在创建存储池时设置其目标大小：
<code class="docutils literal notranslate"><span class="pre">--target-size-bytes</span> <span class="pre">&lt;bytes&gt;</span></code> 和 <code class="docutils literal notranslate"><span class="pre">--target-size-ratio</span> <span class="pre">&lt;ratio&gt;</span></code> 。</p>
<p>注意，如果指定的目标大小值（ target-size value ）不可能实现
（例如，指定的容量比整个集群都大），则会触发健康检查消息
（ <code class="docutils literal notranslate"><span class="pre">POOL_TARGET_SIZE_BYTES_OVERCOMMITTED</span></code> ）。</p>
<p>如果同时为存储池指定了 <code class="docutils literal notranslate"><span class="pre">target_size_ratio</span></code> 和 <code class="docutils literal notranslate"><span class="pre">target_size_bytes</span></code> ，
那么后者将被忽略，前者将用于系统计算，
并触发健康检查消息（ <code class="docutils literal notranslate"><span class="pre">POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO</span></code> ）。</p>
</section>
<section id="id6">
<h3>设置存储池的 PG 数量界限<a class="headerlink" href="#id6" title="Permalink to this heading"></a></h3>
<p>可以给存储池指定最小 PG 数和最大 PG 数。</p>
<section id="pg-pg">
<h4>设置最小 PG 数和最大 PG 数<a class="headerlink" href="#pg-pg" title="Permalink to this heading"></a></h4>
<p>如果设置了最小值，则 Ceph 自己不会将 PG 数量缩减
（也不建议您缩减）到配置值以下。
设置最小值的目的是为了给客户端 I/O 过程的并行数量设定一个下限，
即使存储池大部分是空的也是如此。</p>
<p>如果设置了最大值，则 Ceph 自己不会将 PG 数量增加
（也不建议您增加）到配置值以上。</p>
<p>要设置存储池的最小 PG 数量，
执行以下形式的命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>pg_num_min<span class="w"> </span>&lt;num&gt;</span>
</pre></div></div><p>要设置存储池的最大 PG 数，执行以下命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span>&lt;pool-name&gt;<span class="w"> </span>pg_num_max<span class="w"> </span>&lt;num&gt;</span>
</pre></div></div><p>另外，命令 <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">create</span></code> 有两个命令行选项，
可以用于在创建时指定存储池的最小或最大 PG 数：
<code class="docutils literal notranslate"><span class="pre">--pg-num-min</span> <span class="pre">&lt;num&gt;</span></code> 和 <code class="docutils literal notranslate"><span class="pre">--pg-num-max</span> <span class="pre">&lt;num&gt;</span></code> 。</p>
</section>
</section>
</section>
<section id="pg-num">
<span id="preselection"></span><h2>预定义 pg_num<a class="headerlink" href="#pg-num" title="Permalink to this heading"></a></h2>
<p>用下列命令创建存储池时，你可以加选项预先设置 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 参数的值：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>create<span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span><span class="w"> </span><span class="o">[</span>pg_num<span class="o">]</span></span>
</pre></div></div><p>如果你不在这个命令里指定 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> ，
集群会基于存储池内的数据量，用 PG 自动伸缩器自动配置这个参数，
（见 <a class="reference internal" href="#pg-autoscaler"><span class="std std-ref">自动伸缩归置组</span></a> ）。</p>
<p>不过，创建时指定或不指定 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 对以后集群是否会自动调整该参数没有任何影响。
在前面已经叙述过， PG 的自动伸缩是通过运行以下命令启用或禁用的：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span><span class="w"> </span>pg_autoscale_mode<span class="w"> </span><span class="o">(</span>on<span class="p">|</span>off<span class="p">|</span>warn<span class="o">)</span></span>
</pre></div></div><p>在没有均衡器的情况下，建议的目标是每个 OSD 上大约 100 个 PG 副本。
有均衡器时，初始目标为每个 OSD 上 50 个 PG 副本是合理的。</p>
<p>自动伸缩器试图满足以下条件：</p>
<ul class="simple">
<li><p>每个 OSD 上的 PG 数量应该与存储池中的数据量成比例</p></li>
<li><p>每个存储池应该有 50-100 个 PG ，
包括复制开销或纠删码扇出时各个 PG 在 OSD 上的数量。</p></li>
</ul>
</section>
<section id="id7">
<h2>归置组的使用<a class="headerlink" href="#id7" title="Permalink to this heading"></a></h2>
<p>存储池内的归置组（ PG ）把对象汇聚在一起，
因为跟踪每一个 RADOS 对象的位置及其元数据的计算代价太大——
即一个拥有数百万对象的系统，不可能在对象这一级追踪位置。</p>
<p class="ditaa">
<img src="../../../_images/ditaa-e06a40fed57ca0d6741624eddbbb2c2f8c15d142.png"/>
</p>
<p>Ceph 客户端会计算一个 RADOS 对象应该位于哪个归置组里。
计算时，客户端先给对象 ID 做散列计算，
然后再根据指定存储池里的 PG 数量、存储池 ID 做一个运算。
详情见 <a class="reference external" href="../../../architecture#mapping-pgs-to-osds">PG 映射到 OSD</a> 。</p>
<p>PG 内一个 RADOS 对象的内容存储在一组 OSD 中。
例如，在副本数为 2 的多副本存储池中，每个 PG 将在两个 OSD 上存储对象，
如下图所示：</p>
<p class="ditaa">
<img src="../../../_images/ditaa-819f2e337d50117d4d528affa922b47104cc3b19.png"/>
</p>
<p>如果 OSD #2 出现故障，另一个 OSD 将被分配给归置组 #1 ，
然后用 OSD #1 中所有对象的副本来填充。如果存储池的副本数从两个变为三个，
则将为 PG 分配另一个 OSD ，它将接收 PG 中所有对象的副本。</p>
<p>分配给 PG 的 OSD 并没有被这个 PG 独占，
而是与来自同一存储池或其他存储池的其他 PG 共享。
在本例中， OSD #2 由归置组 #1 和归置组 #2 共享。
如果 OSD #2 出现故障，那么归置组 #2 必须恢复对象副本
（利用 OSD #3 ）。</p>
<p>PG 数量增加时，会产生几种后果。
新 PG 会被分配 OSD 。 CRUSH 函数的结果会发生变化，
这意味着现有 PG 中的某些对象会被复制到新 PG 中，
并从旧的 PG 中移除。</p>
</section>
<section id="id8">
<h2>指定 pg_num 时的相关因素<a class="headerlink" href="#id8" title="Permalink to this heading"></a></h2>
<p>一方面，从数据的持久性和在 OSD 之间的均匀分布来看，使用较多的 PG 比较好；
另一方面，从节省 CPU 资源和尽量减少内存使用量的角度考虑， PG 数量越少越好。</p>
<section id="data-durability">
<span id="id9"></span><h3>数据持久性<a class="headerlink" href="#data-durability" title="Permalink to this heading"></a></h3>
<p>当 OSD 出现故障时，数据丢失的风险就会增加，
除非它托管的数据已经恢复到配置的级别。为了说明这一点，
让我们设想一种情形，会导致一个 PG 内数据永久丢失：</p>
<ol class="arabic simple">
<li><p>某一 OSD 出现故障，其中包含的对象副本全部丢失。
对于 PG 中的每个对象，其副本数量突然从三个减少到两个。</p></li>
<li><p>Ceph 开始恢复此 PG ，选取一个新的 OSD ，
并在该 OSD 上重新创建每个对象的第三个副本。</p></li>
<li><p>同一个 PG 中的另一个 OSD 在新 OSD 完全填充好第三个副本之前发生了故障。
这样，某些对象将只有一个副本仍健在。</p></li>
<li><p>Ceph 选择了另一个 OSD ，并继续复制对象，
以恢复到期望的副本数。</p></li>
<li><p>这个 PG 中的第三个 OSD 在恢复完成前发生了故障。
如果该 OSD 恰好包含某一对象的唯一剩余副本，
则这个对象将永久丢失。</p></li>
</ol>
<p>在一个包含 10 个 OSD 和 512 个 PG 的三副本存储池集群中，
CRUSH 将为每个 PG 分配三个 OSD 。最终，
每个 OSD 承载 <span class="math notranslate nohighlight">\(\frac{(512 * 3)}{10} = ~150\)</span> 个 PG 。
因此，在上述情形下，当第一个 OSD 出现故障时，所有 150 个 PG 将同时开始恢复。</p>
<p>被恢复的 150 个 PG 可能会均匀地分布在剩余的 9 个 OSD 中。
因此，每个仍健在的 OSD 都有可能向所有其他 OSD 发送对象副本，
也有可能要存入一些新对象，
因为它已成为新 PG 的一部分。</p>
<p>完成此类恢复所需的时间取决于 Ceph 集群的架构。
比较两种配置情况：(1) 每个 OSD 由单台计算机上的
1 TB SSD 托管，所有 OSD 都连接到 10 Gb/s 交换机，
单个 OSD 的恢复在可确定的分钟数内完成。
(2) 每台计算机上有两个 OSD ，使用的是不带 SSD WAL+DB 的 HDD 和
1 Gb/s 交换机。在第二种配置中，
恢复速度至少要慢一个数量级。</p>
<p>在这样的集群中， PG 的数量对数据的持久性几乎没有影响。
无论每个 OSD 有 128 个 PG 还是 8192 个 PG，
恢复速度都不会更慢或更快。</p>
<p>不过，增加 OSD 的数量可以提高恢复速度。
假设我们的 Ceph 集群从 10 个 OSD 扩展到 20 个 OSD 。
每个 OSD 现在只参与 ~75 个 PG，而不是 ~150 个 PG。
所有健在的 19 个 OSD 仍然需要复制同样数量的对象才能恢复。
但现在有 20 个 OSD ，每个只需复制 50 GB，
而不是原来的 10 个 OSD ，每个需要复制 ~100 GB 。
如果以前网络是瓶颈，那么现在的恢复速度是以前的两倍。</p>
<p>同样，假设我们的集群发展到 40 个 OSD 。每个 OSD 将只托管 ~38 个 PG 。
如果一个 OSD 掉线，恢复速度会比以前更快，
除非它被另一个瓶颈阻塞。现在，假设我们的集群增加到 200 个 OSD 。
每个 OSD 只需要承载 ~7 个 PG 。如果一个 OSD 掉线，
恢复过程最多涉及与这些 PG 相关的 <span class="math notranslate nohighlight">\(\approx 21 = (7 \times 3)\)</span> 个 OSD 。
这意味着恢复要花费的时间比只有 40 个 OSD 的时候更长。
因此，应该增加 PG 数量。</p>
<p>不论恢复时间有多短，
在此期间都可能有别的 OSD 发生故障。
在前述的有 10 个 OSD 的集群中：如果有任何一个 OSD 出现故障，
都有 <span class="math notranslate nohighlight">\(\approx 17\)</span> （大约 150 除以 9 ）个归置组将只有一份可用副本；
再次假设，剩余的 8 个 OSD 中任意一个失败，
那么， 2 （大约 17 除以 8 ）个归置组中存储的对象就有可能丢失。
这就是为什么设置 <code class="docutils literal notranslate"><span class="pre">size=2</span></code> 风险很大的原因。</p>
<p>当集群里 OSD 数量增长为 20 个时，
失去 3 个 OSD 导致归置组损坏的数量会明显降低。
失去第二个 OSD 会降级大约 <span class="math notranslate nohighlight">\(4\)</span> 或 (<span class="math notranslate nohighlight">\(\frac{75}{19}\)</span>) 个 PG ，
而不是 <span class="math notranslate nohighlight">\(\approx 17\)</span> 个归置组，并且此时失去第三个 OSD 时，
它恰好是包含其余副本的 4 个 OSD 中的一个时，才会导致数据丢失。
换句话说，假设在恢复期间丢失一个 OSD 的概率是 0.0001% ，
那么，在包含 10 个 OSD 的集群中丢失 3 个 OSD 导致数据丢失的概率是 <span class="math notranslate nohighlight">\(\approx 17 \times 10 \times 0.0001%\)</span> ，
而在 20 个 OSD 的集群中将是 <span class="math notranslate nohighlight">\(\approx 4 \times 20 \times 0.0001%\)</span> 。</p>
<p>总之， OSD 数量越多、恢复速度越快、
因级联故障而永久丢失 PG 的风险越低。
就数据持久性而言，在少于 50 个 OSD 的集群中，
PG 是 512 个还是 4096 个都无所谓，影响不大。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>最近加入进集群的 OSD 可能需要很长时间才能填满分配给它的 PG 。不过，
由于 Ceph 在把数据从旧 PG 中删除之前会先将数据填入新 PG 中，
因此这一过程虽然漫长，但不会导致对象降级或影响数据的持久性。</p>
</div>
</section>
<section id="object-distribution">
<span id="id10"></span><h3>一个存储池内的对象分布<a class="headerlink" href="#object-distribution" title="Permalink to this heading"></a></h3>
<p>在理想情况下，对象应该均匀地分布在 PG 中。
由于 CRUSH 会计算每个对象的 PG ，
但它并不知道与 PG 相关的每个 OSD 中存储了多少数据，
而 PG 数量和 OSD 数量的比率会对数据分布产生显著影响。</p>
<p>例如，假设在一个三副本存储池中，十个 OSD 只有一个 PG 。
在这种情况下，由于 CRUSH 没得选，只能使用三个 OSD 。
但是，如果有更多 PG 可用，
RADOS 对象就更有可能平均分配到这些 OSD 上。
CRUSH 会尽力将 OSD 平均地分配给所有现有的 PG 。</p>
<p>只要 PG 的数量比 OSD 多一到两个数量级，
分配就有可能是均匀的。例如： 256 个 PG 对应 3 个 OSD ，
512 个 PG 对应 10 个 OSD ，或 1024 个 PG 对应 10 个 OSD。</p>
<p>然而，除了 PG 与 OSD 的比例之外，其他因素也会导致数据分布不均。
例如，由于 CRUSH 并未考虑 RADOS 对象的大小，
因此，如果出现几个非常大的 RADOS 对象，就会造成不平衡。
假设有 100 万个 4 KB 的 RADOS 对象，总大小为 4 GB，
它们平均分布在 10 个 OSD 上的 1024 个 PG 中。
这些 RADOS 对象在每个 OSD 上将消耗 4 GB / 10 = 400 MB 的空间。
如果向存储池中存入一个大小为 400 MB 的 RADOS 对象，
那么存放这个 RADOS 对象的 PG 对应的三个 OSD 将分别填入 400 MB + 400 MB = 800 MB ，
但其他七个 OSD 仍然只用掉 400 MB。</p>
</section>
<section id="resource-usage">
<span id="id11"></span><h3>内存、处理器和网络使用情况<a class="headerlink" href="#resource-usage" title="Permalink to this heading"></a></h3>
<p>集群中的每一个归置组都会对 OSD 和 MON 产生内存、网络、处理器方面的需求。
这些需求必须随时得到满足，在恢复期间还会增加。
事实上，开发出 PG 的主要原因之一就是通过把对象聚集在一起来分担这种开销。</p>
<p>因此，最小化 PG 数量可节省不少资源。</p>
</section>
</section>
<section id="choosing-number-of-placement-groups">
<span id="id12"></span><h2>确定 PG 数量<a class="headerlink" href="#choosing-number-of-placement-groups" title="Permalink to this heading"></a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>极少有必要按照本小节手动做数学计算。
而是用 <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">autoscale-status</span></code> 命令，
加上 <code class="docutils literal notranslate"><span class="pre">target_size_bytes</span></code> 或 <code class="docutils literal notranslate"><span class="pre">target_size_ratio</span></code> 存储池属性即可。
详情见 <a class="reference internal" href="#pg-autoscaler"><span class="std std-ref">自动伸缩归置组</span></a> 。</p>
</div>
<p>如果 OSD 超过 50 个，我们建议每个 OSD 持有大约 50-100 个 PG ，
以平衡资源使用、数据持久性和数据分布。如果 OSD 少于 50 个，
请遵循<a class="reference internal" href="#pg-num">预定义 pg_num</a>小节的指导。对于单个存储池，使用下列公式获得基准值：</p>
<blockquote>
<div><p>PG 总数 = <span class="math notranslate nohighlight">\(\frac{OSDs \times 100}{pool \: size}\)</span></p>
</div></blockquote>
<p>这里存储池副本数（ pool size ）是指多副本存储池的副本数或纠删码存储池的 K+M 之和。
要获取这个和，运行命令 <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">erasure-code-profile</span> <span class="pre">get</span></code> 。</p>
<p>接下来，检验一下生成的基准值是否与您设计的 Ceph 集群一致，即最大限度地提高<a class="reference internal" href="#id9">数据持久性</a>和<a class="reference internal" href="#id10">一个存储池内的对象分布</a>，并最大限度地减少<a class="reference internal" href="#id11">内存、处理器和网络使用情况</a>。</p>
<p>这个值应该<strong>四舍五入到最接近的 2 的幂次</strong>。</p>
<p>每个存储池的 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 应该是 2 的幂次。
其他值可能会导致数据在 OSD 中的分布不均匀。
最好在设置下一个最高的 2 的幂次是可行和可取的情况下，
再增加存储池的 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 。注意， 2 的幂次这个规则是针对每个存储池的；
把所有存储池的 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 之和凑成 2 的幂次既没有必要，
也不容易。</p>
<p>比如，一个配置了 200 个 OSD 且存储池副本数为 3 的集群，
你可以这样估算归置组数量：</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\frac{200 \times 100}{3} = 6667\)</span>. 四舍五入到最接近的 2 的幂: 8192</p>
</div></blockquote>
<p>当用了多个数据存储池来存储数据时，
你得确保均衡每个存储池的 PG 数量、每个 OSD 的 PG 数量，
以便得到合理的 PG 总量。重要的是，要找到一个数量，
既能为每个 OSD 提供相对较低的差异，又不会额外增加对系统资源的负担或者使互联进程过于缓慢。</p>
<p>以这样一个集群为例，它拥有 10 个存储池、每个存储池有 512 个归置组，
分布在 10 个 OSD 上；即 5120 个 PG 散布在 10 个 OSD 上，
也就是平均每个 OSD 上有 512 个归置组；如此配置，不会占用太多资源。然而，
如果集群有 1000 个存储池，且各存储池分别有 512 个 PG ，
那么每个 OSD 就得处理 5 万多个归置组，
这样的话光是建立互联就需要不少资源和时间。</p>
</section>
<section id="setting-the-number-of-placement-groups">
<span id="id13"></span><h2>设置归置组数量<a class="headerlink" href="#setting-the-number-of-placement-groups" title="Permalink to this heading"></a></h2>
<p><a class="reference internal" href="../pgcalc/#pgcalc"><span class="std std-ref">归置组链接</span></a></p>
<p>要设置某存储池的初始归置组数量，你必须在创建它时就指定好，
详情见 <a class="reference external" href="../pools#createpool">创建存储池</a>。</p>
<p>即使某一存储池已经创建，如果没用 <code class="docutils literal notranslate"><span class="pre">pg_autoscaler</span></code> 去管理 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 数值，
你仍然可以用下面的命令更改归置组数量：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span><span class="w"> </span>pg_num<span class="w"> </span><span class="o">{</span>pg_num<span class="o">}</span></span>
</pre></div></div><p>你增加归置组数量后、
还必须增加用于归置的归置组（ <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> ）数量，
这样才会开始重均衡。 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 数值才是
CRUSH 算法采用的用于归置的归置组数量。
虽然 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 的增加引起了归置组的分割，
但是只有当用于归置的归置组（即 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> ）增加以后，
数据才会被迁移到新归置组里。 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 的数值应等于 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 。
可用下列命令增加用于归置的归置组数量：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span><span class="w"> </span>pgp_num<span class="w"> </span><span class="o">{</span>pgp_num<span class="o">}</span></span>
</pre></div></div><p>减少归置组数量时，系统会自动调整 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 数值。
在 Nautilus 及更高版本（含）的 Ceph 中，
如果不使用 <code class="docutils literal notranslate"><span class="pre">pg_autoscaler</span></code> ， <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 会自动跟随 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 的数值。
这一过程体现在 PG 的重新映射和回填时，是意料中的正常行为。</p>
</section>
<section id="rados-ops-pgs-get-pg-num">
<span id="id14"></span><h2>获取归置组数量<a class="headerlink" href="#rados-ops-pgs-get-pg-num" title="Permalink to this heading"></a></h2>
<p>要获取一个存储池的归置组数量，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>get<span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span><span class="w"> </span>pg_num</span>
</pre></div></div></section>
<section id="id15">
<h2>获取集群的 PG 统计信息<a class="headerlink" href="#id15" title="Permalink to this heading"></a></h2>
<p>要获取集群里归置组的详细信息，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span>dump<span class="w"> </span><span class="o">[</span>--format<span class="w"> </span><span class="o">{</span>format<span class="o">}]</span></span>
</pre></div></div><p>可用格式有纯文本 <code class="docutils literal notranslate"><span class="pre">plain</span></code> （默认）和 <code class="docutils literal notranslate"><span class="pre">json</span></code> 。</p>
</section>
<section id="id16">
<h2>获取卡住的归置组统计信息<a class="headerlink" href="#id16" title="Permalink to this heading"></a></h2>
<p>要获取所有卡在某状态的归置组统计信息，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span>dump_stuck<span class="w"> </span>inactive<span class="p">|</span>unclean<span class="p">|</span>stale<span class="p">|</span>undersized<span class="p">|</span>degraded<span class="w"> </span><span class="o">[</span>--format<span class="w"> </span>&lt;format&gt;<span class="o">]</span><span class="w"> </span><span class="o">[</span>-t<span class="p">|</span>--threshold<span class="w"> </span>&lt;seconds&gt;<span class="o">]</span></span>
</pre></div></div><ul class="simple">
<li><p><strong>Inactive</strong> （不活跃）归置组不能处理读写，因为它们在等待足够多有最新数据的 OSD 回到 <code class="docutils literal notranslate"><span class="pre">up</span></code> 且 <code class="docutils literal notranslate"><span class="pre">in</span></code> 状态。</p></li>
<li><p><strong>Undersized</strong> （副本不够）归置组含有副本数尚未达到期望数量的对象。
正常情况下，可以认为这些 PG 正在恢复中。</p></li>
<li><p><strong>Stale</strong> （不新鲜）归置组处于未知状态：存储它们的 OSD 有段时间没向监视器报告了（由 <code class="docutils literal notranslate"><span class="pre">mon_osd_report_timeout</span></code> 配置）。</p></li>
</ul>
<p>可用格式有 <code class="docutils literal notranslate"><span class="pre">plain</span></code> （默认）和 <code class="docutils literal notranslate"><span class="pre">json</span></code> 。阀值定义的是，
归置组被认为卡住前等待的最短时间，秒数（默认 300 秒）。</p>
</section>
<section id="id17">
<h2>获取一归置组运行图<a class="headerlink" href="#id17" title="Permalink to this heading"></a></h2>
<p>要获取一个具体归置组的归置组图（ PG map ），执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span>map<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span></span>
</pre></div></div><p>例如：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span>map<span class="w"> </span><span class="m">1</span>.6c</span>
</pre></div></div><p>Ceph 将返回归置组图、归置组、和 OSD 状态，其输出长相如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">osdmap<span class="w"> </span>e13<span class="w"> </span>pg<span class="w"> </span><span class="m">1</span>.6c<span class="w"> </span><span class="o">(</span><span class="m">1</span>.6c<span class="o">)</span><span class="w"> </span>-&gt;<span class="w"> </span>up<span class="w"> </span><span class="o">[</span><span class="m">1</span>,0<span class="o">]</span><span class="w"> </span>acting<span class="w"> </span><span class="o">[</span><span class="m">1</span>,0<span class="o">]</span></span>
</pre></div></div></section>
<section id="id18">
<h2>查看一个 PG 的统计信息<a class="headerlink" href="#id18" title="Permalink to this heading"></a></h2>
<p>要查看一个具体归置组的统计信息，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span>query</span>
</pre></div></div></section>
<section id="id19">
<h2>洗刷归置组<a class="headerlink" href="#id19" title="Permalink to this heading"></a></h2>
<p>要强行立即洗刷一个 PG ，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>tell<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span>scrub</span>
</pre></div></div><p>或者</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>tell<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span>deep-scrub</span>
</pre></div></div><p>Ceph 会检查主 OSD 和副本 OSD ，生成归置组里所有对象的目录。
对于每一个对象， Ceph 都会比对它（位于主 OSD 和副本 OSD 内的）的所有例程，
确保它们是一致的。浅层洗刷（由第一种命令格式启动），只比较对象的元数据；
深层洗刷（由第二种命令格式启动）还会比较对象的内容。
如果所有副本都匹配，就进行最终的语义扫描，
确保所有与快照相关的对象元数据是一致的。错误会在日志里报告。</p>
<p>使用上述命令格式启动的洗刷被视为高优先级，会立即执行。
这样的洗刷不受仍在生效的星期几、
几点这些常规的、定期的洗刷规则的约束；
也不受 ‘osd_max_scrubs’ 的限制，也不用等待那些副本的洗刷资源。</p>
<p>第二种命令格式用于启动洗刷，就像定期洗刷一样。命令格式如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>tell<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span>schedule-scrub</span>
</pre></div></div><p>或者</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>tell<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span>schedule-deep-scrub</span>
</pre></div></div><p>要洗刷指定存储池内的所有 PG ，
执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>scrub<span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span></span>
</pre></div></div></section>
<section id="id20">
<h2>改变归置组的回填/恢复优先级<a class="headerlink" href="#id20" title="Permalink to this heading"></a></h2>
<p>你可能会遇到这样的情形，有一大堆归置组需要恢复和/或回填，
但是其中有几个 PG 内的数据比其它 PG 内的更重要
（例如，那些 PG 持有正在运行着的机器的映像数据，
而其它 PG 上的数据是不太活跃的机器、或不相干的数据用着）。
在那种情形下，你可能想更改那些归置组的恢复优先级，
这样它们的性能和/或数据可用性就可以早些恢复。
要提高特定归置组在恢复期间的优先级，
执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span>force-recovery<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span><span class="o">[{</span>pg-id<span class="w"> </span><span class="c1">#2}] [{pg-id #3} ...]</span></span>
</pre></div></div><p>要提高特定归置组在回填期间的优先级，
执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span>force-backfill<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span><span class="o">[{</span>pg-id<span class="w"> </span><span class="c1">#2}] [{pg-id #3} ...]</span></span>
</pre></div></div><p>执行这些命令后， Ceph 就会优先处理指定归置组的恢复或回填工作，
其它则靠后。变更优先级不会中断当前正在进行的回填或恢复，
而是把指定的 PG 放到队列的最前面，这样接下来就轮到它了。
如果你改主意了，或者弄错了归置组，
可以用如下命令取消：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span>cancel-force-recovery<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span><span class="o">[{</span>pg-id<span class="w"> </span><span class="c1">#2}] [{pg-id #3} ...]</span></span>
<span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span>cancel-force-backfill<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span><span class="o">[{</span>pg-id<span class="w"> </span><span class="c1">#2}] [{pg-id #3} ...]</span></span>
</pre></div></div><p>这些命令会删除那些 PG 的 <code class="docutils literal notranslate"><span class="pre">force</span></code> 标记，
然后系统会按正常顺序处理它们。与添加 <code class="docutils literal notranslate"><span class="pre">force</span></code> 标记的情况一样，
这操作不会影响当前正处理着的归置组，
只影响还在队列里的那些。</p>
<p>这些归置组恢复或回填完后，
<code class="docutils literal notranslate"><span class="pre">force</span></code> 标记会被自动清除。</p>
<p>同样，要让 Ceph 优先处理指定存储池中的所有 PG
（即首先执行这些 PG 的恢复或回填），
执行以下一条或两条命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>force-recovery<span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span></span>
<span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>force-backfill<span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span></span>
</pre></div></div><p>这些命令也可以取消。要恢复到默认顺序，
运行以下一条或两条命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>cancel-force-recovery<span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span></span>
<span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span>cancel-force-backfill<span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span></span>
</pre></div></div><div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>这些命令会破坏 Ceph 内部计算优先级的顺序，
因此请谨慎使用！如果当前有多个存储池共享相同的底层 OSD ，
并且某些存储池持有的数据比其他存储池持有的数据更重要，
那么我们建议您执行以下命令，
为所有存储池安排恢复/回填优先级：</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>osd<span class="w"> </span>pool<span class="w"> </span><span class="nb">set</span><span class="w"> </span><span class="o">{</span>pool-name<span class="o">}</span><span class="w"> </span>recovery_priority<span class="w"> </span><span class="o">{</span>value<span class="o">}</span></span>
</pre></div></div><p>例如，假设有 20 个存储池，可以把最重要的存储池优先级设为 <code class="docutils literal notranslate"><span class="pre">20</span></code> ，
次重要的存储池优先级设为 <code class="docutils literal notranslate"><span class="pre">19</span></code> ，以此类推。</p>
<p>另一种方法是只为精心挑选的一部分存储池设置恢复/回填优先级。
在这种情况下，三个重要存储池可能（全部）被指定为优先级 <code class="docutils literal notranslate"><span class="pre">1</span></code> ，
而所有其他存储池则没有指定恢复/回填优先级。
另一种可能是，选择三个重要存储池，
将其恢复/回填优先级分别设置为 <code class="docutils literal notranslate"><span class="pre">3</span></code> 、 <code class="docutils literal notranslate"><span class="pre">2</span></code> 和 <code class="docutils literal notranslate"><span class="pre">1</span></code> 。</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>用 <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">set</span> <span class="pre">{pool-name}</span> <span class="pre">recovery_priority</span> <span class="pre">{value}</span></code>
设置恢复/回填优先级时，
数值大的比数值小的优先级高。
例如，恢复/回填优先级为 <code class="docutils literal notranslate"><span class="pre">30</span></code> 的存储池比恢复/回填优先级为 <code class="docutils literal notranslate"><span class="pre">15</span></code> 的存储池优先级更高。</p>
</div>
</section>
<section id="rados">
<h2>恢复丢失的 RADOS 对象<a class="headerlink" href="#rados" title="Permalink to this heading"></a></h2>
<p>如果集群丢了一或多个 RADOS 对象，
而且必须放弃搜索这些数据，
你就要把未找到的对象标记为丢失（ <code class="docutils literal notranslate"><span class="pre">lost</span></code> ）。</p>
<p>如果所有可能的位置都查询过了，所有的 OSD 状态也是 <code class="docutils literal notranslate"><span class="pre">up</span></code> 且 <code class="docutils literal notranslate"><span class="pre">in</span></code> ，
仍然找不到这些 RADOS 对象，你也许得放弃它们了。
这可能是罕见的、不寻常的故障组合，
它们导致集群在真正的写入恢复并完成前就已经得知：写入已执行。</p>
<p>把 RADOS 对象标记为已丢失（ <code class="docutils literal notranslate"><span class="pre">lost</span></code> ）的命令只支持 <code class="docutils literal notranslate"><span class="pre">revert</span></code> 一个选项。
<code class="docutils literal notranslate"><span class="pre">revert</span></code> 选项可以回滚到 RADOS 对象的前一个版本
（如果它足够老，有以前的版本），也可以完全忘记它
（如果它太新了，没有以前的老版本）。
要把找不到的（ unfound ）对象标记为 <code class="docutils literal notranslate"><span class="pre">lost</span></code> ，执行下列命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>pg<span class="w"> </span><span class="o">{</span>pg-id<span class="o">}</span><span class="w"> </span>mark_unfound_lost<span class="w"> </span>revert<span class="p">|</span>delete</span>
</pre></div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>此功能要谨慎使用。它可能会导致那些期望对象存在的应用程序错乱。</p>
</div>
<div class="toctree-wrapper compound">
</div>
</section>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../cache-tiering/" class="btn btn-neutral float-left" title="分级缓存" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../pg-states/" class="btn btn-neutral float-right" title="归置组状态" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>