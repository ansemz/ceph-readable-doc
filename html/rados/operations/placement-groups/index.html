
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>归置组 &#8212; Ceph Documentation</title>
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css" />
    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/js/ceph.js"></script>
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" />
    <link rel="next" title="归置组状态" href="../pg-states/" />
    <link rel="prev" title="分级缓存" href="../cache-tiering/" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../pg-states/" title="归置组状态"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../cache-tiering/" title="分级缓存"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../">Ceph Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../" >Ceph 存储集群</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../" accesskey="U">集群运维</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            

<div id="dev-warning" class="admonition note" style="display:none;">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>

<div id="eol-warning" class="admonition warning" style="display:none;">
  <p class="first admonition-title">Warning</p>
  <p class="last">This document is for an unsupported version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="display:none; padding: 15px; font-weight: bold;">
    <a id="edit-on-github" href="https://github.com/ceph/ceph/edit/master/doc/rados/operations/placement-groups.rst" rel="nofollow">Edit on GitHub</a> | <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <div class="section" id="id1">
<h1>归置组<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<div class="section" id="pg-autoscaler">
<span id="id2"></span><h2>自伸缩归置组<a class="headerlink" href="#pg-autoscaler" title="Permalink to this headline">¶</a></h2>
<p>归置组（ PG ）是 Ceph 如何散布数据的一个内部实现细节。启用
<em>pg-autoscaling</em> 后，你可以基于集群的用法让集群做出推荐或者自动调整 PG 数。</p>
<p>系统内的每个存储池都有一个 <code class="docutils literal notranslate"><span class="pre">pg_autoscale_mode</span></code> 属性，可以设置为 <code class="docutils literal notranslate"><span class="pre">off</span></code> 、 <code class="docutils literal notranslate"><span class="pre">on</span></code> 、或 <code class="docutils literal notranslate"><span class="pre">warn</span></code> 。</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">off</span></code>: 此存储池禁用自伸缩。由管理员为各个存储池选择合适的
PG 数量。详情参考 <a class="reference internal" href="#choosing-number-of-placement-groups"><span class="std std-ref">确定归置组数量</span></a> 。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">on</span></code>: 在指定存储池上启用 PG 数的自动调整。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">warn</span></code>: PG 数应该调整时发出健康报警。</p></li>
</ul>
<p>为现有存储池设置自伸缩模式，</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="nb">set</span> <span class="o">&lt;</span><span class="n">pool</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="n">pg_autoscale_mode</span> <span class="o">&lt;</span><span class="n">mode</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>例如，要在 <code class="docutils literal notranslate"><span class="pre">foo</span></code> 存储池上启用自伸缩：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="nb">set</span> <span class="n">foo</span> <span class="n">pg_autoscale_mode</span> <span class="n">on</span>
</pre></div>
</div>
<p>你也可以配置默认的 <code class="docutils literal notranslate"><span class="pre">pg_autoscale_mode</span></code> ，它将应用于以后创建的所有存储池：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">config</span> <span class="nb">set</span> <span class="k">global</span> <span class="n">osd_pool_default_pg_autoscale_mode</span> <span class="o">&lt;</span><span class="n">mode</span><span class="o">&gt;</span>
</pre></div>
</div>
<div class="section" id="pg">
<h3>查看 PG 伸缩建议<a class="headerlink" href="#pg" title="Permalink to this headline">¶</a></h3>
<p>用此命令可以查看各个存储池、其相对利用率、以及 PG 数建议的更改数值：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="n">autoscale</span><span class="o">-</span><span class="n">status</span>
</pre></div>
</div>
<p>命令输出形似如下：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">POOL</span>    <span class="n">SIZE</span>  <span class="n">TARGET</span> <span class="n">SIZE</span>  <span class="n">RATE</span>  <span class="n">RAW</span> <span class="n">CAPACITY</span>   <span class="n">RATIO</span>  <span class="n">TARGET</span> <span class="n">RATIO</span>  <span class="n">PG_NUM</span>  <span class="n">NEW</span> <span class="n">PG_NUM</span>  <span class="n">AUTOSCALE</span>
<span class="n">a</span>     <span class="mi">12900</span><span class="n">M</span>                <span class="mf">3.0</span>        <span class="mi">82431</span><span class="n">M</span>  <span class="mf">0.4695</span>                     <span class="mi">8</span>         <span class="mi">128</span>  <span class="n">warn</span>
<span class="n">c</span>         <span class="mi">0</span>                 <span class="mf">3.0</span>        <span class="mi">82431</span><span class="n">M</span>  <span class="mf">0.0000</span>        <span class="mf">0.2000</span>       <span class="mi">1</span>          <span class="mi">64</span>  <span class="n">warn</span>
<span class="n">b</span>         <span class="mi">0</span>        <span class="mf">953.6</span><span class="n">M</span>   <span class="mf">3.0</span>        <span class="mi">82431</span><span class="n">M</span>  <span class="mf">0.0347</span>                     <span class="mi">8</span>              <span class="n">warn</span>
</pre></div>
</div>
<p><strong>SIZE</strong> is the amount of data stored in the pool. <strong>TARGET SIZE</strong>, if
present, is the amount of data the administrator has specified that
they expect to eventually be stored in this pool.  The system uses
the larger of the two values for its calculation.</p>
<p><strong>RATE</strong> is the multiplier for the pool that determines how much raw
storage capacity is consumed.  For example, a 3 replica pool will
have a ratio of 3.0, while a k=4,m=2 erasure coded pool will have a
ratio of 1.5.</p>
<p><strong>RAW CAPACITY</strong> is the total amount of raw storage capacity on the
OSDs that are responsible for storing this pool’s (and perhaps other
pools’) data.  <strong>RATIO</strong> is the ratio of that total capacity that
this pool is consuming (i.e., ratio = size * rate / raw capacity).</p>
<p><strong>TARGET RATIO</strong>, if present, is the ratio of storage that the
administrator has specified that they expect this pool to consume.
The system uses the larger of the actual ratio and the target ratio
for its calculation.  If both target size bytes and ratio are specified, the
ratio takes precedence.</p>
<p><strong>PG_NUM</strong> is the current number of PGs for the pool (or the current
number of PGs that the pool is working towards, if a <code class="docutils literal notranslate"><span class="pre">pg_num</span></code>
change is in progress).  <strong>NEW PG_NUM</strong>, if present, is what the
system believes the pool’s <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> should be changed to.  It is
always a power of 2, and will only be present if the “ideal” value
varies from the current value by more than a factor of 3.</p>
<p>The final column, <strong>AUTOSCALE</strong>, is the pool <code class="docutils literal notranslate"><span class="pre">pg_autoscale_mode</span></code>,
and will be either <code class="docutils literal notranslate"><span class="pre">on</span></code>, <code class="docutils literal notranslate"><span class="pre">off</span></code>, or <code class="docutils literal notranslate"><span class="pre">warn</span></code>.</p>
</div>
<div class="section" id="id3">
<h3>自动化的伸缩<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>Allowing the cluster to automatically scale PGs based on usage is the
simplest approach.  Ceph will look at the total available storage and
target number of PGs for the whole system, look at how much data is
stored in each pool, and try to apportion the PGs accordingly.  The
system is relatively conservative with its approach, only making
changes to a pool when the current number of PGs (<code class="docutils literal notranslate"><span class="pre">pg_num</span></code>) is more
than 3 times off from what it thinks it should be.</p>
<p>The target number of PGs per OSD is based on the
<code class="docutils literal notranslate"><span class="pre">mon_target_pg_per_osd</span></code> configurable (default: 100), which can be
adjusted with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">config</span> <span class="nb">set</span> <span class="k">global</span> <span class="n">mon_target_pg_per_osd</span> <span class="mi">100</span>
</pre></div>
</div>
<p>The autoscaler analyzes pools and adjusts on a per-subtree basis.
Because each pool may map to a different CRUSH rule, and each rule may
distribute data across different devices, Ceph will consider
utilization of each subtree of the hierarchy independently.  For
example, a pool that maps to OSDs of class <cite>ssd</cite> and a pool that maps
to OSDs of class <cite>hdd</cite> will each have optimal PG counts that depend on
the number of those respective device types.</p>
</div>
<div class="section" id="specifying-pool-target-size">
<span id="id4"></span><h3>配置期望的存储池尺寸<a class="headerlink" href="#specifying-pool-target-size" title="Permalink to this headline">¶</a></h3>
<p>When a cluster or pool is first created, it will consume a small
fraction of the total cluster capacity and will appear to the system
as if it should only need a small number of placement groups.
However, in most cases cluster administrators have a good idea which
pools are expected to consume most of the system capacity over time.
By providing this information to Ceph, a more appropriate number of
PGs can be used from the beginning, preventing subsequent changes in
<code class="docutils literal notranslate"><span class="pre">pg_num</span></code> and the overhead associated with moving data around when
those adjustments are made.</p>
<p>The <em>target size*</em> of a pool can be specified in two ways: either in
terms of the absolute size of the pool (i.e., bytes), or as a ratio of
the total cluster capacity.</p>
<p>For example,:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="nb">set</span> <span class="n">mypool</span> <span class="n">target_size_bytes</span> <span class="mi">100</span><span class="n">T</span>
</pre></div>
</div>
<p>will tell the system that <cite>mypool</cite> is expected to consume 100 TiB of
space.  Alternatively,:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="nb">set</span> <span class="n">mypool</span> <span class="n">target_size_ratio</span> <span class="o">.</span><span class="mi">9</span>
</pre></div>
</div>
<p>will tell the system that <cite>mypool</cite> is expected to consume 90% of the
total cluster capacity.</p>
<p>You can also set the target size of a pool at creation time with the optional <code class="docutils literal notranslate"><span class="pre">--target-size-bytes</span> <span class="pre">&lt;bytes&gt;</span></code> or <code class="docutils literal notranslate"><span class="pre">--target-size-ratio</span> <span class="pre">&lt;ratio&gt;</span></code> arguments to the <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">pool</span> <span class="pre">create</span></code> command.</p>
<p>Note that if impossible target size values are specified (for example,
a capacity larger than the total cluster, or ratio(s) that sum to more
than 1.0) then a health warning
(<code class="docutils literal notranslate"><span class="pre">POOL_TARET_SIZE_RATIO_OVERCOMMITTED</span></code> or
<code class="docutils literal notranslate"><span class="pre">POOL_TARGET_SIZE_BYTES_OVERCOMMITTED</span></code>) will be raised.</p>
</div>
<div class="section" id="id5">
<h3>设置存储池的 PG 数量界限<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>It is also possible to specify a minimum number of PGs for a pool.
This is useful for establishing a lower bound on the amount of
parallelism client will see when doing IO, even when a pool is mostly
empty.  Setting the lower bound prevents Ceph from reducing (or
recommending you reduce) the PG number below the configured number.</p>
<p>You can set the minimum number of PGs for a pool with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="nb">set</span> <span class="o">&lt;</span><span class="n">pool</span><span class="o">-</span><span class="n">name</span><span class="o">&gt;</span> <span class="n">pg_num_min</span> <span class="o">&lt;</span><span class="n">num</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>You can also specify the minimum PG count at pool creation time with
the optional <code class="docutils literal notranslate"><span class="pre">--pg-num-min</span> <span class="pre">&lt;num&gt;</span></code> argument to the <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">pool</span>
<span class="pre">create</span></code> command.</p>
</div>
</div>
<div class="section" id="pg-num">
<span id="preselection"></span><h2>预定义 pg_num<a class="headerlink" href="#pg-num" title="Permalink to this headline">¶</a></h2>
<p>用此命令创建存储池时：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="n">create</span> <span class="p">{</span><span class="n">pool</span><span class="o">-</span><span class="n">name</span><span class="p">}</span> <span class="n">pg_num</span>
</pre></div>
</div>
<p>确定 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 取值是强制性的，因为不能自动计算。下面是几个常用的值：</p>
<ul class="simple">
<li><p>少于 5 个 OSD 时可把 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 设置为 128</p></li>
<li><p>OSD 数量在 5 到 10 个时，可把 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 设置为 512</p></li>
<li><p>OSD 数量在 10 到 50 个时，可把 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 设置为 1024</p></li>
<li><p>OSD 数量大于 50 时，你得理解权衡方法、以及如何自己计算 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 取值</p></li>
<li><p>自己计算 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 取值时可借助 <a class="reference external" href="http://ceph.com/pgcalc/">pgcalc</a> 工具</p></li>
</ul>
<p>随着 OSD 数量的增加，正确的 pg_num 取值变得更加重要，因为它显著地影响着集群的行为、以及出错时的数据持久性（即灾难性事件导致数据丢失的概率）。</p>
</div>
<div class="section" id="id6">
<h2>归置组是如何使用的？<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>存储池内的归置组（ PG ）把对象汇聚在一起，因为跟踪每一个对象的位置及其元数据需要大量计算——即一个拥有数百万对象的系统，不可能在对象这一级追踪位置。</p>
<p class="ditaa">
<img src="../../../_images/ditaa-ed04edf277e7a91121edf8404565ef973813a9b4.png"/>
</p>
<p>Ceph 客户端会计算某一对象应该位于哪个归置组里，它是这样实现的，先给对象 ID 做哈希操作，然后再根据指定存储池里的 PG 数量、存储池 ID 做一个运算。详情见 <a class="reference external" href="../../../architecture#mapping-pgs-to-osds">PG 映射到 OSD</a> 。</p>
<p>The object’s contents within a placement group are stored in a set of
OSDs. For instance, in a replicated pool of size two, each placement
group will store objects on two OSDs, as shown below.</p>
<p class="ditaa">
<img src="../../../_images/ditaa-90213ef6077d302853c82075a6b4f12990d7600a.png"/>
</p>
<p>Should OSD #2 fail, another will be assigned to Placement Group #1 and
will be filled with copies of all objects in OSD #1. If the pool size
is changed from two to three, an additional OSD will be assigned to
the placement group and will receive copies of all objects in the
placement group.</p>
<p>Placement groups do not own the OSD; they share it with other
placement groups from the same pool or even other pools. If OSD #2
fails, the Placement Group #2 will also have to restore copies of
objects, using OSD #3.</p>
<p>When the number of placement groups increases, the new placement
groups will be assigned OSDs. The result of the CRUSH function will
also change and some objects from the former placement groups will be
copied over to the new Placement Groups and removed from the old ones.</p>
</div>
<div class="section" id="placement-groups-tradeoffs">
<h2>Placement Groups Tradeoffs<a class="headerlink" href="#placement-groups-tradeoffs" title="Permalink to this headline">¶</a></h2>
<p>Data durability and even distribution among all OSDs call for more
placement groups but their number should be reduced to the minimum to
save CPU and memory.</p>
<div class="section" id="data-durability">
<span id="id7"></span><h3>Data durability<a class="headerlink" href="#data-durability" title="Permalink to this headline">¶</a></h3>
<p>After an OSD fails, the risk of data loss increases until the data it
contained is fully recovered. Let’s imagine a scenario that causes
permanent data loss in a single placement group:</p>
<ul class="simple">
<li><p>某一 OSD 失败了，然后其内的所有对象都丢失了。对于那个归置组内的所有对象来说，副本数一下子从 3 降到了 2 。</p></li>
<li><p>Ceph 开始恢复这个归置组，选定一个新 OSD ，并重建所有对象的第三份副本。</p></li>
<li><p>Another OSD, within the same placement group, fails before the new
OSD is fully populated with the third copy. Some objects will then
only have one surviving copies.</p></li>
<li><p>Ceph picks yet another OSD and keeps copying objects to restore the
desired number of copies.</p></li>
<li><p>A third OSD, within the same placement group, fails before recovery
is complete. If this OSD contained the only remaining copy of an
object, it is permanently lost.</p></li>
</ul>
<p>In a cluster containing 10 OSDs with 512 placement groups in a three
replica pool, CRUSH will give each placement groups three OSDs. In the
end, each OSDs will end up hosting (512 * 3) / 10 = ~150 Placement
Groups. When the first OSD fails, the above scenario will therefore
start recovery for all 150 placement groups at the same time.</p>
<p>The 150 placement groups being recovered are likely to be
homogeneously spread over the 9 remaining OSDs. Each remaining OSD is
therefore likely to send copies of objects to all others and also
receive some new objects to be stored because they became part of a
new placement group.</p>
<p>The amount of time it takes for this recovery to complete entirely
depends on the architecture of the Ceph cluster. Let say each OSD is
hosted by a 1TB SSD on a single machine and all of them are connected
to a 10Gb/s switch and the recovery for a single OSD completes within
M minutes. If there are two OSDs per machine using spinners with no
SSD journal and a 1Gb/s switch, it will at least be an order of
magnitude slower.</p>
<p>In a cluster of this size, the number of placement groups has almost
no influence on data durability. It could be 128 or 8192 and the
recovery would not be slower or faster.</p>
<p>However, growing the same Ceph cluster to 20 OSDs instead of 10 OSDs
is likely to speed up recovery and therefore improve data durability
significantly. Each OSD now participates in only ~75 placement groups
instead of ~150 when there were only 10 OSDs and it will still require
all 19 remaining OSDs to perform the same amount of object copies in
order to recover. But where 10 OSDs had to copy approximately 100GB
each, they now have to copy 50GB each instead. If the network was the
bottleneck, recovery will happen twice as fast. In other words,
recovery goes faster when the number of OSDs increases.</p>
<p>If this cluster grows to 40 OSDs, each of them will only host ~35
placement groups. If an OSD dies, recovery will keep going faster
unless it is blocked by another bottleneck. However, if this cluster
grows to 200 OSDs, each of them will only host ~7 placement groups. If
an OSD dies, recovery will happen between at most of ~21 (7 * 3) OSDs
in these placement groups: recovery will take longer than when there
were 40 OSDs, meaning the number of placement groups should be
increased.</p>
<p>不论恢复时间有多短，在此期间都可能有第二个 OSD 失败。在前述的有 10 个 OSD 的集群中，不管哪个失败了，都有大约 17 个归置组（即需恢复的大约 150 / 9 个归置组）将只有一份可用副本；并且假设剩余的 8 个 OSD 中任意一个失败，两个归置组中最后的对象都有可能丢失（即正在恢复的、大约 17 / 8 个仅剩一个副本的归置组）。</p>
<p>当集群大小变为 20 个 OSD 时， 3 个 OSD 丢失导致的归置组损坏会降低。第二个 OSD 丢失会降级大约 4 个（即需恢复的归置组约为 75 / 19 ）而不是约 17 个归置组，并且只有当第三个 OSD 恰好是包含可用副本的四分之一个 OSD 时、才会丢失数据。换句话说，假设在恢复期间丢失一个 OSD 的概率是 0.0001% ，那么，在包含 10 个 OSD 的集群中丢失 OSD 的概率是 17 * 10 * 0.0001% ，而在 20 个 OSD 的集群中将是 4 * 20 * 0.0001% 。</p>
<p>In a nutshell, more OSDs mean faster recovery and a lower risk of
cascading failures leading to the permanent loss of a Placement
Group. Having 512 or 4096 Placement Groups is roughly equivalent in a
cluster with less than 50 OSDs as far as data durability is concerned.</p>
<p>Note: It may take a long time for a new OSD added to the cluster to be
populated with placement groups that were assigned to it. However
there is no degradation of any object and it has no impact on the
durability of the data contained in the Cluster.</p>
</div>
<div class="section" id="object-distribution-within-a-pool">
<span id="object-distribution"></span><h3>Object distribution within a pool<a class="headerlink" href="#object-distribution-within-a-pool" title="Permalink to this headline">¶</a></h3>
<p>Ideally objects are evenly distributed in each placement group. Since
CRUSH computes the placement group for each object, but does not
actually know how much data is stored in each OSD within this
placement group, the ratio between the number of placement groups and
the number of OSDs may influence the distribution of the data
significantly.</p>
<p>For instance, if there was a single placement group for ten OSDs in a
three replica pool, only three OSD would be used because CRUSH would
have no other choice. When more placement groups are available,
objects are more likely to be evenly spread among them. CRUSH also
makes every effort to evenly spread OSDs among all existing Placement
Groups.</p>
<p>As long as there are one or two orders of magnitude more Placement
Groups than OSDs, the distribution should be even. For instance, 300
placement groups for 3 OSDs, 1000 placement groups for 10 OSDs etc.</p>
<p>Uneven data distribution can be caused by factors other than the ratio
between OSDs and placement groups. Since CRUSH does not take into
account the size of the objects, a few very large objects may create
an imbalance. Let say one million 4K objects totaling 4GB are evenly
spread among 1000 placement groups on 10 OSDs. They will use 4GB / 10
= 400MB on each OSD. If one 400MB object is added to the pool, the
three OSDs supporting the placement group in which the object has been
placed will be filled with 400MB + 400MB = 800MB while the seven
others will remain occupied with only 400MB.</p>
</div>
<div class="section" id="resource-usage">
<span id="id8"></span><h3>内存、处理器和网络使用情况<a class="headerlink" href="#resource-usage" title="Permalink to this headline">¶</a></h3>
<p>各个归置组、 OSD 和监视器都一直需要内存、网络、处理器，在恢复期间需求更大。为消除过载而把对象聚集成簇是归置组存在的主要原因。</p>
<p>最小化归置组数量可节省不少资源。</p>
</div>
</div>
<div class="section" id="choosing-number-of-placement-groups">
<span id="id9"></span><h2>确定归置组数量<a class="headerlink" href="#choosing-number-of-placement-groups" title="Permalink to this headline">¶</a></h2>
<p>If you have more than 50 OSDs, we recommend approximately 50-100
placement groups per OSD to balance out resource usage, data
durability and distribution. If you have less than 50 OSDs, chosing
among the <a class="reference internal" href="#preselection">preselection</a> above is best. For a single pool of objects,
you can use the following formula to get a baseline:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>             <span class="p">(</span><span class="n">OSDs</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">Total</span> <span class="n">PGs</span> <span class="o">=</span>  <span class="o">------------</span>
              <span class="n">pool</span> <span class="n">size</span>
</pre></div>
</div>
<p>Where <strong>pool size</strong> is either the number of replicas for replicated
pools or the K+M sum for erasure coded pools (as returned by <strong>ceph
osd erasure-code-profile get</strong>).</p>
<p>You should then check if the result makes sense with the way you
designed your Ceph cluster to maximize <a class="reference internal" href="#data-durability">data durability</a>,
<a class="reference internal" href="#object-distribution">object distribution</a> and minimize <a class="reference internal" href="#resource-usage">resource usage</a>.</p>
<p>其结果<strong>汇总后应该接近 2 的幂</strong>。汇总并非强制的，如果你想确保所有归置组内的对象数大致相等，最好检查下。</p>
<p>比如，一个配置了 200 个 OSD 且副本数为 3 的集群，你可以这样估算归置组数量：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">200</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
<span class="o">-----------</span> <span class="o">=</span> <span class="mf">6667.</span> <span class="n">Nearest</span> <span class="n">power</span> <span class="n">of</span> <span class="mi">2</span><span class="p">:</span> <span class="mi">8192</span>
     <span class="mi">3</span>
</pre></div>
</div>
<p>当用了多个数据存储池来存储数据时，你得确保均衡每个存储池的归置组数量、且归置组数量分摊到每个 OSD ，这样才能达到较合理的归置组总量，并因此使得每个 OSD 无需耗费过多系统资源或拖慢连接进程就能实现较小变迁。</p>
<p>以这样一个集群为例，它拥有 10 个存储池、每个存储池有 512 个归置组，分布在 10 个 OSD 上；即 5120 个归置组散布在 10 个 OSD
上，也就是平均每个 OSD 上有 512 个归置组；如此配置，不会占用太多资源。然而，如果创建的是 1000 个存储池，且各存储池分别有 512
个归置组，那么每个 OSD 就得处理 5 万多个归置组，这样的话光是建立互联就需要不少资源和时间。</p>
<p>你可以借助于 <a class="reference external" href="http://ceph.com/pgcalc/">PGCalc</a> 工具。</p>
</div>
<div class="section" id="setting-the-number-of-placement-groups">
<span id="id10"></span><h2>设置归置组数量<a class="headerlink" href="#setting-the-number-of-placement-groups" title="Permalink to this headline">¶</a></h2>
<p>要设置某存储池的归置组数量，你必须在创建它时就指定好，详情见<a class="reference external" href="../pools#createpool">创建存储池</a>。即使某一存储池已创建，你仍然可以用下面的命令更改归置组数量：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="nb">set</span> <span class="p">{</span><span class="n">pool</span><span class="o">-</span><span class="n">name</span><span class="p">}</span> <span class="n">pg_num</span> <span class="p">{</span><span class="n">pg_num</span><span class="p">}</span>
</pre></div>
</div>
<p>你增加归置组数量后、还必须增加用于归置的归置组（ <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> ）数量，这样才会开始重均衡。 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 数值才是 CRUSH 算法采用的用于归置的归置组数量。虽然 <code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 的增加引起了归置组的分割，但是只有当用于归置的归置组（即 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> ）增加以后，数据才会被迁移到新归置组里。 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 的数值应等于
<code class="docutils literal notranslate"><span class="pre">pg_num</span></code> 。可用下列命令增加用于归置的归置组数量：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="nb">set</span> <span class="p">{</span><span class="n">pool</span><span class="o">-</span><span class="n">name</span><span class="p">}</span> <span class="n">pgp_num</span> <span class="p">{</span><span class="n">pgp_num</span><span class="p">}</span>
</pre></div>
</div>
<p>减少归置组数量时，系统会自动调整 <code class="docutils literal notranslate"><span class="pre">pgp_num</span></code> 数值。</p>
</div>
<div class="section" id="id11">
<h2>获取归置组数量<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>要获取一个存储池的归置组数量，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">osd</span> <span class="n">pool</span> <span class="n">get</span> <span class="p">{</span><span class="n">pool</span><span class="o">-</span><span class="n">name</span><span class="p">}</span> <span class="n">pg_num</span>
</pre></div>
</div>
</div>
<div class="section" id="id12">
<h2>获取归置组统计信息<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<p>要获取集群里归置组的统计信息，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="n">dump</span> <span class="p">[</span><span class="o">--</span><span class="nb">format</span> <span class="p">{</span><span class="nb">format</span><span class="p">}]</span>
</pre></div>
</div>
<p>可用格式有纯文本 <code class="docutils literal notranslate"><span class="pre">plain</span></code> （默认）和 <code class="docutils literal notranslate"><span class="pre">json</span></code> 。</p>
</div>
<div class="section" id="id13">
<h2>获取卡住的归置组统计信息<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<p>要获取所有卡在某状态的归置组统计信息，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="n">dump_stuck</span> <span class="n">inactive</span><span class="o">|</span><span class="n">unclean</span><span class="o">|</span><span class="n">stale</span><span class="o">|</span><span class="n">undersized</span><span class="o">|</span><span class="n">degraded</span> <span class="p">[</span><span class="o">--</span><span class="nb">format</span> <span class="o">&lt;</span><span class="nb">format</span><span class="o">&gt;</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">t</span><span class="o">|--</span><span class="n">threshold</span> <span class="o">&lt;</span><span class="n">seconds</span><span class="o">&gt;</span><span class="p">]</span>
</pre></div>
</div>
<p><strong>Inactive</strong> （不活跃）归置组不能处理读写，因为它们在等待一个有最新数据的 OSD 复活且进入集群。</p>
<p><strong>Unclean</strong> （不干净）归置组含有复制数未达到期望数量的对象，它们应该在恢复中。</p>
<p><strong>Stale</strong> （不新鲜）归置组处于未知状态：存储它们的 OSD 有段时间没向监视器报告了（由  <code class="docutils literal notranslate"><span class="pre">mon_osd_report_timeout</span></code> 配置）。</p>
<p>可用格式有 <code class="docutils literal notranslate"><span class="pre">plain</span></code> （默认）和 <code class="docutils literal notranslate"><span class="pre">json</span></code> 。阀值定义的是，归置组被认为卡住前等待的最小时间（默认 300 秒）。</p>
</div>
<div class="section" id="id14">
<h2>获取一归置组运行图<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<p>要获取一个具体归置组的归置组图，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="nb">map</span> <span class="p">{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span><span class="p">}</span>
</pre></div>
</div>
<p>例如：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="nb">map</span> <span class="mf">1.6</span><span class="n">c</span>
</pre></div>
</div>
<p>Ceph 将返回归置组图、归置组、和 OSD 状态：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">osdmap</span> <span class="n">e13</span> <span class="n">pg</span> <span class="mf">1.6</span><span class="n">c</span> <span class="p">(</span><span class="mf">1.6</span><span class="n">c</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">up</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="n">acting</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="id15">
<h2>获取一 PG 的统计信息<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h2>
<p>要查看一个具体归置组的统计信息，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="p">{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span><span class="p">}</span> <span class="n">query</span>
</pre></div>
</div>
</div>
<div class="section" id="id16">
<h2>洗刷归置组<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h2>
<p>要洗刷一个归置组，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="n">scrub</span> <span class="p">{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span><span class="p">}</span>
</pre></div>
</div>
<p>Ceph 检查原始的和任何复制节点，生成归置组里所有对象的目录，然后再对比，确保没有对象丢失或不匹配，并且它们的内容一致。</p>
</div>
<div class="section" id="id17">
<h2>改变归置组的回填/恢复优先级<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h2>
<p>你可能会遇到这样的情形，有一大堆归置组需要恢复和/或回填，而其中有几个组内的数据比其它的更重要（例如，那些 PG 持有正在运行着的机器的映像数据，而其它 PG 上的数据是不太活跃的机器、或不相干的数据用着）。在那种情形下，你可能想更改那些归置组的恢复优先级，这样它们的性能和/或数据可用性就可以早些恢复。要达成这个目标（把特定归置组标记为在回填或恢复期间有高优先级），用此命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="n">force</span><span class="o">-</span><span class="n">recovery</span> <span class="p">{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span><span class="p">}</span> <span class="p">[{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span> <span class="c1">#2}] [{pg-id #3} ...]</span>
<span class="n">ceph</span> <span class="n">pg</span> <span class="n">force</span><span class="o">-</span><span class="n">backfill</span> <span class="p">{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span><span class="p">}</span> <span class="p">[{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span> <span class="c1">#2}] [{pg-id #3} ...]</span>
</pre></div>
</div>
<p>这样， Ceph 就会优先处理指定归置组的恢复或回填工作，其它则靠后。这个设置不会中断当前正在进行的回填或恢复，而是让指定的
PG 尽快被处理。如果你改主意了，或者弄错了归置组，可以用如下命令取消：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="n">cancel</span><span class="o">-</span><span class="n">force</span><span class="o">-</span><span class="n">recovery</span> <span class="p">{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span><span class="p">}</span> <span class="p">[{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span> <span class="c1">#2}] [{pg-id #3} ...]</span>
<span class="n">ceph</span> <span class="n">pg</span> <span class="n">cancel</span><span class="o">-</span><span class="n">force</span><span class="o">-</span><span class="n">backfill</span> <span class="p">{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span><span class="p">}</span> <span class="p">[{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span> <span class="c1">#2}] [{pg-id #3} ...]</span>
</pre></div>
</div>
<p>这会删除那些 PG 的 force 标记，然后系统会按默认顺序处理它们。再次提醒，这操作不会影响当前正处理着的归置组，只影响还在队列里的那些。</p>
<p>归置组恢复或回填完后， force 标记会被自动清除。</p>
</div>
<div class="section" id="id18">
<h2>恢复丢失的<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h2>
<p>如果集群丢了一或多个对象，而且必须放弃搜索这些数据，你就要把未找到的对象标记为丢失（ <code class="docutils literal notranslate"><span class="pre">lost</span></code> ）。</p>
<p>如果所有可能的位置都查询过了，而仍找不到这些对象，你也许得放弃它们了。这可能是罕见的失败组合导致的，集群在写入完成前，未能得知写入是否已执行。</p>
<p>当前只支持 revert 选项，它使得回滚到对象的前一个版本（如果它是新对象）或完全忽略它。要把 unfound 对象标记为 lost ，执行命令：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ceph</span> <span class="n">pg</span> <span class="p">{</span><span class="n">pg</span><span class="o">-</span><span class="nb">id</span><span class="p">}</span> <span class="n">mark_unfound_lost</span> <span class="n">revert</span><span class="o">|</span><span class="n">delete</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>此功能要谨慎使用，它可能迷惑那些期望对象存在的应用程序。</p>
</div>
<div class="toctree-wrapper compound">
</div>
</div>
</div>



          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../">
              <img class="logo" src="../../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephadm/">Cephadm</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../">Ceph 存储集群</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../configuration/">配置</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment/">部署</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../">运维</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../operating/">操纵集群</a></li>
<li class="toctree-l3"><a class="reference internal" href="../health-checks/">健康检查</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitoring/">监控集群</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitoring-osd-pg/">监控 OSD 和归置组</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user-management/">用户管理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pg-repair/">修复 PG 不一致状态</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data-placement/">数据归置概览</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pools/">存储池</a></li>
<li class="toctree-l3"><a class="reference internal" href="../erasure-code/">纠删码</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cache-tiering/">分级缓存</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">归置组</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#pg-autoscaler">自伸缩归置组</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#pg">查看 PG 伸缩建议</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id3">自动化的伸缩</a></li>
<li class="toctree-l5"><a class="reference internal" href="#specifying-pool-target-size">配置期望的存储池尺寸</a></li>
<li class="toctree-l5"><a class="reference internal" href="#id5">设置存储池的 PG 数量界限</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#pg-num">预定义 pg_num</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">归置组是如何使用的？</a></li>
<li class="toctree-l4"><a class="reference internal" href="#placement-groups-tradeoffs">Placement Groups Tradeoffs</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#data-durability">Data durability</a></li>
<li class="toctree-l5"><a class="reference internal" href="#object-distribution-within-a-pool">Object distribution within a pool</a></li>
<li class="toctree-l5"><a class="reference internal" href="#resource-usage">内存、处理器和网络使用情况</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#choosing-number-of-placement-groups">确定归置组数量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setting-the-number-of-placement-groups">设置归置组数量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">获取归置组数量</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">获取归置组统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id13">获取卡住的归置组统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id14">获取一归置组运行图</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id15">获取一 PG 的统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id16">洗刷归置组</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id17">改变归置组的回填/恢复优先级</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id18">恢复丢失的</a><ul>
<li class="toctree-l5"><a class="reference internal" href="../pg-states/">归置组状态</a></li>
<li class="toctree-l5"><a class="reference internal" href="../pg-concepts/">归置组术语解释</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../balancer/">均衡器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../upmap/">使用 pg-upmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="../crush-map/">CRUSH 图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../crush-map-edits/">手动编辑一个 CRUSH 图</a></li>
<li class="toctree-l3"><a class="reference internal" href="../add-or-rm-osds/">增加/删除 OSD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../add-or-rm-mons/">增加/删除监视器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../devices/">设备管理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../bluestore-migration/">迁移到 BlueStore</a></li>
<li class="toctree-l3"><a class="reference internal" href="../control/">命令参考</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/community/">Ceph 社区</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-mon/">监视器故障排除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-osd/">OSD 故障排除</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-pg/">归置组排障</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/log-and-debug/">日志记录和调试</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/cpu-profiling/">CPU 剖析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/memory-profiling/">内存剖析</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../man/">手册页</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../troubleshooting/">故障排除</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/">APIs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/developer_guide/">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../translation_cn/">中文版翻译资源</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search/" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../pg-states/" title="归置组状态"
             >next</a> |</li>
        <li class="right" >
          <a href="../cache-tiering/" title="分级缓存"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../../">Ceph Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../" >Ceph 存储集群</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../" >集群运维</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).
    </div>
  </body>
</html>