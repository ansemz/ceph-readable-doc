
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Cache Tiering &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../../" />
    <link rel="up" title="Cluster Operations" href="../" />
    <link rel="next" title="Placement Groups" href="../placement-groups/" />
    <link rel="prev" title="SHEC erasure code plugin" href="../erasure-code-shec/" />
    <script type="text/javascript" src="http://ayni.ceph.com/public/js/ceph.js"></script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../placement-groups/" title="Placement Groups"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../erasure-code-shec/" title="SHEC erasure code plugin"
             accesskey="P">previous</a> |</li>
        <li><a href="../../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../../" >Ceph Storage Cluster</a> &raquo;</li>
          <li><a href="../" accesskey="U">Cluster Operations</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="cache-tiering">
<h1>Cache Tiering<a class="headerlink" href="#cache-tiering" title="Permalink to this headline">¶</a></h1>
<p>A cache tier provides Ceph Clients with better I/O performance for a subset of
the data stored in a backing storage tier. Cache tiering involves creating a
pool of relatively fast/expensive storage devices (e.g., solid state drives)
configured to act as a cache tier, and a backing pool of either erasure-coded
or relatively slower/cheaper devices configured to act as an economical storage
tier. The Ceph objecter handles where to place the objects and the tiering
agent determines when to flush objects from the cache to the backing storage
tier. So the cache tier and the backing storage tier are completely transparent
to Ceph clients.</p>
<p class="ditaa">
<img src="../../../_images/ditaa-2982c5ed3031cac4f9e40545139e51fdb0b33897.png"/>
</p>
<p>The cache tiering agent handles the migration of data between the cache tier
and the backing storage tier automatically. However, admins have the ability to
configure how this migration takes place. There are two main scenarios:</p>
<ul class="simple">
<li><strong>Writeback Mode:</strong> When admins configure tiers with <tt class="docutils literal"><span class="pre">writeback</span></tt> mode, Ceph
clients write data to the cache tier and receive an ACK from the cache tier.
In time, the data written to the cache tier migrates to the storage tier
and gets flushed from the cache tier. Conceptually, the cache tier is
overlaid &#8220;in front&#8221; of the backing storage tier. When a Ceph client needs
data that resides in the storage tier, the cache tiering agent migrates the
data to the cache tier on read, then it is sent to the Ceph client.
Thereafter, the Ceph client can perform I/O using the cache tier, until the
data becomes inactive. This is ideal for mutable data (e.g., photo/video
editing, transactional data, etc.).</li>
<li><strong>Read-proxy Mode:</strong> This mode will use any objects that already
exist in the cache tier, but if an object is not present in the
cache the request will be proxied to the base tier.  This is useful
for transitioning from <tt class="docutils literal"><span class="pre">writeback</span></tt> mode to a disabled cache as it
allows the workload to function properly while the cache is drained,
without adding any new objects to the cache.</li>
</ul>
<div class="section" id="a-word-of-caution">
<h2>A word of caution<a class="headerlink" href="#a-word-of-caution" title="Permalink to this headline">¶</a></h2>
<p>Cache tiering will <em>degrade</em> performance for most workloads.  Users should use
extreme caution before using this feature.</p>
<ul class="simple">
<li><em>Workload dependent</em>: Whether a cache will improve performance is
highly dependent on the workload.  Because there is a cost
associated with moving objects into or out of the cache, it can only
be effective when there is a <em>large skew</em> in the access pattern in
the data set, such that most of the requests touch a small number of
objects.  The cache pool should be large enough to capture the
working set for your workload to avoid thrashing.</li>
<li><em>Difficult to benchmark</em>: Most benchmarks that users run to measure
performance will show terrible performance with cache tiering, in
part because very few of them skew requests toward a small set of
objects, it can take a long time for the cache to &#8220;warm up,&#8221; and
because the warm-up cost can be high.</li>
<li><em>Usually slower</em>: For workloads that are not cache tiering-friendly,
performance is often slower than a normal RADOS pool without cache
tiering enabled.</li>
<li><em>librados object enumeration</em>: The librados-level object enumeration
API is not meant to be coherent in the presence of the case.  If
your applicatoin is using librados directly and relies on object
enumeration, cache tiering will probably not work as expected.
(This is not a problem for RGW, RBD, or CephFS.)</li>
<li><em>Complexity</em>: Enabling cache tiering means that a lot of additional
machinery and complexity within the RADOS cluster is being used.
This increases the probability that you will encounter a bug in the system
that other users have not yet encountered and will put your deployment at a
higher level of risk.</li>
</ul>
<div class="section" id="known-good-workloads">
<h3>Known Good Workloads<a class="headerlink" href="#known-good-workloads" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><em>RGW time-skewed</em>: If the RGW workload is such that almost all read
operations are directed at recently written objects, a simple cache
tiering configuration that destages recently written objects from
the cache to the base tier after a configurable period can work
well.</li>
</ul>
</div>
<div class="section" id="known-bad-workloads">
<h3>Known Bad Workloads<a class="headerlink" href="#known-bad-workloads" title="Permalink to this headline">¶</a></h3>
<p>The following configurations are <em>known to work poorly</em> with cache
tiering.</p>
<ul class="simple">
<li><em>RBD with replicated cache and erasure-coded base</em>: This is a common
request, but usually does not perform well.  Even reasonably skewed
workloads still send some small writes to cold objects, and because
small writes are not yet supported by the erasure-coded pool, entire
(usually 4 MB) objects must be migrated into the cache in order to
satisfy a small (often 4 KB) write.  Only a handful of users have
successfully deployed this configuration, and it only works for them
because their data is extremely cold (backups) and they are not in
any way sensitive to performance.</li>
<li><em>RBD with replicated cache and base</em>: RBD with a replicated base
tier does better than when the base is erasure coded, but it is
still highly dependent on the amount of skew in the workload, and
very difficult to validate.  The user will need to have a good
understanding of their workload and will need to tune the cache
tiering parameters carefully.</li>
</ul>
</div>
</div>
<div class="section" id="setting-up-pools">
<h2>Setting Up Pools<a class="headerlink" href="#setting-up-pools" title="Permalink to this headline">¶</a></h2>
<p>To set up cache tiering, you must have two pools. One will act as the
backing storage and the other will act as the cache.</p>
<div class="section" id="setting-up-a-backing-storage-pool">
<h3>Setting Up a Backing Storage Pool<a class="headerlink" href="#setting-up-a-backing-storage-pool" title="Permalink to this headline">¶</a></h3>
<p>Setting up a backing storage pool typically involves one of two scenarios:</p>
<ul class="simple">
<li><strong>Standard Storage</strong>: In this scenario, the pool stores multiple copies
of an object in the Ceph Storage Cluster.</li>
<li><strong>Erasure Coding:</strong> In this scenario, the pool uses erasure coding to
store data much more efficiently with a small performance tradeoff.</li>
</ul>
<p>In the standard storage scenario, you can setup a CRUSH ruleset to establish
the failure domain (e.g., osd, host, chassis, rack, row, etc.). Ceph OSD
Daemons perform optimally when all storage drives in the ruleset are of the
same size, speed (both RPMs and throughput) and type. See <a class="reference external" href="../crush-map">CRUSH Maps</a>
for details on creating a ruleset. Once you have created a ruleset, create
a backing storage pool.</p>
<p>In the erasure coding scenario, the pool creation arguments will generate the
appropriate ruleset automatically. See <a class="reference external" href="../pools#create-a-pool">Create a Pool</a> for details.</p>
<p>In subsequent examples, we will refer to the backing storage pool
as <tt class="docutils literal"><span class="pre">cold-storage</span></tt>.</p>
</div>
<div class="section" id="setting-up-a-cache-pool">
<h3>Setting Up a Cache Pool<a class="headerlink" href="#setting-up-a-cache-pool" title="Permalink to this headline">¶</a></h3>
<p>Setting up a cache pool follows the same procedure as the standard storage
scenario, but with this difference: the drives for the cache tier are typically
high performance drives that reside in their own servers and have their own
ruleset.  When setting up a ruleset, it should take account of the hosts that
have the high performance drives while omitting the hosts that don&#8217;t. See
<a class="reference external" href="../crush-map/#placing-different-pools-on-different-osds">Placing Different Pools on Different OSDs</a> for details.</p>
<p>In subsequent examples, we will refer to the cache pool as <tt class="docutils literal"><span class="pre">hot-storage</span></tt> and
the backing pool as <tt class="docutils literal"><span class="pre">cold-storage</span></tt>.</p>
<p>For cache tier configuration and default values, see
<a class="reference external" href="../pools#set-pool-values">Pools - Set Pool Values</a>.</p>
</div>
</div>
<div class="section" id="creating-a-cache-tier">
<h2>Creating a Cache Tier<a class="headerlink" href="#creating-a-cache-tier" title="Permalink to this headline">¶</a></h2>
<p>Setting up a cache tier involves associating a backing storage pool with
a cache pool</p>
<div class="highlight-python"><pre>ceph osd tier add {storagepool} {cachepool}</pre>
</div>
<p>For example</p>
<div class="highlight-python"><pre>ceph osd tier add cold-storage hot-storage</pre>
</div>
<p>To set the cache mode, execute the following:</p>
<div class="highlight-python"><pre>ceph osd tier cache-mode {cachepool} {cache-mode}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph osd tier cache-mode hot-storage writeback</pre>
</div>
<p>The cache tiers overlay the backing storage tier, so they require one
additional step: you must direct all client traffic from the storage pool to
the cache pool. To direct client traffic directly to the cache pool, execute
the following:</p>
<div class="highlight-python"><pre>ceph osd tier set-overlay {storagepool} {cachepool}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph osd tier set-overlay cold-storage hot-storage</pre>
</div>
</div>
<div class="section" id="configuring-a-cache-tier">
<h2>Configuring a Cache Tier<a class="headerlink" href="#configuring-a-cache-tier" title="Permalink to this headline">¶</a></h2>
<p>Cache tiers have several configuration options. You may set
cache tier configuration options with the following usage:</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} {key} {value}</pre>
</div>
<p>See <a class="reference external" href="../pools#set-pool-values">Pools - Set Pool Values</a> for details.</p>
<div class="section" id="target-size-and-type">
<h3>Target Size and Type<a class="headerlink" href="#target-size-and-type" title="Permalink to this headline">¶</a></h3>
<p>Ceph&#8217;s production cache tiers use a <a class="reference external" href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</a> for the <tt class="docutils literal"><span class="pre">hit_set_type</span></tt>:</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} hit_set_type bloom</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph osd pool set hot-storage hit_set_type bloom</pre>
</div>
<p>The <tt class="docutils literal"><span class="pre">hit_set_count</span></tt> and <tt class="docutils literal"><span class="pre">hit_set_period</span></tt> define how much time each HitSet
should cover, and how many such HitSets to store.</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} hit_set_count 12
ceph osd pool set {cachepool} hit_set_period 14400
ceph osd pool set {cachepool} target_max_bytes 1000000000000</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">A larger <tt class="docutils literal"><span class="pre">hit_set_count</span></tt> results in more RAM consumed by
the <tt class="docutils literal"><span class="pre">ceph-osd</span></tt> process.</p>
</div>
<p>Binning accesses over time allows Ceph to determine whether a Ceph client
accessed an object at least once, or more than once over a time period
(&#8220;age&#8221; vs &#8220;temperature&#8221;).</p>
<p>The <tt class="docutils literal"><span class="pre">min_read_recency_for_promote</span></tt> defines how many HitSets to check for the
existence of an object when handling a read operation. The checking result is
used to decide whether to promote the object asynchronously. Its value should be
between 0 and <tt class="docutils literal"><span class="pre">hit_set_count</span></tt>. If it&#8217;s set to 0, the object is always promoted.
If it&#8217;s set to 1, the current HitSet is checked. And if this object is in the
current HitSet, it&#8217;s promoted. Otherwise not. For the other values, the exact
number of archive HitSets are checked. The object is promoted if the object is
found in any of the most recent <tt class="docutils literal"><span class="pre">min_read_recency_for_promote</span></tt> HitSets.</p>
<p>A similar parameter can be set for the write operation, which is
<tt class="docutils literal"><span class="pre">min_write_recency_for_promote</span></tt>.</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} min_read_recency_for_promote 2
ceph osd pool set {cachepool} min_write_recency_for_promote 2</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The longer the period and the higher the
<tt class="docutils literal"><span class="pre">min_read_recency_for_promote</span></tt> and
<tt class="docutils literal"><span class="pre">min_write_recency_for_promote``values,</span> <span class="pre">the</span> <span class="pre">more</span> <span class="pre">RAM</span> <span class="pre">the</span> <span class="pre">``ceph-osd</span></tt>
daemon consumes. In particular, when the agent is active to flush
or evict cache objects, all <tt class="docutils literal"><span class="pre">hit_set_count</span></tt> HitSets are loaded
into RAM.</p>
</div>
</div>
<div class="section" id="cache-sizing">
<h3>Cache Sizing<a class="headerlink" href="#cache-sizing" title="Permalink to this headline">¶</a></h3>
<p>The cache tiering agent performs two main functions:</p>
<ul class="simple">
<li><strong>Flushing:</strong> The agent identifies modified (or dirty) objects and forwards
them to the storage pool for long-term storage.</li>
<li><strong>Evicting:</strong> The agent identifies objects that haven&#8217;t been modified
(or clean) and evicts the least recently used among them from the cache.</li>
</ul>
<div class="section" id="absolute-sizing">
<h4>Absolute Sizing<a class="headerlink" href="#absolute-sizing" title="Permalink to this headline">¶</a></h4>
<p>The cache tiering agent can flush or evict objects based upon the total number
of bytes or the total number of objects. To specify a maximum number of bytes,
execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} target_max_bytes {#bytes}</pre>
</div>
<p>For example, to flush or evict at 1 TB, execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool set hot-storage target_max_bytes 1099511627776</pre>
</div>
<p>To specify the maximum number of objects, execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} target_max_objects {#objects}</pre>
</div>
<p>For example, to flush or evict at 1M objects, execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool set hot-storage target_max_objects 1000000</pre>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Ceph is not able to determine the size of a cache pool automatically, so
the configuration on the absolute size is required here, otherwise the
flush/evict will not work. If you specify both limits, the cache tiering
agent will begin flushing or evicting when either threshold is triggered.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">All client requests will be blocked only when  <tt class="docutils literal"><span class="pre">target_max_bytes</span></tt> or
<tt class="docutils literal"><span class="pre">target_max_objects</span></tt> reached</p>
</div>
</div>
<div class="section" id="relative-sizing">
<h4>Relative Sizing<a class="headerlink" href="#relative-sizing" title="Permalink to this headline">¶</a></h4>
<p>The cache tiering agent can flush or evict objects relative to the size of the
cache pool(specified by <tt class="docutils literal"><span class="pre">target_max_bytes</span></tt> / <tt class="docutils literal"><span class="pre">target_max_objects</span></tt> in
<a class="reference external" href="#absolute-sizing">Absolute sizing</a>).  When the cache pool consists of a certain percentage of
modified (or dirty) objects, the cache tiering agent will flush them to the
storage pool. To set the <tt class="docutils literal"><span class="pre">cache_target_dirty_ratio</span></tt>, execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} cache_target_dirty_ratio {0.0..1.0}</pre>
</div>
<p>For example, setting the value to <tt class="docutils literal"><span class="pre">0.4</span></tt> will begin flushing modified
(dirty) objects when they reach 40% of the cache pool&#8217;s capacity:</p>
<div class="highlight-python"><pre>ceph osd pool set hot-storage cache_target_dirty_ratio 0.4</pre>
</div>
<p>When the dirty objects reaches a certain percentage of its capacity, flush dirty
objects with a higher speed. To set the <tt class="docutils literal"><span class="pre">cache_target_dirty_high_ratio</span></tt>:</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} cache_target_dirty_high_ratio {0.0..1.0}</pre>
</div>
<p>For example, setting the value to <tt class="docutils literal"><span class="pre">0.6</span></tt> will begin aggressively flush dirty objects
when they reach 60% of the cache pool&#8217;s capacity. obviously, we&#8217;d better set the value
between dirty_ratio and full_ratio:</p>
<div class="highlight-python"><pre>ceph osd pool set hot-storage cache_target_dirty_high_ratio 0.6</pre>
</div>
<p>When the cache pool reaches a certain percentage of its capacity, the cache
tiering agent will evict objects to maintain free capacity. To set the
<tt class="docutils literal"><span class="pre">cache_target_full_ratio</span></tt>, execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} cache_target_full_ratio {0.0..1.0}</pre>
</div>
<p>For example, setting the value to <tt class="docutils literal"><span class="pre">0.8</span></tt> will begin flushing unmodified
(clean) objects when they reach 80% of the cache pool&#8217;s capacity:</p>
<div class="highlight-python"><pre>ceph osd pool set hot-storage cache_target_full_ratio 0.8</pre>
</div>
</div>
</div>
<div class="section" id="cache-age">
<h3>Cache Age<a class="headerlink" href="#cache-age" title="Permalink to this headline">¶</a></h3>
<p>You can specify the minimum age of an object before the cache tiering agent
flushes a recently modified (or dirty) object to the backing storage pool:</p>
<div class="highlight-python"><pre>ceph osd pool set {cachepool} cache_min_flush_age {#seconds}</pre>
</div>
<p>For example, to flush modified (or dirty) objects after 10 minutes, execute
the following:</p>
<div class="highlight-python"><pre>ceph osd pool set hot-storage cache_min_flush_age 600</pre>
</div>
<p>You can specify the minimum age of an object before it will be evicted from
the cache tier:</p>
<div class="highlight-python"><pre>ceph osd pool {cache-tier} cache_min_evict_age {#seconds}</pre>
</div>
<p>For example, to evict objects after 30 minutes, execute the following:</p>
<div class="highlight-python"><pre>ceph osd pool set hot-storage cache_min_evict_age 1800</pre>
</div>
</div>
</div>
<div class="section" id="removing-a-cache-tier">
<h2>Removing a Cache Tier<a class="headerlink" href="#removing-a-cache-tier" title="Permalink to this headline">¶</a></h2>
<p>Removing a cache tier differs depending on whether it is a writeback
cache or a read-only cache.</p>
<div class="section" id="removing-a-read-only-cache">
<h3>Removing a Read-Only Cache<a class="headerlink" href="#removing-a-read-only-cache" title="Permalink to this headline">¶</a></h3>
<p>Since a read-only cache does not have modified data, you can disable
and remove it without losing any recent changes to objects in the cache.</p>
<ol class="arabic">
<li><p class="first">Change the cache-mode to <tt class="docutils literal"><span class="pre">none</span></tt> to disable it.</p>
<div class="highlight-python"><pre>ceph osd tier cache-mode {cachepool} none</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph osd tier cache-mode hot-storage none</pre>
</div>
</li>
<li><p class="first">Remove the cache pool from the backing pool.</p>
<div class="highlight-python"><pre>ceph osd tier remove {storagepool} {cachepool}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph osd tier remove cold-storage hot-storage</pre>
</div>
</li>
</ol>
</div>
<div class="section" id="removing-a-writeback-cache">
<h3>Removing a Writeback Cache<a class="headerlink" href="#removing-a-writeback-cache" title="Permalink to this headline">¶</a></h3>
<p>Since a writeback cache may have modified data, you must take steps to ensure
that you do not lose any recent changes to objects in the cache before you
disable and remove it.</p>
<ol class="arabic">
<li><p class="first">Change the cache mode to <tt class="docutils literal"><span class="pre">forward</span></tt> so that new and modified objects will
flush to the backing storage pool.</p>
<div class="highlight-python"><pre>ceph osd tier cache-mode {cachepool} forward</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph osd tier cache-mode hot-storage forward</pre>
</div>
</li>
<li><p class="first">Ensure that the cache pool has been flushed. This may take a few minutes:</p>
<div class="highlight-python"><pre>rados -p {cachepool} ls</pre>
</div>
<p>If the cache pool still has objects, you can flush them manually.
For example:</p>
<div class="highlight-python"><pre>rados -p {cachepool} cache-flush-evict-all</pre>
</div>
</li>
<li><p class="first">Remove the overlay so that clients will not direct traffic to the cache.</p>
<div class="highlight-python"><pre>ceph osd tier remove-overlay {storagetier}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph osd tier remove-overlay cold-storage</pre>
</div>
</li>
<li><p class="first">Finally, remove the cache tier pool from the backing storage pool.</p>
<div class="highlight-python"><pre>ceph osd tier remove {storagepool} {cachepool}</pre>
</div>
<p>For example:</p>
<div class="highlight-python"><pre>ceph osd tier remove cold-storage hot-storage</pre>
</div>
</li>
</ol>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../">
              <img class="logo" src="../../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../start/intro/">Intro to Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../start/">Installation (Quick)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">Installation (Manual)</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../">Ceph Storage Cluster</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../configuration/">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deployment/">Deployment</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../">Operations</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../operating/">Operating a Cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitoring/">Monitoring a Cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitoring-osd-pg/">Monitoring OSDs and PGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../user-management/">User Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../data-placement/">Data Placement Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pools/">Pools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../erasure-code/">Erasure code</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="">Cache Tiering</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#a-word-of-caution">A word of caution</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#known-good-workloads">Known Good Workloads</a></li>
<li class="toctree-l5"><a class="reference internal" href="#known-bad-workloads">Known Bad Workloads</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#setting-up-pools">Setting Up Pools</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#setting-up-a-backing-storage-pool">Setting Up a Backing Storage Pool</a></li>
<li class="toctree-l5"><a class="reference internal" href="#setting-up-a-cache-pool">Setting Up a Cache Pool</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#creating-a-cache-tier">Creating a Cache Tier</a></li>
<li class="toctree-l4"><a class="reference internal" href="#configuring-a-cache-tier">Configuring a Cache Tier</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#target-size-and-type">Target Size and Type</a></li>
<li class="toctree-l5"><a class="reference internal" href="#cache-sizing">Cache Sizing</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#absolute-sizing">Absolute Sizing</a></li>
<li class="toctree-l6"><a class="reference internal" href="#relative-sizing">Relative Sizing</a></li>
</ul>
</li>
<li class="toctree-l5"><a class="reference internal" href="#cache-age">Cache Age</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#removing-a-cache-tier">Removing a Cache Tier</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#removing-a-read-only-cache">Removing a Read-Only Cache</a></li>
<li class="toctree-l5"><a class="reference internal" href="#removing-a-writeback-cache">Removing a Writeback Cache</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../placement-groups/">Placement Groups</a></li>
<li class="toctree-l3"><a class="reference internal" href="../crush-map/">CRUSH Maps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../add-or-rm-osds/">Adding/Removing OSDs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../add-or-rm-mons/">Adding/Removing Monitors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../control/">Command Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/community/">The Ceph Community</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-mon/">Troubleshooting Monitors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-osd/">Troubleshooting OSDs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/troubleshooting-pg/">Troubleshooting PGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/log-and-debug/">Logging and Debugging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/cpu-profiling/">CPU Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../troubleshooting/memory-profiling/">Memory Profiling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../man/">Man Pages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../troubleshooting/">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../api/">APIs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph Filesystem</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/rbd/">Ceph Block Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph Object Gateway</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/">Ceph Manager Daemon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/">Ceph Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Glossary</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../placement-groups/" title="Placement Groups"
             >next</a> |</li>
        <li class="right" >
          <a href="../erasure-code-shec/" title="SHEC erasure code plugin"
             >previous</a> |</li>
        <li><a href="../../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../../" >Ceph Storage Cluster</a> &raquo;</li>
          <li><a href="../" >Cluster Operations</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2016, Red Hat, Inc, and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>