===========
 Ceph 术语
===========

.. glossary::

    Application
    应用程序
        更恰当的叫法是 :term:`客户端` ，它是相对于 Ceph 之外的任意程序，
        使用 Ceph 集群来存储和复制数据。

    :ref:`BlueStore<rados_config_storage_devices_bluestore>`
        OSD BlueStore 是 OSD 守护进程使用的一个存储后端，是特意为 Ceph 设计的 。
        BlueStore 最早在 Kraken 版本引进，从 Luminous 版起， BlueStore 取代
        FileStore 被提升为默认 OSD 后端，完全取代了 FileStore 。到 Reef 版时，
        就不能再使用 FileStore 作为存储后端了。

        BlueStore 直接在原始块设备或分区上存储对象，不会与挂载的文件系统\
        产生交互操作。 BlueStore 利用 RocksDB 的键/值数据库来把对象名映射到\
        磁盘上的块位置。

    Bucket
    桶
        在 :term:`RGW` 的语境中，“桶”是一组对象。
        以文件系统类比的话，对象对应文件，桶对应目录。
        要想对一个区域到另一个区域的数据移动进行细粒度控制，可以在桶上配置
        :ref:`多站点同步策略 <radosgw-multisite-sync-policy>` 。

        桶的概念是从 AWS S3 借鉴来的。请参考 `AWS S3 网页上有关创建桶
        <https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-buckets-s3.html>`_
        和 `AWS S3 “桶概览”页面 <https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html>`_ 。

        在 RGW 和 AWS 上称为“桶”的东西，到了 OpenStack Swift 上叫作术语“容器（ containers ）”。
        见 `OpenStack Storage API 概览页面 <https://docs.openstack.org/swift/latest/api/object_api_v1_overview.html>`_ 。

    Ceph
        Ceph 是一套支持分布式元数据管理和 POSIX 语义的
        分布式网络存储和文件系统。

    `ceph-ansible <https://docs.ceph.com/projects/ceph-ansible/en/latest/index.html>`_
        一个 GitHub 仓库，支持从 Jewel 版到 Quincy 版的
        Ceph 集群安装。

    Ceph Block Device
    Ceph 块设备
        也叫作 “RADOS Block Device” 和 :term:`RBD` 。
        是个软件工具，在 Ceph 中组织基于块数据的存储。
        Ceph 块设备把基于块的应用数据分割成“块”，
        RADOS 将这些块存储为对象。
        Ceph 块设备功能组织着这些对象的在集群中的存储。

    Ceph Block Storage
    Ceph 块存储
        Ceph 支持的三种存储之一
        （另外两种是对象存储和文件存储）。
        Ceph 块存储是块存储“产品”，
        它在与三类软件套件对接使用时指的是块存储相关的服务及其功能，
        可以对接 (1) ``librbd`` （一个 Python 模块，它能使我们\
        像访问文件一样访问 :term:`RBD` 映像）、
        (2) 虚拟化管理程序如QEMU 或 Xen 、和
        (3) 虚拟化管理程序抽象层，如 ``libvirt`` 。

    :ref:`Ceph Client <architecture_ceph_clients>`
    Ceph 客户端
        任何能够访问 Ceph 存储集群的 Ceph 组件，
        其中包括 Ceph 对象网关、 Ceph 块设备、
        Ceph 文件系统、及其对应的库，
        也包括内核模块和 FUSE （用户空间文件系统）。

    Ceph Client Libraries
    Ceph 客户端库
        能够和相应 Ceph 集群的组件相交互的一组库。

    Ceph Cluster Map
    Ceph 集群运行图
        见 :term:`Cluster Map`

    Ceph Dashboard
    Ceph 仪表盘
        :ref:`Ceph 仪表盘<mgr-dashboard>` 是一个内建的、
        基于网页的 Ceph 管理和监控应用程序，
        可用于调查和管理集群内的各种资源。
        仪表盘是以一个 Ceph 管理器模块（ :ref:`ceph-manager-daemon` ）实现的。

    Ceph Filesystem
    Ceph 文件系统
        见 :term:`CephFS`

    :ref:`CephFS<ceph-file-system>`
        Ceph 文件系统（ **Ceph F**\ile **S**\ystem ）或者 CephFS ，
        是一个与 POSIX 兼容的文件系统，
        底层基于 Ceph 的分布式对象存储 RADOS 。
        详情见 :ref:`CephFS 体系结构 <arch-cephfs>` 。

    :ref:`ceph-fuse <man-ceph-fuse>`
        :ref:`ceph-fuse <man-ceph-fuse>` 是 CephFS 的一个 FUSE
        (用户空间文件系统， "**F**\ilesystem in **USE**\rspace") 客户端。
        ceph-fuse 可以把 Ceph FS 挂载到指定的挂载点。

	Ceph Interim Release
    Ceph 临时版
        见 :term:`Releases`.

    Ceph Kernel Modules
    Ceph 内核模块
        一组能够成功和 Ceph 系统交互的内核模块
        （比如 ``ceph.ko`` 、 ``rbd.ko`` ）。

    :ref:`Ceph Manager<ceph-manager-daemon>`
    Ceph 管理器
        Ceph 管理器守护进程（ ceph-mgr ）是与监控守护进程\
        一起运行的守护进程，可提供监控功能并与外部监控和管理系统连接。
        从 Luminous 版（12.x）起，所有 Ceph 集群要想正常运作，
        都必须有正常运行的 ceph-mgr 守护进程。

    Ceph Manager Dashboard
    Ceph 管理器仪表盘
        见 :term:`Ceph Dashboard` 。

    Ceph Metadata Server
    Ceph 元数据服务器
        见 :term:`MDS` 。

    Ceph Monitor
    Ceph 监视器
        维护集群状态图的守护进程。
        集群状态包括监控器运行图、管理器运行图、OSD 运行图和 CRUSH 运行图。
        一套 Ceph 集群必须至少包含三个正常运行的监控器，
        才能同时实现冗余和高可用性。
        Ceph 监视器以及它所在的节点通常也叫做 mon 。
        见 :ref:`监视器配置参考 <monitor-config-reference>` 。

    Ceph Node
    Ceph 节点
        Ceph 节点是 Ceph 集群的一个单元，它与 Ceph 集群中的其他节点通信，
        以便复制和重新分布数据。所有节点加在一起称为 :term:`Ceph 存储集群`\ 。
        Ceph 节点包括 :term:`OSD` 、 :term:`Ceph 监视器`\ 、
        :term:`Ceph 管理器`\ 和 :term:`MDS`\ 。在 Ceph 文档中，
        术语 “节点（ node）” 通常等同于 “主机（ host ）”。
        如果您有一个正常运行的 Ceph 群集，可以用
        ``ceph node ls all`` 命令列出其内的所有节点。

    :ref:`Ceph Object Gateway<object-gateway>`
    Ceph 对象网关
        建立在 librados 基础上的一个对象存储接口。
        Ceph 对象网关符合 REST 规范，它位于应用程序和 Ceph 存储集群之间。

    Ceph Object Storage
    Ceph 对象存储
        见 :term:`Ceph Object Store` 。

    Ceph Object Store
    Ceph 对象存储库
        Ceph 对象存储库由 :term:`Ceph 存储集群` 和
        :term:`Ceph 对象网关` （RGW）组成。

    :ref:`Ceph OSD<rados_configuration_storage-devices_ceph_osd>`
        Ceph **O**\bject **S**\torage **D**\aemon （ Ceph 对象存储守护进程）。
        是指 Ceph OSD 软件，负责与逻辑磁盘（ :term:`OSD` ）交互。2013 年前后，
        “研究和行业”（ Sage 自己说的）曾试图坚持让 “OSD” 这个术语仅指
        “对象存储设备”，但 Ceph 社区此前一直用该术语指 “对象存储守护进程”，
        而且不会有人比 Sage Weil 本人更权威了，
        他在 2022 年 11 月确认，“守护进程这个说法对于 Ceph 的构建方式更精确”
        （ Zac Dover 和 Sage Weil 之间的私人通信，2022 年 11 月 7 日）。

    Ceph OSD Daemon
    Ceph OSD 守护进程
        见 :term:`Ceph OSD`.

    Ceph OSD Daemons
        见 :term:`Ceph OSD`.

    Ceph Platform
    Ceph 平台
        所有与 Ceph 相关的软件，包括所有位于 `https://github.com/ceph`_ 的源码。

    Ceph Point Release
    Ceph 修正版
    Ceph 小版本
        见 :term:`Releases`.

    Ceph Project
    Ceph 项目
        关于 Ceph 的团队、软件、任务和基础架构的统称。

    Ceph Release
    Ceph 版本
        见 :term:`Releases` 。

    Ceph Release Candidate
    Ceph 发布候选版
        见 :term:`Releases` 。

    Ceph Stable Release
    Ceph 稳定版
        见 :term:`Releases` 。

    Ceph Stack
    Ceph 软件栈
        Ceph 之中两个或更多组件的组合。

    :ref:`Ceph Storage Cluster<arch-ceph-storage-cluster>`
    :ref:`Ceph 存储集群<arch-ceph-storage-cluster>`
        Ceph 监视器、Ceph 管理器、Ceph 元数据服务器和 OSD 的集合，它们共同存储和复制数据，供应用程序、Ceph 用户和 Ceph 客户端使用。Ceph 存储集群从 Ceph 客户端接收数据。

    CephX
        Ceph 的认证协议， Cephx 的运行机制类似 Kerberos ，但它没有单故障点。

    Client
    客户端
        A client is any program external to Ceph that uses a Ceph
        Cluster to store and replicate data.

    Cloud Platforms
    Cloud Stacks
    云平台
    云软件栈
        第三方云服务平台，比如 OpenStack, CloudStack, OpenNebula, Proxmox VE 等等。

    Cluster Map
    集群运行图
        一系列的运行图，包括监视器运行图、 OSD 运行图、 PG 图、 MDS 运行图\
        和 CRUSH 图。详情见 `集群运行图`_ 。

    Crimson
        A next-generation OSD architecture whose main aim is the
        reduction of latency costs incurred due to cross-core
        communications. A re-design of the OSD reduces lock
        contention by reducing communication between shards in the data
        path. Crimson improves upon the performance of classic Ceph
        OSDs by eliminating reliance on thread pools. See `Crimson:
        Next-generation Ceph OSD for Multi-core Scalability
        <https://ceph.io/en/news/blog/2023/crimson-multi-core-scalability/>`_.
        See the :ref:`Crimson developer
        documentation<crimson_dev_doc>`.

    CRUSH
        Controlled Replication Under Scalable Hashing ，可伸缩哈希控制的复制。
        它是 Ceph 用以计算对象存储位置的算法。

    CRUSH rule
    CRUSH 规则
        应用到某个特定存储池（们）的 CRUSH 数据归置规则。

    DAS
    直连存储器
        **D**\irect-\ **A**\ttached **S**\torage. Storage that is
        attached directly to the computer accessing it, without passing
        through a network.  Contrast with NAS and SAN.

    :ref:`Dashboard<mgr-dashboard>`
    仪表盘
        A built-in web-based Ceph management and monitoring application
        to administer various aspects and objects of the cluster. The
        dashboard is implemented as a Ceph Manager module. See
        :ref:`mgr-dashboard` for more details.

    Dashboard Module
    仪表盘模块
        Another name for :term:`Dashboard`.

    Dashboard Plugin
    仪表盘插件
        <原文空>

    Flapping OSD
        An OSD that is repeatedly marked ``up`` and then ``down`` in
        rapid succession. See :ref:`rados_tshooting_flapping_osd`.

    FQDN
    全资域名
        **F**\ully **Q**\ualified **D**\omain **N**\ame. A domain name
        that is applied to a node in a network and that specifies the
        node's exact location in the tree hierarchy of the DNS.

        In the context of Ceph cluster administration, FQDNs are often
        applied to hosts. In this documentation, the term "FQDN" is
        used mostly to distinguish between FQDNs and relatively simpler
        hostnames, which do not specify the exact location of the host
        in the tree hierarchy of the DNS but merely name the host.

    Host
    主机
        Any single machine or server in a Ceph Cluster. See :term:`Ceph
        Node`.

    Hybrid OSD
    混合式 OSD
        Refers to an OSD that has both HDD and SSD drives.

    librados
        An API that can be used to create a custom interface to a Ceph
        storage cluster. ``librados`` makes it possible to interact
        with Ceph Monitors and with OSDs. See :ref:`Introduction to
        librados <librados-intro>`. See :ref:`librados (Python)
        <librados-python>`.

    LVM tags
    LVM 标签
        LVM 卷和组的可扩展元数据，我们用它来存储 Ceph 相关的信息，
        如各设备、以及它们与 OSD 的关系。

    MDS
    元数据服务器
        The Ceph **M**\eta\ **D**\ata **S**\erver daemon. Also referred
        to as "ceph-mds". The Ceph metadata server daemon must be
        running in any Ceph cluster that runs the CephFS file system.
        The MDS stores all filesystem metadata. :term:`Client`\s work
        together with either a single MDS or a group of MDSes to
        maintain a distributed metadata cache that is required by
        CephFS.

        See :ref:`Deploying Metadata Servers<cephfs_add_remote_mds>`.

        See the :ref:`ceph-mds man page<ceph_mds_man>`.

    MGR
    管理器
        The Ceph manager software, which collects all the state from
        the whole cluster in one place.

    :ref:`MON<arch_monitor>`
    监视器
        The Ceph monitor software.

    Monitor Store
    监视器存储系统
        The persistent storage that is used by the Monitor. This
        includes the Monitor's RocksDB and all related files in
        ``/var/lib/ceph``.

    Node
        See :term:`Ceph Node`.

    Object Storage
                Object storage is one of three kinds of storage relevant to
                Ceph. The other two kinds of storage relevant to Ceph are file
                storage and block storage. Object storage is the category of
                storage most fundamental to Ceph.

    Object Storage Device
    对象存储设备（实体）
        一个物理的或逻辑的存储单元（如 LUN ）。有时候， Ceph 用户以术语 OSD 来引用
        :term:`Ceph OSD 守护进程`\ ，然而恰当的术语应该是 Ceph OSD 。

    OMAP
        "object map". A key-value store (a database) that is used to
        reduce the time it takes to read data from and to write to the
        Ceph cluster. RGW bucket indexes are stored as OMAPs.
        Erasure-coded pools cannot store RADOS OMAP data structures.

        Run the command ``ceph osd df`` to see your OMAPs.

        See Eleanor Cawthon's 2012 paper `A Distributed Key-Value Store
        using Ceph
        <https://ceph.io/assets/pdfs/CawthonKeyValueStore.pdf>`_ (17
        pages).

    OpenStack Swift
        In the context of Ceph, OpenStack Swift is one of the two APIs
        supported by the Ceph Object Store. The other API supported by
        the Ceph Object Store is S3.

        See `the OpenStack Storage API overview page
        <https://docs.openstack.org/swift/latest/api/object_api_v1_overview.html>`_.

    OSD
    对象存储设备
        Probably :term:`Ceph OSD`, but not necessarily. Sometimes
        (especially in older correspondence, and especially in
        documentation that is not written specifically for Ceph), "OSD"
        means "**O**\bject **S**\torage **D**\evice", which refers to a
        physical or logical storage unit (for example: LUN). The Ceph
        community has always used the term "OSD" to refer to
        :term:`Ceph OSD Daemon` despite an industry push in the
        mid-2010s to insist that "OSD" should refer to "Object Storage
        Device", so it is important to know which meaning is intended.

    OSD, flapping
        见 :term:`Flapping OSD`.

    OSD FSID
        The OSD fsid is a unique identifier that is used to identify an
        OSD. It is found in the OSD path in a file called ``osd_fsid``.
        The term ``FSID`` is used interchangeably with ``UUID``.
        这是一个唯一的标识符，用以提高一个 OSD 的唯一性，它位于 OSD 路径内、
        一个名为 ``osd_fsid`` 的文件里。这个 ``fsid`` 可以和 ``uuid`` 互换着用。

    OSD ID
        定义一个 OSD 的整数。它是在新建 OSD 期间由监视器们生成的。

    OSD UUID
        就像 OSD fsid ，这是 OSD 的唯一标识符，并且可以和 ``fsid`` 互换着用。

    Period
        In the context of :term:`RGW`, a period is the configuration
        state of the :term:`Realm`. The period stores the configuration
        state of a multi-site configuration. When the period is updated,
        the "epoch" is said thereby to have been changed.

    Placement Groups (PGs)
    归置组（ PGs ）
        Placement groups (PGs) are subsets of each logical Ceph pool.
        Placement groups perform the function of placing objects (as a
        group) into OSDs. Ceph manages data internally at
        placement-group granularity: this scales better than would
        managing individual (and therefore more numerous) RADOS
        objects. A cluster that has a larger number of placement groups
        (for example, 100 per OSD) is better balanced than an otherwise
        identical cluster with a smaller number of placement groups.

        Ceph's internal RADOS objects are each mapped to a specific
        placement group, and each placement group belongs to exactly
        one Ceph pool.

    PLP
            **P**\ower **L**\oss **P**\rotection. A technology that
            protects the data of solid-state drives by using capacitors to
            extend the amount of time available for transferring data from
            the DRAM cache to the SSD's permanent memory. Consumer-grade
            SSDs are rarely equipped with PLP.

    :ref:`Pool<rados_pools>`
    存储池
        A pool is a logical partition used to store objects.

    Pools
        See :term:`pool`.

    :ref:`Primary Affinity <rados_ops_primary_affinity>`
    主亲和性
        The characteristic of an OSD that governs the likelihood that
        a given OSD will be selected as the primary OSD (or "lead
        OSD") in an acting set. Primary affinity was introduced in
        Firefly (v. 0.80). See :ref:`Primary Affinity
        <rados_ops_primary_affinity>`.

    :ref:`Prometheus <mgr-prometheus>`
            An open-source monitoring and alerting toolkit. Ceph offers a
            :ref:`"Prometheus module" <mgr-prometheus>`, which provides a
            Prometheus exporter that passes performance counters from a
            collection point in ``ceph-mgr`` to Prometheus.

    Quorum
    法定人数
        Quorum is the state that exists when a majority of the
        :ref:`Monitors<arch_monitor>` in the cluster are ``up``. A
        minimum of three :ref:`Monitors<arch_monitor>` must exist in
        the cluster in order for Quorum to be possible.

    RADOS
        **R**\eliable **A**\utonomic **D**\istributed **O**\bject
        **S**\tore. RADOS is the object store that provides a scalable
        service for variably-sized objects. The RADOS object store is
        the core component of a Ceph cluster.  `This blog post from
        2009
        <https://ceph.io/en/news/blog/2009/the-rados-distributed-object-store/>`_
        provides a beginner's introduction to RADOS. Readers interested
        in a deeper understanding of RADOS are directed to `RADOS: A
        Scalable, Reliable Storage Service for Petabyte-scale Storage
        Clusters <https://ceph.io/assets/pdfs/weil-rados-pdsw07.pdf>`_.

    RADOS Cluster
    RADOS 集群
        A proper subset of the Ceph Cluster consisting of
        :term:`OSD`\s, :term:`Ceph Monitor`\s, and :term:`Ceph
        Manager`\s.

    RADOS Gateway
    RADOS 网关
        See :term:`RGW`.

    RBD
        **R**\ADOS **B**\lock **D**\evice. See :term:`Ceph Block
        Device`.

    :ref:`Realm<rgw-realms>`
        In the context of RADOS Gateway (RGW), a realm is a globally
        unique namespace that consists of one or more zonegroups.

    Releases

        Ceph Interim Release
            Ceph 临时版。
            尚未通过质检测试、但包含新功能的 Ceph 版本。

        Ceph Point Release
            Ceph 修正版。
            所有只包含缺陷修正或安全修正的特殊版本。

        Ceph Release
            Ceph 版本。
            任何版本号不同的 Ceph 。

        Ceph Release Candidate
            Ceph 发布候选版。
            Ceph 的主要版本，已经通过了初步质检测试，
            并且准备好给测试员试用了。

        Ceph Stable Release
            Ceph 稳定版。
            Ceph 的主要版本，之前临时版本里所有的功能
            都成功通过了质检测试。

    Reliable Autonomic Distributed Object Store
    可靠自主的分布式对象存储库
        The core set of storage software which stores the user's data
        (MON+OSD). See also :term:`RADOS`.

    :ref:`RGW<object-gateway>`
        **R**\ADOS **G**\ate\ **w**\ay. RADOS 网关。

        Also called "Ceph Object Gateway". The component of Ceph that
        provides a gateway to both the Amazon S3 RESTful API and the
        OpenStack Swift API.

    S3
            In the context of Ceph, S3 is one of the two APIs supported by
            the Ceph Object Store. The other API supported by the Ceph
            Object Store is OpenStack Swift.

            See `the Amazon S3 overview page
            <https://aws.amazon.com/s3/>`_.

    scrubs
        The processes by which Ceph ensures data integrity. During the
        process of scrubbing, Ceph generates a catalog of all objects
        in a placement group, then ensures that none of the objects are
        missing or mismatched by comparing each primary object against
        its replicas, which are stored across other OSDs. Any PG
        is determined to have a copy of an object that is different
        than the other copies or is missing entirely is marked
        "inconsistent" (that is, the PG is marked "inconsistent").

        There are two kinds of scrubbing: light scrubbing and deep
        scrubbing (also called "shallow scrubbing" and "deep scrubbing",
        respectively). Light scrubbing is performed daily and does
        nothing more than confirm that a given object exists and that
        its metadata is correct. Deep scrubbing is performed weekly and
        reads the data and uses checksums to ensure data integrity.

        See :ref:`Scrubbing <rados_config_scrubbing>` in the RADOS OSD
        Configuration Reference Guide and page 141 of *Mastering Ceph,
        second edition* (Fisk, Nick. 2019).

    secrets
        Secrets are credentials used to perform digital authentication
        whenever privileged users must access systems that require
        authentication. Secrets can be passwords, API keys, tokens, SSH
        keys, private certificates, or encryption keys.

    SDS
        **S**\oftware-**d**\efined **S**\torage.

    systemd oneshot
        一种 systemd 类型 ``type`` ，用于确定 ``ExecStart`` 的命令完成后是否退出
        （不想把它作为守护进程）。

    Swift
            See :term:`OpenStack Swift`.

    Teuthology
    测试方法学
        对 Ceph 进行脚本化测试的一系列软件。

    User
        An individual or a system actor (for example, an application)
        that uses Ceph clients to interact with the :term:`Ceph Storage
        Cluster`. See :ref:`User<rados-ops-user>` and :ref:`User
        Management<user-management>`.

    Zone
        In the context of :term:`RGW`, a zone is a logical group that
        consists of one or more :term:`RGW` instances.  A zone's
        configuration state is stored in the :term:`period`. See
        :ref:`Zones<radosgw-zones>`.


.. _https://github.com/ceph: https://github.com/ceph
.. _集群运行图: ../architecture#cluster-map
