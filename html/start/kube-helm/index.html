
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>安装（ Kubernetes + Helm ） &#8212; Ceph Documentation</title>
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/js/ceph.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../">Ceph Documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            

<div id="dev-warning" class="admonition note" style="display:none;">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>

<div id="eol-warning" class="admonition warning" style="display:none;">
  <p class="first admonition-title">Warning</p>
  <p class="last">This document is for an unsupported version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="display:none; padding: 15px; font-weight: bold;">
    <a id="edit-on-github" href="https://github.com/ceph/ceph/edit/master/doc/start/kube-helm.rst" rel="nofollow">Edit on GitHub</a> | <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <div class="section" id="kubernetes-helm">
<h1>安装（ Kubernetes + Helm ）<a class="headerlink" href="#kubernetes-helm" title="Permalink to this headline">¶</a></h1>
<p>The <a class="reference external" href="https://github.com/ceph/ceph-helm/">ceph-helm</a> project enables you to deploy Ceph in a Kubernetes environment.
This documentation assumes a Kubernetes environment is available.</p>
<div class="section" id="id1">
<h2>当前的局限性<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>The public and cluster networks must be the same</p></li>
<li><p>If the storage class user id is not admin, you will have to manually create the user
in your Ceph cluster and create its secret in Kubernetes</p></li>
<li><p>ceph-mgr can only run with 1 replica</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="helm">
<h2>安装并启动 Helm<a class="headerlink" href="#helm" title="Permalink to this headline">¶</a></h2>
<p>Helm can be installed by following these <a class="reference external" href="https://github.com/kubernetes/helm/blob/master/docs/install.md">instructions</a>.</p>
<p>Helm finds the Kubernetes cluster by reading from the local Kubernetes config file; make sure this is downloaded and accessible to the <code class="docutils literal notranslate"><span class="pre">helm</span></code> client.</p>
<p>A Tiller server must be configured and running for your Kubernetes cluster, and the local Helm client must be connected to it. It may be helpful to look at the Helm documentation for <a class="reference external" href="https://github.com/kubernetes/helm/blob/master/docs/helm/helm_init.md">init</a>. To run Tiller locally and connect Helm to it, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ helm init
</pre></div>
</div>
<p>The ceph-helm project uses a local Helm repo by default to store charts. To start a local Helm repo server, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ helm serve &amp;
$ helm repo add local http://localhost:8879/charts
</pre></div>
</div>
</div>
<div class="section" id="ceph-help-helm">
<h2>把 ceph-help 加进 Helm 的本地仓库<a class="headerlink" href="#ceph-help-helm" title="Permalink to this headline">¶</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ git clone https://github.com/ceph/ceph-helm
$ cd ceph-helm/ceph
$ make
</pre></div>
</div>
</div>
<div class="section" id="ceph">
<h2>配置 Ceph 集群<a class="headerlink" href="#ceph" title="Permalink to this headline">¶</a></h2>
<p>Create a <code class="docutils literal notranslate"><span class="pre">ceph-overrides.yaml</span></code> that will contain your Ceph configuration. This file may exist anywhere, but for this document will be assumed to reside in the user’s home directory.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat ~/ceph-overrides.yaml
network:
  public:   172.21.0.0/20
  cluster:   172.21.0.0/20

osd_devices:
  - name: dev-sdd
    device: /dev/sdd
    zap: &quot;1&quot;
  - name: dev-sde
    device: /dev/sde
    zap: &quot;1&quot;

storageclass:
  name: ceph-rbd
  pool: rbd
  user_id: k8s
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If journal is not set it will be colocated with device</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ceph-helm/ceph/ceph/values.yaml</span></code> file contains the full
list of option that can be set</p>
</div>
</div>
<div class="section" id="id2">
<h2>创建 Ceph 集群命名空间<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>By default, ceph-helm components assume they are to be run in the <code class="docutils literal notranslate"><span class="pre">ceph</span></code> Kubernetes namespace. To create the namespace, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl create namespace ceph
</pre></div>
</div>
</div>
<div class="section" id="rbac">
<h2>配置 RBAC 权限<a class="headerlink" href="#rbac" title="Permalink to this headline">¶</a></h2>
<p>Kubernetes &gt;=v1.6 makes RBAC the default admission controller. ceph-helm provides RBAC roles and permissions for each component:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl create -f ~/ceph-helm/ceph/rbac.yaml
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">rbac.yaml</span></code> file assumes that the Ceph cluster will be deployed in the <code class="docutils literal notranslate"><span class="pre">ceph</span></code> namespace.</p>
</div>
<div class="section" id="kubelets">
<h2>给 kubelets 打标<a class="headerlink" href="#kubelets" title="Permalink to this headline">¶</a></h2>
<dl class="simple">
<dt>The following labels need to be set to deploy a Ceph cluster:</dt><dd><ul class="simple">
<li><p>ceph-mon=enabled</p></li>
<li><p>ceph-mgr=enabled</p></li>
<li><p>ceph-osd=enabled</p></li>
<li><p>ceph-osd-device-&lt;name&gt;=enabled</p></li>
</ul>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">ceph-osd-device-&lt;name&gt;</span></code> label is created based on the osd_devices name value defined in our <code class="docutils literal notranslate"><span class="pre">ceph-overrides.yaml</span></code>.
From our example above we will have the two following label: <code class="docutils literal notranslate"><span class="pre">ceph-osd-device-dev-sdd</span></code> and <code class="docutils literal notranslate"><span class="pre">ceph-osd-device-dev-sde</span></code>.</p>
<p>For each Ceph Monitor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl label node &lt;nodename&gt; ceph-mon=enabled ceph-mgr=enabled
</pre></div>
</div>
<p>For each OSD node:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl label node &lt;nodename&gt; ceph-osd=enabled ceph-osd-device-dev-sdd=enabled ceph-osd-device-dev-sde=enabled
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h2>Ceph 部署<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>Run the helm install command to deploy Ceph:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ helm install --name=ceph local/ceph --namespace=ceph -f ~/ceph-overrides.yaml
NAME:   ceph
LAST DEPLOYED: Wed Oct 18 22:25:06 2017
NAMESPACE: ceph
STATUS: DEPLOYED

RESOURCES:
==&gt; v1/Secret
NAME                    TYPE    DATA  AGE
ceph-keystone-user-rgw  Opaque  7     1s

==&gt; v1/ConfigMap
NAME              DATA  AGE
ceph-bin-clients  2     1s
ceph-bin          24    1s
ceph-etc          1     1s
ceph-templates    5     1s

==&gt; v1/Service
NAME      CLUSTER-IP      EXTERNAL-IP  PORT(S)   AGE
ceph-mon  None            &lt;none&gt;       6789/TCP  1s
ceph-rgw  10.101.219.239  &lt;none&gt;       8088/TCP  1s

==&gt; v1beta1/DaemonSet
NAME              DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE  NODE-SELECTOR                                     AGE
ceph-mon          3        3        0      3           0          ceph-mon=enabled                                  1s
ceph-osd-dev-sde  3        3        0      3           0          ceph-osd-device-dev-sde=enabled,ceph-osd=enabled  1s
ceph-osd-dev-sdd  3        3        0      3           0          ceph-osd-device-dev-sdd=enabled,ceph-osd=enabled  1s

==&gt; v1beta1/Deployment
NAME                  DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE
ceph-mds              1        1        1           0          1s
ceph-mgr              1        1        1           0          1s
ceph-mon-check        1        1        1           0          1s
ceph-rbd-provisioner  2        2        2           0          1s
ceph-rgw              1        1        1           0          1s

==&gt; v1/Job
NAME                                 DESIRED  SUCCESSFUL  AGE
ceph-mgr-keyring-generator           1        0           1s
ceph-mds-keyring-generator           1        0           1s
ceph-osd-keyring-generator           1        0           1s
ceph-rgw-keyring-generator           1        0           1s
ceph-mon-keyring-generator           1        0           1s
ceph-namespace-client-key-generator  1        0           1s
ceph-storage-keys-generator          1        0           1s

==&gt; v1/StorageClass
NAME     TYPE
ceph-rbd  ceph.com/rbd
</pre></div>
</div>
<p>The output from helm install shows us the different types of resources that will be deployed.</p>
<p>A StorageClass named <code class="docutils literal notranslate"><span class="pre">ceph-rbd</span></code> of type <code class="docutils literal notranslate"><span class="pre">ceph.com/rbd</span></code> will be created with <code class="docutils literal notranslate"><span class="pre">ceph-rbd-provisioner</span></code> Pods. These
will allow a RBD to be automatically provisioned upon creation of a PVC. RBDs will also be formatted when mapped for the first
time. All RBDs will use the ext4 file system. <code class="docutils literal notranslate"><span class="pre">ceph.com/rbd</span></code> does not support the <code class="docutils literal notranslate"><span class="pre">fsType</span></code> option.
By default, RBDs will use image format 2 and layering. You can overwrite the following storageclass’ defaults in your values file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">storageclass</span><span class="p">:</span>
  <span class="n">name</span><span class="p">:</span> <span class="n">ceph</span><span class="o">-</span><span class="n">rbd</span>
  <span class="n">pool</span><span class="p">:</span> <span class="n">rbd</span>
  <span class="n">user_id</span><span class="p">:</span> <span class="n">k8s</span>
  <span class="n">user_secret_name</span><span class="p">:</span> <span class="n">pvc</span><span class="o">-</span><span class="n">ceph</span><span class="o">-</span><span class="n">client</span><span class="o">-</span><span class="n">key</span>
  <span class="n">image_format</span><span class="p">:</span> <span class="s2">&quot;2&quot;</span>
  <span class="n">image_features</span><span class="p">:</span> <span class="n">layering</span>
</pre></div>
</div>
<p>Check that all Pods are running with the command below. This might take a few minutes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl -n ceph get pods
NAME                                    READY     STATUS    RESTARTS   AGE
ceph-mds-3804776627-976z9               0/1       Pending   0          1m
ceph-mgr-3367933990-b368c               1/1       Running   0          1m
ceph-mon-check-1818208419-0vkb7         1/1       Running   0          1m
ceph-mon-cppdk                          3/3       Running   0          1m
ceph-mon-t4stn                          3/3       Running   0          1m
ceph-mon-vqzl0                          3/3       Running   0          1m
ceph-osd-dev-sdd-6dphp                  1/1       Running   0          1m
ceph-osd-dev-sdd-6w7ng                  1/1       Running   0          1m
ceph-osd-dev-sdd-l80vv                  1/1       Running   0          1m
ceph-osd-dev-sde-6dq6w                  1/1       Running   0          1m
ceph-osd-dev-sde-kqt0r                  1/1       Running   0          1m
ceph-osd-dev-sde-lp2pf                  1/1       Running   0          1m
ceph-rbd-provisioner-2099367036-4prvt   1/1       Running   0          1m
ceph-rbd-provisioner-2099367036-h9kw7   1/1       Running   0          1m
ceph-rgw-3375847861-4wr74               0/1       Pending   0          1m
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The MDS and RGW Pods are pending since we did not label any nodes with
<code class="docutils literal notranslate"><span class="pre">ceph-rgw=enabled</span></code> or <code class="docutils literal notranslate"><span class="pre">ceph-mds=enabled</span></code></p>
</div>
<p>Once all Pods are running, check the status of the Ceph cluster from one Mon:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl -n ceph exec -ti ceph-mon-cppdk -c ceph-mon -- ceph -s
cluster:
  id:     e8f9da03-c2d2-4ad3-b807-2a13d0775504
  health: HEALTH_OK

services:
  mon: 3 daemons, quorum mira115,mira110,mira109
  mgr: mira109(active)
  osd: 6 osds: 6 up, 6 in

data:
  pools:   0 pools, 0 pgs
  objects: 0 objects, 0 bytes
  usage:   644 MB used, 5555 GB / 5556 GB avail
  pgs:
</pre></div>
</div>
</div>
<div class="section" id="pod-ceph-persistentvolume">
<h2>配置 pod 以使用 Ceph 提供的 PersistentVolume<a class="headerlink" href="#pod-ceph-persistentvolume" title="Permalink to this headline">¶</a></h2>
<p>Create a keyring for the k8s user defined in the <code class="docutils literal notranslate"><span class="pre">~/ceph-overwrite.yaml</span></code> and convert
it to base64:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl -n ceph exec -ti ceph-mon-cppdk -c ceph-mon -- bash
# ceph auth get-or-create-key client.k8s mon &#39;allow r&#39; osd &#39;allow rwx pool=rbd&#39;  | base64
QVFCLzdPaFoxeUxCRVJBQUVEVGdHcE9YU3BYMVBSdURHUEU0T0E9PQo=
# exit
</pre></div>
</div>
<p>Edit the user secret present in the <code class="docutils literal notranslate"><span class="pre">ceph</span></code> namespace:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl -n ceph edit secrets/pvc-ceph-client-key
</pre></div>
</div>
<p>Add the base64 value to the key value with your own and save:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">apiVersion</span><span class="p">:</span> <span class="n">v1</span>
<span class="n">data</span><span class="p">:</span>
  <span class="n">key</span><span class="p">:</span> <span class="n">QVFCLzdPaFoxeUxCRVJBQUVEVGdHcE9YU3BYMVBSdURHUEU0T0E9PQo</span><span class="o">=</span>
<span class="n">kind</span><span class="p">:</span> <span class="n">Secret</span>
<span class="n">metadata</span><span class="p">:</span>
  <span class="n">creationTimestamp</span><span class="p">:</span> <span class="mi">2017</span><span class="o">-</span><span class="mi">10</span><span class="o">-</span><span class="mi">19</span><span class="n">T17</span><span class="p">:</span><span class="mi">34</span><span class="p">:</span><span class="mi">04</span><span class="n">Z</span>
  <span class="n">name</span><span class="p">:</span> <span class="n">pvc</span><span class="o">-</span><span class="n">ceph</span><span class="o">-</span><span class="n">client</span><span class="o">-</span><span class="n">key</span>
  <span class="n">namespace</span><span class="p">:</span> <span class="n">ceph</span>
  <span class="n">resourceVersion</span><span class="p">:</span> <span class="s2">&quot;8665522&quot;</span>
  <span class="n">selfLink</span><span class="p">:</span> <span class="o">/</span><span class="n">api</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">namespaces</span><span class="o">/</span><span class="n">ceph</span><span class="o">/</span><span class="n">secrets</span><span class="o">/</span><span class="n">pvc</span><span class="o">-</span><span class="n">ceph</span><span class="o">-</span><span class="n">client</span><span class="o">-</span><span class="n">key</span>
  <span class="n">uid</span><span class="p">:</span> <span class="n">b4085944</span><span class="o">-</span><span class="n">b4f3</span><span class="o">-</span><span class="mf">11e7</span><span class="o">-</span><span class="n">add7</span><span class="o">-</span><span class="mi">002590347682</span>
<span class="nb">type</span><span class="p">:</span> <span class="n">kubernetes</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">rbd</span>
</pre></div>
</div>
<p>We are going to create a Pod that consumes a RBD in the default namespace.
Copy the user secret from the <code class="docutils literal notranslate"><span class="pre">ceph</span></code> namespace to <code class="docutils literal notranslate"><span class="pre">default</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl -n ceph get secrets/pvc-ceph-client-key -o json | jq &#39;.metadata.namespace = &quot;default&quot;&#39; | kubectl create -f -
secret &quot;pvc-ceph-client-key&quot; created
$ kubectl get secrets
NAME                  TYPE                                  DATA      AGE
default-token-r43wl   kubernetes.io/service-account-token   3         61d
pvc-ceph-client-key   kubernetes.io/rbd                     1         20s
</pre></div>
</div>
<p>Create and initialize the RBD pool:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl -n ceph exec -ti ceph-mon-cppdk -c ceph-mon -- ceph osd pool create rbd
pool &#39;rbd&#39; created
$ kubectl -n ceph exec -ti ceph-mon-cppdk -c ceph-mon -- rbd pool init rbd
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Kubernetes uses the RBD kernel module to map RBDs to hosts. Luminous requires
CRUSH_TUNABLES 5 (Jewel). The minimal kernel version for these tunables is 4.5.
If your kernel does not support these tunables, run <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">osd</span> <span class="pre">crush</span> <span class="pre">tunables</span> <span class="pre">hammer</span></code></p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Since RBDs are mapped on the host system. Hosts need to be able to resolve
the ceph-mon.ceph.svc.cluster.local name managed by the kube-dns service. To get the
IP address of the kube-dns service, run <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">-n</span> <span class="pre">kube-system</span> <span class="pre">get</span> <span class="pre">svc/kube-dns</span></code></p>
</div>
<p>Create a PVC:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat pvc-rbd.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ceph-pvc
spec:
  accessModes:
   - ReadWriteOnce
  resources:
    requests:
       storage: 20Gi
  storageClassName: ceph-rbd

$ kubectl create -f pvc-rbd.yaml
persistentvolumeclaim &quot;ceph-pvc&quot; created
$ kubectl get pvc
NAME       STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS   AGE
ceph-pvc   Bound     pvc-1c2ada50-b456-11e7-add7-002590347682   20Gi       RWO           ceph-rbd        3s
</pre></div>
</div>
<p>You can check that the RBD has been created on your cluster:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl -n ceph exec -ti ceph-mon-cppdk -c ceph-mon -- rbd ls
kubernetes-dynamic-pvc-1c2e9442-b456-11e7-9bd2-2a4159ce3915
$ kubectl -n ceph exec -ti ceph-mon-cppdk -c ceph-mon -- rbd info kubernetes-dynamic-pvc-1c2e9442-b456-11e7-9bd2-2a4159ce3915
rbd image &#39;kubernetes-dynamic-pvc-1c2e9442-b456-11e7-9bd2-2a4159ce3915&#39;:
    size 20480 MB in 5120 objects
    order 22 (4096 kB objects)
    block_name_prefix: rbd_data.10762ae8944a
    format: 2
    features: layering
    flags:
    create_timestamp: Wed Oct 18 22:45:59 2017
</pre></div>
</div>
<p>Create a Pod that will use the PVC:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat pod-with-rbd.yaml
kind: Pod
apiVersion: v1
metadata:
  name: mypod
spec:
  containers:
    - name: busybox
      image: busybox
      command:
        - sleep
        - &quot;3600&quot;
      volumeMounts:
      - mountPath: &quot;/mnt/rbd&quot;
        name: vol1
  volumes:
    - name: vol1
      persistentVolumeClaim:
        claimName: ceph-pvc

$ kubectl create -f pod-with-rbd.yaml
pod &quot;mypod&quot; created
</pre></div>
</div>
<p>Check the Pod:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl get pods
NAME      READY     STATUS    RESTARTS   AGE
mypod     1/1       Running   0          17s
$ kubectl exec mypod -- mount | grep rbd
/dev/rbd0 on /mnt/rbd type ext4 (rw,relatime,stripe=1024,data=ordered)
</pre></div>
</div>
</div>
<div class="section" id="id4">
<h2>日志<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h2>
<p>OSDs and Monitor logs can be accessed via the <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">logs</span> <span class="pre">[-f]</span></code> command. Monitors have multiple stream of logging,
each stream is accessible from a container running in the ceph-mon Pod.</p>
<dl class="simple">
<dt>There are 3 containers running in the ceph-mon Pod:</dt><dd><ul class="simple">
<li><p>ceph-mon, equivalent of ceph-mon.hostname.log on baremetal</p></li>
<li><p>cluster-audit-log-tailer, equivalent of ceph.audit.log on baremetal</p></li>
<li><p>cluster-log-tailer, equivalent of ceph.log on baremetal or <code class="docutils literal notranslate"><span class="pre">ceph</span> <span class="pre">-w</span></code></p></li>
</ul>
</dd>
</dl>
<p>Each container is accessible via the <code class="docutils literal notranslate"><span class="pre">--container</span></code> or <code class="docutils literal notranslate"><span class="pre">-c</span></code> option.
For instance, to access the cluster-tail-log, one can run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ kubectl -n ceph logs ceph-mon-cppdk -c cluster-log-tailer
</pre></div>
</div>
</div>
</div>



          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../">
              <img class="logo" src="../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../">Table Of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">安装（手动）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/developer_guide/">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../translation_cn/">中文版翻译资源</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../">Ceph Documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).
    </div>
  </body>
</html>