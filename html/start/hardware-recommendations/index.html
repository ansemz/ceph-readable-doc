
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>硬件推荐 &#8212; Ceph Documentation</title>
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/language_data.js"></script>
    <script src="../../_static/js/ceph.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="推荐操作系统" href="../os-recommendations/" />
    <link rel="prev" title="Ceph 简介" href="../intro/" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../os-recommendations/" title="推荐操作系统"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../intro/" title="Ceph 简介"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../">Ceph Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../intro/" accesskey="U">Ceph 简介</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            

<div id="dev-warning" class="admonition note" style="display:none;">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>

<div id="eol-warning" class="admonition warning" style="display:none;">
  <p class="first admonition-title">Warning</p>
  <p class="last">This document is for an unsupported version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="display:none; padding: 15px; font-weight: bold;">
    <a id="edit-on-github" href="https://github.com/ceph/ceph/edit/master/doc/start/hardware-recommendations.rst" rel="nofollow">Edit on GitHub</a> | <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <div class="section" id="hardware-recommendations">
<span id="id1"></span><h1>硬件推荐<a class="headerlink" href="#hardware-recommendations" title="Permalink to this headline">¶</a></h1>
<p>Ceph 为普通硬件设计，这可使构建、维护 PB 级数据集群的费用相对低廉。规划集群硬件时，需要均衡几方面的因素，包括区域失效和潜在的性能问题。硬件规划要包含把使用 Ceph 集群的 Ceph 守护进程和其他进程恰当分布。通常，我们推荐在一台机器上只运行一种类型的守护进程。我们推荐把使用数据集群的进程（如 OpenStack 、 CloudStack 等）安装在别的机器上。</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>另请参考 <a class="reference external" href="https://ceph.com/community/blog/">Ceph 博客文章</a>。</p>
</div>
<div class="section" id="cpu">
<h2>CPU<a class="headerlink" href="#cpu" title="Permalink to this headline">¶</a></h2>
<p>Ceph 元数据服务器对 CPU 敏感，它会动态地重分布它们的负载，所以你的元数据服务器应该有足够的处理能力（如 4 核或更强悍的 CPU
）。 Ceph 的 OSD 运行着 <a class="reference internal" href="../../glossary/#term-rados"><span class="xref std std-term">RADOS</span></a> 服务、用 <a class="reference internal" href="../../glossary/#term-crush"><span class="xref std std-term">CRUSH</span></a>
计算数据存放位置、复制数据、维护它自己的集群运行图副本，因此
OSD 需要一定的处理能力（如双核 CPU ）。监视器只简单地维护着集群运行图的副本，因此对 CPU 不敏感；但必须考虑机器以后是否还会运行 Ceph 监视器以外的 CPU 密集型任务。例如，如果服务器以后要运行用于计算的虚拟机（如 OpenStack Nova ），你就要确保给
Ceph 进程保留了足够的处理能力，所以我们推荐在其他机器上运行
CPU 密集型任务。</p>
</div>
<div class="section" id="ram">
<h2>RAM 内存<a class="headerlink" href="#ram" title="Permalink to this headline">¶</a></h2>
<p>一般来说，内存越多越好。</p>
<div class="section" id="ceph-mon-ceph-mgr">
<h3>监视器和管理器（ ceph-mon 和 ceph-mgr ）<a class="headerlink" href="#ceph-mon-ceph-mgr" title="Permalink to this headline">¶</a></h3>
<p>Monitor and manager daemon memory usage generally scales with the size of the
cluster.  For small clusters, 1-2 GB is generally sufficient.  For
large clusters, you should provide more (5-10 GB).  You may also want
to consider tuning settings like <code class="docutils literal notranslate"><span class="pre">mon_osd_cache_size</span></code> or
<code class="docutils literal notranslate"><span class="pre">rocksdb_cache_size</span></code>.</p>
</div>
<div class="section" id="ceph-mds">
<h3>元数据服务器（ ceph-mds ）<a class="headerlink" href="#ceph-mds" title="Permalink to this headline">¶</a></h3>
<p>The metadata daemon memory utilization depends on how much memory its cache is
configured to consume.  We recommend 1 GB as a minimum for most systems.  See
<code class="docutils literal notranslate"><span class="pre">mds_cache_memory</span></code>.</p>
</div>
<div class="section" id="osds-ceph-osd">
<h3>OSDs (ceph-osd)<a class="headerlink" href="#osds-ceph-osd" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="id2">
<h2>内存<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Bluestore uses its own memory to cache data rather than relying on the
operating system page cache.  In bluestore you can adjust the amount of memory
the OSD attempts to consume with the <code class="docutils literal notranslate"><span class="pre">osd_memory_target</span></code> configuration
option.</p>
<ul class="simple">
<li><p>Setting the osd_memory_target below 2GB is typically not recommended (it may
fail to keep the memory that low and may also cause extremely slow performance.</p></li>
<li><p>Setting the memory target between 2GB and 4GB typically works but may result
in degraded performance as metadata may be read from disk during IO unless the
active data set is relatively small.</p></li>
<li><p>4GB is the current default osd_memory_target size and was set that way to try
and balance memory requirements and OSD performance for typical use cases.</p></li>
<li><p>Setting the osd_memory_target higher than 4GB may improve performance when
there are many (small) objects or large (256GB/OSD or more) data sets being
processed.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>The OSD memory autotuning is “best effort”.  While the OSD may
unmap memory to allow the kernel to reclaim it, there is no guarantee that
the kernel will actually reclaim freed memory within any specific time
frame.  This is especially true in older versions of Ceph where transparent
huge pages can prevent the kernel from reclaiming memory freed from
fragmented huge pages. Modern versions of Ceph disable transparent huge
pages at the application level to avoid this, though that still does not
guarantee that the kernel will immediately reclaim unmapped memory.  The OSD
may still at times exceed it’s memory target.  We recommend budgeting around
20% extra memory on your system to prevent OSDs from going OOM during
temporary spikes or due to any delay in reclaiming freed pages by the
kernel.  That value may be more or less than needed depending on the exact
configuration of the system.</p>
</div>
<p>When using the legacy FileStore backend, the page cache is used for caching
data, so no tuning is normally needed, and the OSD memory consumption is
generally related to the number of PGs per daemon in the system.</p>
</div>
<div class="section" id="id3">
<h2>数据存储<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>要谨慎地规划数据存储配置，因为其间涉及明显的成本和性能折衷。来自操作系统的并行操作和到单个硬盘的多个守护进程并发读、写请求操作会极大地降低性能。</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>因为 Ceph 发送 ACK 前必须把所有数据写入日志（至少对 xfs 来说是），因此均衡日志和 OSD 性能相当重要。</p>
</div>
<div class="section" id="id4">
<h3>硬盘驱动器<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>OSD 应该有足够的空间用于存储对象数据。考虑到大硬盘的每 GB 成本，我们建议用容量大于 1TB 的硬盘。建议用 GB 数除以硬盘价格来计算每 GB 成本，因为较大的硬盘通常会对每 GB 成本有较大影响，例如，单价为 $75 的 1TB 硬盘其每 GB 价格为 $0.07 （
$75/1024=0.0732 ），又如单价为 $150 的 3TB 硬盘其每 GB 价格为
$0.05 （ $150/3072=0.0488 ），这样使用 1TB 硬盘会增加 40% 的每 GB 价格，它将表现为较低的经济性。另外，单个驱动器容量越大，其对应的 OSD 所需内存就越大，特别是在重均衡、回填、恢复期间。根据经验， 1TB 的存储空间大约需要 1GB 内存。</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>不顾分区而在单个硬盘上运行多个OSD，这样<strong>不明智</strong>！</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>不顾分区而在运行了OSD的硬盘上同时运行监视器或元数据服务器也<strong>不明智</strong>！</p>
</div>
<p>存储驱动器受限于寻道时间、访问时间、读写时间、还有总吞吐量，这些物理局限性影响着整体系统性能，尤其在系统恢复期间。因此我们推荐独立的驱动器用于安装操作系统和软件，另外每个 OSD 守护进程占用一个驱动器。大多数 “slow OSD”问题的起因都是在相同的硬盘上运行了操作系统、多个 OSD 、和/或多个日志文件。鉴于解决性能问题的成本差不多会超过另外增加磁盘驱动器，你应该在设计时就避免增加 OSD 存储驱动器的负担来提升性能。</p>
<p>Ceph 允许你在每块硬盘驱动器上运行多个 OSD ，但这会导致资源竞争并降低总体吞吐量； Ceph 也允许把日志和对象数据存储在相同驱动器上，但这会增加记录写日志并回应客户端的延时，因为 Ceph 必须先写入日志才会回应确认了写动作。</p>
<p>Ceph 最佳实践指示，你应该分别在单独的硬盘运行操作系统、 OSD
数据和 OSD 日志。</p>
</div>
<div class="section" id="id5">
<h3>固态硬盘<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>一种提升性能的方法是使用固态硬盘（ SSD ）来降低随机访问时间和读延时，同时增加吞吐量。 SSD 和硬盘相比每 GB 成本通常要高 10 倍以上，但访问时间至少比硬盘快 100 倍。</p>
<p>SSD 没有可移动机械部件，所以不存在和硬盘一样的局限性。但 SSD 也有局限性，评估SSD 时，顺序读写性能很重要，在为多个 OSD 存储日志时，有着 400MB/s 顺序读写吞吐量的 SSD 其性能远高于 120MB/s 的。</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>我们建议发掘 SSD 的用法来提升性能。然而在大量投入 SSD 前，我们<strong>强烈建议</strong>核实 SSD 的性能指标，并在测试环境下衡量性能。</p>
</div>
<p>正因为 SSD 没有移动机械部件，所以它很适合 Ceph 里不需要太多存储空间的地方。相对廉价的 SSD 很诱人，慎用！可接受的 IOPS 指标对选择用于 Ceph 的 SSD 还不够，用于日志和 SSD 时还有几个重要考量：</p>
<ul class="simple">
<li><p><strong>写密集语义：</strong> 记日志涉及写密集语义，所以你要确保选用的 SSD 写入性能和硬盘相当或好于硬盘。廉价 SSD 可能在加速访问的同时引入写延时，有时候高性能硬盘的写入速度可以和便宜 SSD 相媲美。</p></li>
<li><p><strong>顺序写入：</strong> 在一个 SSD 上为多个 OSD 存储多个日志时也必须考虑 SSD 的顺序写入极限，因为它们要同时处理多个 OSD 日志的写入请求。</p></li>
<li><p><strong>分区对齐：</strong> 采用了 SSD 的一个常见问题是人们喜欢分区，却常常忽略了分区对齐，这会导致 SSD 的数据传输速率慢很多，所以请确保分区对齐了。</p></li>
</ul>
<p>SSD 用于对象存储太昂贵了，但是把 OSD 的日志存到 SSD 、把对象数据存储到独立的硬盘可以明显提升性能。 <code class="docutils literal notranslate"><span class="pre">osd</span> <span class="pre">journal</span></code> 选项的默认值是 <code class="docutils literal notranslate"><span class="pre">/var/lib/ceph/osd/$cluster-$id/journal</span></code> ，你可以把它挂载到一个 SSD 或 SSD 分区，这样它就不再是和对象数据一样存储在同一个硬盘上的文件了。</p>
<p>提升 CephFS 文件系统性能的一种方法是从 CephFS 文件内容里分离出元数据。 Ceph 提供了默认的 <code class="docutils literal notranslate"><span class="pre">metadata</span></code> 存储池来存储 CephFS 元数据，所以你不需要给 CephFS 元数据创建存储池，但是可以给它创建一个仅指向某主机 SSD 的 CRUSH 运行图。详情见<a class="reference external" href="../../rados/operations/crush-map#placing-different-pools-on-different-osds">给存储池指定 OSD</a> 。</p>
</div>
<div class="section" id="id6">
<h3>控制器<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>硬盘控制器对写吞吐量也有显著影响，要谨慎地选择，以免产生性能瓶颈。</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><a class="reference external" href="https://ceph.com/community/blog/">Ceph 博客文章</a> 常常是优秀的 Ceph 性能问题信息源，见
<a class="reference external" href="http://ceph.com/community/ceph-performance-part-1-disk-controller-write-throughput/">Ceph Write Throughput 1</a> 和 <a class="reference external" href="http://ceph.com/community/ceph-performance-part-2-write-throughput-without-ssd-journals/">Ceph Write Throughput 2</a> 。</p>
</div>
</div>
<div class="section" id="id7">
<h3>其他注意事项<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>你可以在同一主机上运行多个 OSD ，但要确保 OSD 硬盘总吞吐量不超过为客户端提供读写服务所需的网络带宽；还要考虑集群在每台主机上所存储的数据占总体的百分比，如果一台主机所占百分比太大而它挂了，就可能导致诸如超过 <code class="docutils literal notranslate"><span class="pre">full</span> <span class="pre">ratio</span></code> 的问题，此问题会使 Ceph 中止运作以防数据丢失。</p>
<p>如果每台主机运行多个 OSD ，也得保证内核是最新的。参阅<a class="reference external" href="../os-recommendations">操作系统推荐</a>里关于 <code class="docutils literal notranslate"><span class="pre">glibc</span></code> 和 <code class="docutils literal notranslate"><span class="pre">syncfs(2)</span></code> 的部分，确保硬件性能可达期望值。</p>
<p>OSD 数量较多（如 20 个以上）的主机会派生出大量线程，尤其是在恢复和重均衡期间。很多 Linux 内核默认的最大线程数较小（如 32k 个），如果您遇到了这类问题，可以把 <code class="docutils literal notranslate"><span class="pre">kernel.pid_max</span></code> 值调高些。理论最大值是 4194303 。例如把下列这行加入 <code class="docutils literal notranslate"><span class="pre">/etc/sysctl.conf</span></code> 文件：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span><span class="o">.</span><span class="n">pid_max</span> <span class="o">=</span> <span class="mi">4194303</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id8">
<h2>网络<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>建议每台机器最少两个千兆网卡，现在大多数机械硬盘都能达到大概 100MB/s 的吞吐量，网卡应该能处理所有 OSD 硬盘总吞吐量，所以推荐最少两个千兆网卡，分别用于公网（前端）和集群网络（后端）。集群网络（最好别连接到国际互联网）用于处理由数据复制产生的额外负载，而且可防止拒绝服务攻击，拒绝服务攻击会干扰数据归置组，使之在 OSD 数据复制时不能回到 <code class="docutils literal notranslate"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span></code> 状态。请考虑部署万兆网卡。通过 1Gbps 网络复制 1TB 数据耗时 3 小时，而 3TB （典型配置）需要 9 小时，相比之下，如果使用 10Gbps 复制时间可分别缩减到 20 分钟和 1 小时。在一个 PB 级集群中， OSD 磁盘失败是常态，而非异常；在性价比合理的的前提下，系统管理员想让 PG 尽快从 <code class="docutils literal notranslate"><span class="pre">degraded</span></code> （降级）状态恢复到 <code class="docutils literal notranslate"><span class="pre">active</span> <span class="pre">+</span> <span class="pre">clean</span></code> 状态。另外，一些部署工具（如 Dell 的 Crowbar ）部署了 5 个不同的网络，但使用了 VLAN 以提高网络和硬件可管理性。 VLAN 使用 802.1q 协议，还需要采用支持 VLAN 功能的网卡和交换机，增加的硬件成本可用节省的运营（网络安装、维护）成本抵消。使用 VLAN 来处理集群和计算栈（如 OpenStack 、 CloudStack 等等）之间的 VM 流量时，采用 10G 网卡仍然值得。每个网络的机架路由器到核心路由器应该有更大的带宽，如 40Gbps 到 100Gbps 。</p>
<p>服务器应配置底板管理控制器（ Baseboard Management Controller, BMC ），管理和部署工具也应该大规模使用 BMC ，所以请考虑带外网络管理的成本/效益平衡，此程序管理着 SSH 访问、 VM 映像上传、操作系统安装、端口管理、等等，会徒增网络负载。运营 3 个网络有点过分，但是每条流量路径都指示了部署一个大型数据集群前要仔细考虑的潜能力、吞吐量、性能瓶颈。</p>
</div>
<div class="section" id="id9">
<h2>故障域<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>故障域指任何导致不能访问一个或多个 OSD 的故障，可以是主机上停止的进程、硬盘故障、操作系统崩溃、有问题的网卡、损坏的电源、断网、断电等等。规划硬件需求时，要在多个需求间寻求平衡点，像付出很多努力减少故障域带来的成本削减、隔离每个潜在故障域增加的成本。</p>
</div>
<div class="section" id="id10">
<h2>最低硬件推荐<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>Ceph 可以运行在廉价的普通硬件上，小型生产集群和开发集群可以在一般的硬件上。</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 20%" />
<col style="width: 23%" />
<col style="width: 58%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>进程</p></th>
<th class="head"><p>规范</p></th>
<th class="head"><p>最低建议</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td rowspan="5"><p><code class="docutils literal notranslate"><span class="pre">ceph-osd</span></code></p></td>
<td><p>Processor</p></td>
<td><ul class="simple">
<li><p>1 core minimum</p></li>
<li><p>1 core per 200-500 MB/s</p></li>
<li><p>1 core per 1000-3000 IOPS</p></li>
</ul>
<ul class="simple">
<li><p>Results are before replication.</p></li>
<li><p>Results may vary with different
CPU models and Ceph features.
(erasure coding, compression, etc)</p></li>
<li><p>ARM processors specifically may
require additional cores.</p></li>
<li><p>Actual performance depends on many
factors including disk, network, and
client throughput and latency.
Benchmarking is highly recommended.</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>RAM</p></td>
<td><ul class="simple">
<li><p>4GB+ per daemon (more is better)</p></li>
<li><p>2-4GB often functions (may be slow)</p></li>
<li><p>Less than 2GB not recommended</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>Volume Storage</p></td>
<td><p>1x storage drive per daemon</p></td>
</tr>
<tr class="row-odd"><td><p>DB/WAL</p></td>
<td><p>1x SSD partition per daemon (optional)</p></td>
</tr>
<tr class="row-even"><td><p>Network</p></td>
<td><p>1x 1GbE+ NICs (10GbE+ recommended)</p></td>
</tr>
<tr class="row-odd"><td rowspan="4"><p><code class="docutils literal notranslate"><span class="pre">ceph-mon</span></code></p></td>
<td><p>Processor</p></td>
<td><ul class="simple">
<li><p>1 core minimum</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>RAM</p></td>
<td><p>2GB+ per daemon</p></td>
</tr>
<tr class="row-odd"><td><p>Disk Space</p></td>
<td><p>10 GB per daemon</p></td>
</tr>
<tr class="row-even"><td><p>Network</p></td>
<td><p>1x 1GbE+ NICs</p></td>
</tr>
<tr class="row-odd"><td rowspan="4"><p><code class="docutils literal notranslate"><span class="pre">ceph-mds</span></code></p></td>
<td><p>Processor</p></td>
<td><ul class="simple">
<li><p>1 core minimum</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>RAM</p></td>
<td><p>2GB+ per daemon</p></td>
</tr>
<tr class="row-odd"><td><p>Disk Space</p></td>
<td><p>1 MB per daemon</p></td>
</tr>
<tr class="row-even"><td><p>Network</p></td>
<td><p>1x 1GbE+ NICs</p></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>如果在只有一块硬盘的机器上运行 OSD ，要把数据和操作系统分别放到不同分区；一般来说，我们推荐操作系统和数据分别使用不同的硬盘。</p>
</div>
</div>
</div>



          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../">
              <img class="logo" src="../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../intro/">Ceph 简介</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">硬件推荐</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cpu">CPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ram">RAM 内存</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#ceph-mon-ceph-mgr">监视器和管理器（ ceph-mon 和 ceph-mgr ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#ceph-mds">元数据服务器（ ceph-mds ）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#osds-ceph-osd">OSDs (ceph-osd)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id2">内存</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">数据存储</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">硬盘驱动器</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">固态硬盘</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">控制器</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">其他注意事项</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id8">网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id9">故障域</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">最低硬件推荐</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../os-recommendations/">操作系统推荐</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get-involved/">加入 Ceph 社区！</a></li>
<li class="toctree-l2"><a class="reference internal" href="../documenting-ceph/">贡献 Ceph 文档</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/developer_guide/">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../translation_cn/">中文版翻译资源</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../os-recommendations/" title="推荐操作系统"
             >next</a> |</li>
        <li class="right" >
          <a href="../intro/" title="Ceph 简介"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../">Ceph Documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../intro/" >Ceph 简介</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).
    </div>
  </body>
</html>