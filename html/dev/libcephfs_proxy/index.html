

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Design of the libcephfs proxy &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="库体系结构" href="../libs/" />
    <link rel="prev" title="Hacking on Ceph in Kubernetes with Rook" href="../kubernetes/" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../internals/">Ceph 内幕</a> &raquo;</li>
      <li>Design of the libcephfs proxy</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/dev/libcephfs_proxy.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../">
          

          
            
            <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/">开发者指南</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../internals/">Ceph 内幕</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../balancer-design/">Ceph 如何均衡（读写、容量）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blkin/">Tracing Ceph With LTTng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blkin/#tracing-ceph-with-blkin">Tracing Ceph With Blkin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bluestore/">BlueStore Internals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ceph_krb_auth/">如何配置好 Ceph Kerberos 认证的详细文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-mirroring/">CephFS Mirroring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-reclaim/">CephFS Reclaim Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-snapshots/">CephFS 快照</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephx/">Cephx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephx_protocol/">Cephx 认证协议详细阐述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../config/">配置管理系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../config-key/">config-key layout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../context/">CephContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="../continuous-integration/">Continuous Integration Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../corpus/">资料库结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cpu-profiler/">Oprofile 的安装</a></li>
<li class="toctree-l2"><a class="reference internal" href="../crush-msr/">CRUSH MSR (Multi-step Retry)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cxx/">C++17 and libstdc++ ABI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deduplication/">去重</a></li>
<li class="toctree-l2"><a class="reference internal" href="../delayed-delete/">CephFS delayed deletion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev_cluster_deployment/">开发集群的部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev_cluster_deployment/#id5">在同一机器上部署多套开发集群</a></li>
<li class="toctree-l2"><a class="reference internal" href="../development-workflow/">开发流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../documenting/">为 Ceph 写作文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dpdk/">Ceph messenger DPDKStack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../encoding/">序列化（编码、解码）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../erasure-coded-pool/">纠删码存储池</a></li>
<li class="toctree-l2"><a class="reference internal" href="../file-striping/">File striping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../freebsd/">FreeBSD Implementation details</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generatedocs/">Ceph 文档的构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../health-reports/">Health Reports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../iana/">IANA 号</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kclient/">Testing changes to the Linux Kernel CephFS driver</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kclient/#step-one-build-the-kernel">Step One: build the kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kclient/#step-two-create-a-vm">Step Two: create a VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kclient/#step-three-networking-the-vm">Step Three: Networking the VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kubernetes/">Hacking on Ceph in Kubernetes with Rook</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Design of the libcephfs proxy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#description-of-the-problem">Description of the problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="#proposed-solution">Proposed solution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#high-level-approach">High level approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="#design-of-common-components">Design of common components</a></li>
<li class="toctree-l4"><a class="reference internal" href="#design-of-the-libcephfs-proxy-so-library">Design of the <em>libcephfs_proxy.so</em> library</a></li>
<li class="toctree-l4"><a class="reference internal" href="#design-of-the-libcephfsd-daemon">Design of the <em>libcephfsd</em> daemon</a></li>
<li class="toctree-l4"><a class="reference internal" href="#special-functions">Special functions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libs/">库体系结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../logging/">集群日志的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../logs/">调试日志</a></li>
<li class="toctree-l2"><a class="reference internal" href="../macos/">在 MacOS 上构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mempool_accounting/">What is a mempool?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mempool_accounting/#some-common-mempools-that-we-can-track">Some common mempools that we can track</a></li>
<li class="toctree-l2"><a class="reference internal" href="../messenger/">Messenger notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-bootstrap/">Monitor bootstrap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-elections/">Monitor Elections</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-on-disk-formats/">ON-DISK FORMAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-osdmap-prune/">FULL OSDMAP VERSION PRUNING</a></li>
<li class="toctree-l2"><a class="reference internal" href="../msgr2/">msgr2 协议（ msgr2.0 和 msgr2.1 ）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../network-encoding/">Network Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../network-protocol/">网络协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../object-store/">对象存储架构概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../osd-class-path/">OSD class path issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../peering/">互联</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf/">Using perf</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf_counters/">性能计数器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf_histograms/">Perf histograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../placement-group/">PG （归置组）说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quick_guide/">开发者指南（快速）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rados-client-protocol/">RADOS 客户端协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-diff/">RBD 增量备份</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-export/">RBD Export &amp; Import</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-layering/">RBD Layering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../release-checklists/">Release checklists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../release-process/">Ceph Release Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="../seastore/">SeaStore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../sepia/">Sepia 社区测试实验室</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session_authentication/">Session Authentication for the Cephx Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../testing/">测试笔记</a></li>
<li class="toctree-l2"><a class="reference internal" href="../versions/">Public OSD Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vstart-ganesha/">NFS CephFS-RGW Developer Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../wireshark/">Wireshark Dissector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../zoned-storage/">Zoned Storage Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../osd_internals/">OSD 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mds_internals/">MDS 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../radosgw/">RADOS 网关开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ceph-volume/">ceph-volume 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../crimson/">Crimson developer documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hardware-monitoring/">硬件监控</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="design-of-the-libcephfs-proxy">
<h1>Design of the libcephfs proxy<a class="headerlink" href="#design-of-the-libcephfs-proxy" title="Permalink to this heading"></a></h1>
<section id="description-of-the-problem">
<h2>Description of the problem<a class="headerlink" href="#description-of-the-problem" title="Permalink to this heading"></a></h2>
<p>When an application connects to a Ceph volume through the <em>libcephfs.so</em>
library, a cache is created locally inside the process. The <em>libcephfs.so</em>
implementation already deals with memory usage of the cache and adjusts it so
that it doesn’t consume all the available memory. However, if multiple
processes connect to CephFS through different instances of the library, each
one of them will keep a private cache. In this case memory management is not
effective because, even configuring memory limits, the number of libcephfs
instances that can be created is unbounded and they can’t work in a coordinated
way to correctly control ressource usage. Due to this, it’s relatively easy to
consume all memory when all processes are using data cache intensively. This
causes the OOM killer to terminate those processes.</p>
</section>
<section id="proposed-solution">
<h2>Proposed solution<a class="headerlink" href="#proposed-solution" title="Permalink to this heading"></a></h2>
<section id="high-level-approach">
<h3>High level approach<a class="headerlink" href="#high-level-approach" title="Permalink to this heading"></a></h3>
<p>The main idea is to create a <em>libcephfs_proxy.so</em> library that will provide the
same API as the original <em>libcephfs.so</em>, but won’t cache any data. This library
can be used by any application currently using <em>libcephfs.so</em> in a transparent
way (i.e. no code modification required) just by linking against
<em>libcephfs_proxy.so</em> instead of <em>libcephfs.so</em>, or even using <em>LD_PRELOAD</em>.</p>
<p>A new <em>libcephfsd</em> daemon will also be created. This daemon will link against
the real <em>libcephfs.so</em> library and will listen for incoming connections on a
UNIX socket.</p>
<p>When an application starts and initiates CephFS requests through the
<em>libcephfs_proxy.so</em> library, it will connect to the <em>libcephfsd</em> daemon
through the UNIX socket and it will forward all CephFS requests to it. The
daemon will use the real <em>libcephfs.so</em> to execute those requests and the
answers will be returned back to the application, potentially caching data in
the <em>libcephfsd</em> process itself. All this will happen transparently, without
any knowledge from the application.</p>
<p>The daemon will share low level <em>libcephfs.so</em> mounts between different
applications to avoid creating an instance for each application, which would
have the same effect on memory as linking each application directly to the
<em>libcephfs.so</em> library. This will be done only if the configuration defined by
the applications is identical. Otherwise new independent instances will still
be created.</p>
<p>Some <em>libcephfs.so</em> functions will need to be implemented in an special way
inside the <em>libcephfsd</em> daemon to hide the differences caused by sharing the
same mount instance with more than one client (for example chdir/getcwd cannot
rely directly on the <code class="docutils literal notranslate"><span class="pre">ceph_chdir()</span></code>/<code class="docutils literal notranslate"><span class="pre">ceph_getcwd()</span></code> of <em>libcephfs.so</em>).</p>
<p>Initially, only the subset of the low-level interface functions of
<em>libcephfs.so</em> that are used by the Samba’s VFS CephFS module will be provided.</p>
</section>
<section id="design-of-common-components">
<h3>Design of common components<a class="headerlink" href="#design-of-common-components" title="Permalink to this heading"></a></h3>
<section id="network-protocol">
<h4>Network protocol<a class="headerlink" href="#network-protocol" title="Permalink to this heading"></a></h4>
<p>Since the connection through the UNIX socket is to another process that runs on
the same machine and the data we need to pass is quite simple, we’ll avoid all
the overhead of generic XDR encoding/decoding and RPC transmissions by using a
very simple serialization implemented in the code itself. For the future we may
consider using cap’n proto (<a class="reference external" href="https://capnproto.org">https://capnproto.org</a>), which claims to have zero
overhead for encoding and decoding, and would provide an easy way to support
backward compatibility if the network protocol needs to be modified in the
future.</p>
</section>
</section>
<section id="design-of-the-libcephfs-proxy-so-library">
<h3>Design of the <em>libcephfs_proxy.so</em> library<a class="headerlink" href="#design-of-the-libcephfs-proxy-so-library" title="Permalink to this heading"></a></h3>
<p>This library will basically connect to the UNIX socket where the <em>libcephfsd</em>
daemon is listening, wait for requests coming from the application, serialize
all function arguments and send them to the daemon. Once the daemon responds it
will deserialize the answer and return the result to the application.</p>
<section id="local-caching">
<h4>Local caching<a class="headerlink" href="#local-caching" title="Permalink to this heading"></a></h4>
<p>While the main purpose of this library is to avoid independent caches on each
process, some preliminary testing has shown a big performance drop for
workloads based on metadata operations and/or small files when all requests go
through the proxy daemon. To minimize this, metadata caching should be
implemented. Metadata cache is much smaller than data cache and will provide a
good trade-off between memory usage and performance.</p>
<p>To implement caching in a safe way, it’s required to correctly invalidate data
before it becomes stale. Currently libcephfs.so provides invalidation
notifications that can be used to implement this, but its semantics are not
fully understood yet, so the cache in the libcephfs_proxy.so library will be
designed and implemented in a future version.</p>
</section>
</section>
<section id="design-of-the-libcephfsd-daemon">
<h3>Design of the <em>libcephfsd</em> daemon<a class="headerlink" href="#design-of-the-libcephfsd-daemon" title="Permalink to this heading"></a></h3>
<p>The daemon will be a regular process that will centralize libcephfs requests
coming from other processes on the same machine.</p>
<section id="process-maintenance">
<h4>Process maintenance<a class="headerlink" href="#process-maintenance" title="Permalink to this heading"></a></h4>
<p>Since the process will work as a standalone daemon, a simple systemd unit file
will be provided to manage it as a regular system service. Most probably this
will be integrated inside cephadm in the future.</p>
<p>In case the <em>libcephfsd</em> daemon crashes, we’ll rely on systemd to restart it.</p>
</section>
</section>
<section id="special-functions">
<h3>Special functions<a class="headerlink" href="#special-functions" title="Permalink to this heading"></a></h3>
<p>Some functions will need to be handled in a special way inside the <em>libcephfsd</em>
daemon to provide correct functionality since forwarding them directly to
<em>libcephfs.so</em> could return incorrect results due to the sharing of low-level
mounts.</p>
<p><strong>Sharing of underlying struct ceph_mount_info</strong></p>
<p>The main purpose of the proxy is to avoid creating a new mount for each process
when they are accessing the same data. To be able to provide this we need to
“virtualize” the mount points and let the application believe it’s using its
own mount when, in fact, it could be using a shared mount.</p>
<p>The daemon will track the Ceph account used to connect to the volume, the
configuration file and any specific configuration changes done before mounting
the volume. Only if all settings are exactly the same as another already
mounted instance, then the mount will be shared. The daemon won’t understand
CephFS settings nor any potential dependencies between settings. For this
reason, a very strict comparison will be done: the configuration file needs to
be identical and any other changes made afterwards need to be set to the exact
same value and in the same order so that two configurations can be considered
identical.</p>
<p>The check to determine whether two configurations are identical or not will be
done just before mounting the volume (i.e. <code class="docutils literal notranslate"><span class="pre">ceph_mount()</span></code>). This means that
during the configuration phase, we may have many simultaneous mounts allocated
but not yet mounted. However only one of them will become a real mount. The
others will remain unmounted and will be eventually destroyed once users
unmount and release them.</p>
<p>The following functions will be affected:</p>
<ul>
<li><p><strong>ceph_create</strong></p>
<p>This one will allocate a new ceph_mount_info structure, and the provided id
will be recorded for future comparison of potentially matching mounts.</p>
</li>
<li><p><strong>ceph_release</strong></p>
<p>This one will release an unmounted ceph_mount_info structure. Unmounted
structures won’t be shared with anyone else.</p>
</li>
<li><p><strong>ceph_conf_read_file</strong></p>
<p>This one will read the configuration file, compute a checksum and make a
copy. The copy will make sure that there are no changes in the configuration
file since the time the checksum was computed, and the checksum will be
recorded for future comparison of potentially matching mounts.</p>
</li>
<li><p><strong>ceph_conf_get</strong></p>
<p>This one will obtain the requested setting, recording it for future
comparison of potentially matching mounts.</p>
<p>Even though this may seem unnecessary, since the daemon is considering the
configuration as a black box, it could be possible to have some dynamic
setting that could return different values depending on external factors, so
the daemon also requires that any requested setting returns the same value to
consider two configurations identical.</p>
</li>
<li><p><strong>ceph_conf_set</strong></p>
<p>This one will record the modified value for future comparison of potentially
matching mounts.</p>
<p>In normal circumstances, some settings may be set even after having mounted
the volume. The proxy won’t allow that to avoid potential interferences with
other clients sharing the same mount.</p>
</li>
<li><p><strong>ceph_init</strong></p>
<p>This one will be a no-op. Calling this function triggers the allocation of
several resources and starts some threads. This is just a waste of resources
if this <em>ceph_mount_info</em> structure is not finally mounted because it matches
with an already existing mount.</p>
<p>Only if at the time of mount (i.e. <code class="docutils literal notranslate"><span class="pre">ceph_mount()</span></code>) there’s no match with
already existing mounts, then the mount will be initialized and mounted at
the same time.</p>
</li>
<li><p><strong>ceph_select_filesystem</strong></p>
<p>This one will record the selected file system for future comparison of
potentially matching mounts.</p>
</li>
<li><p><strong>ceph_mount</strong></p>
<p>This one will try to find an active mount that matches with all the
configurations defined for this <em>ceph_mount_info</em> structure. If none is
found, it will be mounted. Otherwise, the already existing mount will be
shared with this client.</p>
<p>The unmounted <em>ceph_mount_info</em> structures will be kept around associated
with the mounted one.</p>
<p>All “real” mounts will be made against the absolute root of the volume
(i.e. “/”) to make sure they can be shared with other clients later,
regardless of whether they use the same mount point or not. This means that
just after mounting, the daemon will need to resolve and store the root inode
of the “virtual” mount point.</p>
<p>The CWD (Current Working Directory) will also be initialized to the same
inode.</p>
</li>
<li><p><strong>ceph_unmount</strong></p>
<p>This one will detach the client from the mounted <em>ceph_mount_info</em> structure
and reattach it to one of the associated unmounted structures. If this was
the last user of the mount, it’s finally unmounted instead.</p>
<p>After calling this function, the client continues using a private
<em>ceph_mount_info</em> structure that is used exclusively by itself, so other
configuration changes and operations can be done safely.</p>
</li>
</ul>
<p><strong>Confine accesses to the intended mount point</strong></p>
<p>Since the effective mount point may not match the real mount point, some
functions could be able to return inodes outside of the effective mount point
if not handled with care. To avoid it and provide the result that the user
application expects, we will need to simulate some of them inside the
<em>libcephfsd</em> daemon.</p>
<p>There are three special cases to consider:</p>
<ol class="arabic simple">
<li><p>Handling of paths starting with “/”</p></li>
<li><p>Handling of paths containing “..” (i.e. parent directory)</p></li>
<li><p>Handling of paths containing symbolic links</p></li>
</ol>
<p>When these special paths are found, they need to be handled in a special way to
make sure that the returned inodes are what the client expects.</p>
<p>The following functions will be affected:</p>
<ul>
<li><p><strong>ceph_ll_lookup</strong></p>
<p>Lookup accepts “..” as the name to resolve. If the parent directory is the
root of the “virtual” mount point (which may not be the same as the real
mount point), we’ll need to return the inode corresponding to the “virtual”
mount point stored at the time of mount, instead of the real parent.</p>
</li>
<li><p><strong>ceph_ll_lookup_root</strong></p>
<p>This one needs to return the root inode stored at the time of mount.</p>
</li>
<li><p><strong>ceph_ll_walk</strong></p>
<p>This one will be completely reimplemented inside the daemon to be able to
correctly parse each path component and symbolic link, and handle “/” and
“..” in the correct way.</p>
</li>
<li><p><strong>ceph_chdir</strong></p>
<p>This one will resolve the passed path and store it along the corresponding
inode inside the current “virtual” mount. The real <code class="docutils literal notranslate"><span class="pre">ceph_chdir()</span></code> won’t be
called.</p>
</li>
<li><p><strong>ceph_getcwd</strong></p>
<p>This one will just return the path stored in the “virtual” mount from
previous <code class="docutils literal notranslate"><span class="pre">ceph_chdir()</span></code> calls.</p>
</li>
</ul>
<p><strong>Handle AT_FDCWD</strong></p>
<p>Any function that receives a file descriptor could also receive the special
<em>AT_FDCWD</em> value. These functions need to check for that value and use the
“virtual” CWD instead.</p>
</section>
</section>
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="Permalink to this heading"></a></h2>
<p>The proxy should be transparent to any application already using
<em>libcephfs.so</em>. This also applies to testing scripts and applications. So any
existing test against the regular <em>libcephfs.so</em> library can also be used to
test the proxy.</p>
</section>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../kubernetes/" class="btn btn-neutral float-left" title="Hacking on Ceph in Kubernetes with Rook" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../libs/" class="btn btn-neutral float-right" title="库体系结构" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>