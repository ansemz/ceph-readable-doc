
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>ECBackend 实现策略 &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../../../" />
    

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../">Ceph Documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="ecbackend">
<span id="ecbackend-implementation-strategy"></span><h1>ECBackend 实现策略<a class="headerlink" href="#ecbackend" title="Permalink to this headline">¶</a></h1>
<div class="section" id="misc-initial-design-notes">
<span id="id1"></span><h2>设计初稿的各种起因<a class="headerlink" href="#misc-initial-design-notes" title="Permalink to this headline">¶</a></h2>
<p>The initial (and still true for ec pools without the hacky ec
overwrites debug flag enabled) design for ec pools restricted
EC pools to operations which can be easily rolled back:</p>
<ul class="simple">
<li>CEPH_OSD_OP_APPEND: We can roll back an append locally by
including the previous object size as part of the PG log event.</li>
<li>CEPH_OSD_OP_DELETE: The possibility of rolling back a delete
requires that we retain the deleted object until all replicas have
persisted the deletion event.  ErasureCoded backend will therefore
need to store objects with the version at which they were created
included in the key provided to the filestore.  Old versions of an
object can be pruned when all replicas have committed up to the log
event deleting the object.</li>
<li>CEPH_OSD_OP_(SET|RM)ATTR: If we include the prior value of the attr
to be set or removed, we can roll back these operations locally.</li>
</ul>
<p>Log entries contain a structure explaining how to locally undo the
operation represented by the operation
(see osd_types.h:TransactionInfo::LocalRollBack).</p>
<div class="section" id="pgtemp-and-crush">
<h3>PGTemp and Crush<a class="headerlink" href="#pgtemp-and-crush" title="Permalink to this headline">¶</a></h3>
<p>Primaries are able to request a temp acting set mapping in order to
allow an up-to-date OSD to serve requests while a new primary is
backfilled (and for other reasons).  An erasure coded pg needs to be
able to designate a primary for these reasons without putting it in
the first position of the acting set.  It also needs to be able to
leave holes in the requested acting set.</p>
<p>Core Changes:</p>
<ul class="simple">
<li>OSDMap::pg_to_*_osds needs to separately return a primary.  For most
cases, this can continue to be acting[0].</li>
<li>MOSDPGTemp (and related OSD structures) needs to be able to specify
a primary as well as an acting set.</li>
<li>Much of the existing code base assumes that acting[0] is the primary
and that all elements of acting are valid.  This needs to be cleaned
up since the acting set may contain holes.</li>
</ul>
</div>
<div class="section" id="distinguished-acting-set-positions">
<h3>Distinguished acting set positions<a class="headerlink" href="#distinguished-acting-set-positions" title="Permalink to this headline">¶</a></h3>
<p>With the replicated strategy, all replicas of a PG are
interchangeable.  With erasure coding, different positions in the
acting set have different pieces of the erasure coding scheme and are
not interchangeable.  Worse, crush might cause chunk 2 to be written
to an OSD which happens already to contain an (old) copy of chunk 4.
This means that the OSD and PG messages need to work in terms of a
type like pair&lt;shard_t, pg_t&gt; in order to distinguish different pg
chunks on a single OSD.</p>
<p>Because the mapping of object name to object in the filestore must
be 1-to-1, we must ensure that the objects in chunk 2 and the objects
in chunk 4 have different names.  To that end, the objectstore must
include the chunk id in the object key.</p>
<p>Core changes:</p>
<ul class="simple">
<li>The objectstore <a class="reference external" href="https://github.com/ceph/ceph/blob/firefly/src/common/hobject.h#L241">ghobject_t needs to also include a chunk id</a> making it more like
tuple&lt;hobject_t, gen_t, shard_t&gt;.</li>
<li>coll_t needs to include a shard_t.</li>
<li>The OSD pg_map and similar pg mappings need to work in terms of a
spg_t (essentially
pair&lt;pg_t, shard_t&gt;).  Similarly, pg-&gt;pg messages need to include
a shard_t</li>
<li>For client-&gt;PG messages, the OSD will need a way to know which PG
chunk should get the message since the OSD may contain both a
primary and non-primary chunk for the same pg</li>
</ul>
</div>
<div class="section" id="object-classes">
<h3>Object Classes<a class="headerlink" href="#object-classes" title="Permalink to this headline">¶</a></h3>
<p>Reads from object classes will return ENOTSUP on ec pools by invoking
a special SYNC read.</p>
</div>
<div class="section" id="scrub">
<h3>Scrub<a class="headerlink" href="#scrub" title="Permalink to this headline">¶</a></h3>
<p>The main catch, however, for ec pools is that sending a crc32 of the
stored chunk on a replica isn&#8217;t particularly helpful since the chunks
on different replicas presumably store different data.  Because we
don&#8217;t support overwrites except via DELETE, however, we have the
option of maintaining a crc32 on each chunk through each append.
Thus, each replica instead simply computes a crc32 of its own stored
chunk and compares it with the locally stored checksum.  The replica
then reports to the primary whether the checksums match.</p>
<p>With overwrites, all scrubs are disabled for now until we work out
what to do (see doc/dev/osd_internals/erasure_coding/proposals.rst).</p>
</div>
<div class="section" id="crush">
<h3>Crush<a class="headerlink" href="#crush" title="Permalink to this headline">¶</a></h3>
<p>If crush is unable to generate a replacement for a down member of an
acting set, the acting set should have a hole at that position rather
than shifting the other elements of the acting set out of position.</p>
</div>
</div>
</div>
<div class="section" id="id2">
<h1>ECBackend<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h1>
<div class="section" id="main-operation-overview">
<h2>MAIN OPERATION OVERVIEW<a class="headerlink" href="#main-operation-overview" title="Permalink to this headline">¶</a></h2>
<p>A RADOS put operation can span
multiple stripes of a single object. There must be code that
tessellates the application level write into a set of per-stripe write
operations &#8211; some whole-stripes and up to two partial
stripes. Without loss of generality, for the remainder of this
document we will focus exclusively on writing a single stripe (whole
or partial). We will use the symbol &#8220;W&#8221; to represent the number of
blocks within a stripe that are being written, i.e., W &lt;= K.</p>
<p>There are three data flows for handling a write into an EC stripe. The
choice of which of the three data flows to choose is based on the size
of the write operation and the arithmetic properties of the selected
parity-generation algorithm.</p>
<ol class="arabic simple">
<li>whole stripe is written/overwritten</li>
<li>a read-modify-write operation is performed.</li>
</ol>
<div class="section" id="whole-stripe-write">
<h3>WHOLE STRIPE WRITE<a class="headerlink" href="#whole-stripe-write" title="Permalink to this headline">¶</a></h3>
<p>This is the simple case, and is already performed in the existing code
(for appends, that is). The primary receives all of the data for the
stripe in the RADOS request, computes the appropriate parity blocks
and send the data and parity blocks to their destination shards which
write them. This is essentially the current EC code.</p>
</div>
<div class="section" id="read-modify-write">
<h3>READ-MODIFY-WRITE<a class="headerlink" href="#read-modify-write" title="Permalink to this headline">¶</a></h3>
<p>The primary determines which of the K-W blocks are to be unmodified,
and reads them from the shards. Once all of the data is received it is
combined with the received new data and new parity blocks are
computed. The modified blocks are sent to their respective shards and
written. The RADOS operation is acknowledged.</p>
</div>
<div class="section" id="osd-object-write-and-consistency">
<h3>OSD Object Write and Consistency<a class="headerlink" href="#osd-object-write-and-consistency" title="Permalink to this headline">¶</a></h3>
<p>Regardless of the algorithm chosen above, writing of the data is a two
phase process: commit and rollforward. The primary sends the log
entries with the operation described (see
osd_types.h:TransactionInfo::(LocalRollForward|LocalRollBack).
In all cases, the &#8220;commit&#8221; is performed in place, possibly leaving some
information required for a rollback in a write-aside object.  The
rollforward phase occurs once all acting set replicas have committed
the commit (sorry, overloaded term) and removes the rollback information.</p>
<p>In the case of overwrites of exsting stripes, the rollback information
has the form of a sparse object containing the old values of the
overwritten extents populated using clone_range.  This is essentially
a place-holder implementation, in real life, bluestore will have an
efficient primitive for this.</p>
<p>The rollforward part can be delayed since we report the operation as
committed once all replicas have committed.  Currently, whenever we
send a write, we also indicate that all previously committed
operations should be rolled forward (see
ECBackend::try_reads_to_commit).  If there aren&#8217;t any in the pipeline
when we arrive at the waiting_rollforward queue, we start a dummy
write to move things along (see the Pipeline section later on and
ECBackend::try_finish_rmw).</p>
</div>
<div class="section" id="extentcache">
<h3>ExtentCache<a class="headerlink" href="#extentcache" title="Permalink to this headline">¶</a></h3>
<p>It&#8217;s pretty important to be able to pipeline writes on the same
object.  For this reason, there is a cache of extents written by
cacheable operations.  Each extent remains pinned until the operations
referring to it are committed.  The pipeline prevents rmw operations
from running until uncacheable transactions (clones, etc) are flushed
from the pipeline.</p>
<p>See ExtentCache.h for a detailed explanation of how the cache
states correspond to the higher level invariants about the conditions
under which cuncurrent operations can refer to the same object.</p>
</div>
<div class="section" id="pipeline">
<h3>Pipeline<a class="headerlink" href="#pipeline" title="Permalink to this headline">¶</a></h3>
<p>Reading src/osd/ExtentCache.h should have given a good idea of how
operations might overlap.  There are several states involved in
processing a write operation and an important invariant which
isn&#8217;t enforced by PrimaryLogPG at a higher level which need to be
managed by ECBackend.  The important invariant is that we can&#8217;t
have uncacheable and rmw operations running at the same time
on the same object.  For simplicity, we simply enforce that any
operation which contains an rmw operation must wait until
all in-progress uncacheable operations complete.</p>
<p>There are improvements to be made here in the future.</p>
<p>For more details, see ECBackend::waiting_* and
ECBackend::try_&lt;from&gt;_to_&lt;to&gt;.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../../../">
              <img class="logo" src="../../../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../../../">Table Of Contents</a></h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../start/">安装（快速）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/">安装（手动）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rbd/rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../">开发文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../releases/">Ceph 版本</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../README/">中文版翻译说明</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../translation-convention/">中文版词语翻译惯例</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../">Ceph Documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2016, Red Hat, Inc, and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>