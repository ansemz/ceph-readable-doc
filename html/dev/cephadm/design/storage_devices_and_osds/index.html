

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Storage Devices and OSDs Management Workflows &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/underscore.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../../genindex/" />
    <link rel="search" title="Search" href="../../../../search/" />
    <link rel="next" title="Notes and Thoughts on Cephadm’s scalability" href="../../scalability-notes/" />
    <link rel="prev" title="Compliance Check" href="../../compliance-check/" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../../cephadm/">Cephadm</a> &raquo;</li>
          <li><a href="../../">CEPHADM 开发者文档</a> &raquo;</li>
      <li>Storage Devices and OSDs Management Workflows</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/dev/cephadm/design/storage_devices_and_osds.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../../../">
          

          
            
            <img src="../../../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../architecture/">体系结构</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../developer_guide/">开发者指南</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/intro/">简介</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/essentials/">必备知识</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/merging/">何时、合并了什么</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/issue-tracker/">问题追踪器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/basic-workflow/">基本工作流</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/tests-unit-tests/">测试：单元测试</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/testing_integration_tests/">测试：集成测试(Teuthology)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/running-tests-locally/">测试：在本地运行测试</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/tests-windows/">测试: Windows</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/dash-devel/">Ceph Dashboard 开发者文档 (之前是 HACKING.rst)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/jaegertracing/">Tracing 开发者文档</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../">Cephadm 开发者文档</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../developing-cephadm/">Developing with cephadm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../host-maintenance/">Host Maintenance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../compliance-check/">Compliance Check</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">存储设备和 OSD 管理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#retrieve-device-information-inventory">1. Retrieve device information. Inventory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#add-osds">2. Add OSDs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#remove-osds">3. Remove OSDs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#replace-osds">4. Replace OSDs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../scalability-notes/">Notes and Thoughts on Cephadm’s scalability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../developer_guide/debugging-gdb/">用 GDB 调试</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hardware-monitoring/">Hardware monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="storage-devices-and-osds-management-workflows">
<h1>Storage Devices and OSDs Management Workflows<a class="headerlink" href="#storage-devices-and-osds-management-workflows" title="Permalink to this heading"></a></h1>
<p>The cluster storage devices are the physical storage devices installed in each of the cluster’s hosts. We need to execute different operations over them and also to retrieve information about physical features and working behavior.
The basic use cases we have in this area are:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#retrieve-device-information-inventory">1. Retrieve device information. Inventory</a></p></li>
<li><p><a class="reference internal" href="#add-osds">2. Add OSDs</a></p></li>
<li><p><a class="reference internal" href="#remove-osds">3. Remove OSDs</a></p></li>
<li><p><a class="reference internal" href="#replace-osds">4. Replace OSDs</a></p></li>
</ul>
<section id="retrieve-device-information-inventory">
<h2>1. Retrieve device information. Inventory<a class="headerlink" href="#retrieve-device-information-inventory" title="Permalink to this heading"></a></h2>
<p>We must be able to review what is the current state and condition of the cluster storage devices. We need the identification and features detail (including ident/fault led on/off capable) and if the device is used or not as an OSD/DB/WAL device.</p>
<p>The information required for each device will be at least:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Hostname</span>   <span class="n">Path</span>      <span class="n">Type</span>  <span class="n">Serial</span>    <span class="n">Size</span>   <span class="n">Health</span>  <span class="n">Ident</span>  <span class="n">Fault</span>  <span class="n">Available</span>
</pre></div>
</div>
<p>In order to know what is the current condition of the device, we need to know what is the amount of storage used, the percentage of free space, the average number of IOPS and the fault led state.
This information should be provided probably by the Ceph orchestrator which is the component where we have access to this kind of information.</p>
<p>Another important question around retrieving device information is “efficiency”. The information about devices can be critical in components like the Orchestrator or the Dashboard, because this information usually is used to take decisions.
When we talk about efficiency we need to be sure that all the points are covered:</p>
<ol class="arabic simple">
<li><p>Get the complete information for each device in the most fast way.</p></li>
<li><p>All the information about all the devices in one host is accessible always immediately.</p></li>
<li><p>The information is constantly updated in each host. A device failure or the addition of a new device must be detected in the smallest possible timeframe</p></li>
<li><p>Scalability. To work with thousands of devices in hundreds of hosts shouldn’t be a problem.</p></li>
</ol>
<section id="a-current-workflow">
<h3>A. Current workflow:<a class="headerlink" href="#a-current-workflow" title="Permalink to this heading"></a></h3>
<dl class="simple">
<dt><strong>CLI</strong>:</dt><dd><p>Operations:</p>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "# ";
}
</style><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>device<span class="w"> </span>ls</span>
<span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>device<span class="w"> </span>ls<span class="w"> </span>json<span class="w"> </span><span class="o">(</span><span class="w"> </span>to<span class="w"> </span>get<span class="w"> </span>all<span class="w"> </span>the<span class="w"> </span>fields<span class="w"> </span><span class="k">for</span><span class="w"> </span>each<span class="w"> </span>device<span class="w"> </span><span class="o">)</span></span>
<span class="prompt1"></span>
<span class="prompt1"><span class="w">  </span>Problems<span class="w"> </span><span class="k">in</span><span class="w"> </span>current<span class="w"> </span>implementation:</span>
<span class="prompt1"><span class="w">      </span>*<span class="w"> </span>Does<span class="w"> </span>not<span class="w"> </span>scale.</span>
</pre></div></div><dl class="simple">
<dt><strong>GUI</strong>:</dt><dd><dl class="simple">
<dt>Operations:</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>cluster.Inventory section:</dt><dd><p>The cluster.Inventory section presents a basic list of the devices in the cluster. It is a fixed list with only a few fields. Only the “ident light on” operation is possible although we do not know if it is possible or not until the operation is launched.</p>
</dd>
</dl>
</li>
</ul>
</dd>
<dt>Problems in current implementation:</dt><dd><ul class="simple">
<li><p>Does not scale (depends of the orchestrator)</p></li>
<li><p>Rigid user experience</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</section>
<section id="b-proposed-workflow">
<h3>B. Proposed workflow:<a class="headerlink" href="#b-proposed-workflow" title="Permalink to this heading"></a></h3>
<p><strong>CLI</strong>:
The current API is good enough, we only need to be sure that we have:</p>
<blockquote>
<div><ul class="simple">
<li><p>all the attribute/health/operative state fields from each device</p></li>
<li><p>fast response</p></li>
<li><p>scalable</p></li>
</ul>
</div></blockquote>
<p><strong>GUI</strong>:
The inventory should be able to be customized in order to show the desired fields of information for each device. Being customizable also the position of each field(column) and the sort order.</p>
<p>The inventory should be filtered using any of the fields present in the list of devices.</p>
<p>A customized inventory list together with the filter and sort order should be able to be stored for easy utilization. In this way we can provide a set of interesting predefined inventory lists. For example:</p>
<blockquote>
<div><ul class="simple">
<li><p>Devices available</p></li>
<li><p>Devices more used (more average iops) (should be an alert/trigger)</p></li>
<li><p>Devices biggers than n Gb</p></li>
</ul>
</div></blockquote>
<p>The inventory should also provide a way to do directly operations over physical devices:</p>
<blockquote>
<div><ul class="simple">
<li><p>Identify:  Start/stop to blink the identification light</p></li>
<li><p>Create OSD: Create an OSD using this disk device if it is available.</p></li>
<li><p>Remove OSD: Delete the OSD using this disk device.</p></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="add-osds">
<h2>2. Add OSDs<a class="headerlink" href="#add-osds" title="Permalink to this heading"></a></h2>
<section id="id1">
<h3>A. Current workflow<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h3>
<p><strong>CLI</strong>:
We can specify specific devices or use a “drive group” specification to create OSD’s in different hosts. By default, the definition of the OSDs to create is “declarative“ unless you use the unmanaged parameter.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>daemon<span class="w"> </span>add<span class="w"> </span>osd<span class="w"> </span>&lt;host&gt;:device1,device2<span class="w"> </span><span class="o">[</span>--unmanaged<span class="o">=</span>true<span class="o">]</span><span class="w">  </span><span class="o">(</span>manual<span class="w"> </span>approach<span class="o">)</span></span>
<span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>osd<span class="w"> </span>-i<span class="w"> </span>&lt;json_file/yaml_file&gt;<span class="w"> </span><span class="o">[</span>--dry-run<span class="o">]</span><span class="w"> </span><span class="o">[</span>--unmanaged<span class="o">=</span>true<span class="o">]</span>*<span class="w">  </span><span class="o">(</span>Service<span class="w"> </span>Spec<span class="w"> </span>based<span class="w"> </span>approach<span class="o">)</span></span>
</pre></div></div><p><strong>GUI</strong>:
Implemented in the dashboard section “cluster.OSDs”.
There is a button to create the OSDs, that presents a page dialog box to select the physical devices that are going to be part of the OSDs as main, WAL or Db devices.
It Is very difficult to make a selection ( or to understand how to make the selection). This is even worse if your cluster has the same kind of devices, resulting in the weird thing that is not possible to create an OSD using only one storage device (because you cannot select it)
The problem here is the UI has been designed to work with “drive groups” and not to work for the user. The “drive group” is an abstract concept that must be used only in the background. Users must not be aware of this concept.</p>
</section>
<section id="id2">
<h3>B. Proposed workflow<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h3>
<p><strong>CLI and GUI</strong></p>
<p>The utilization of “declarative” drive groups makes it very difficult to understand how to configure OSD’s and the implications. Also make difficult the implementation because the multiple possibilities and the big amount of different conditions that we can find in a production system makes very complex the right evaluation and use of a declarative description of the storage devices used.
This results in unexpected situations. For example:
* A disk cleaned can be reused automatically and without any warning for creating a new OSD.
* New installed disks are used automatically for OSDs (without any warning)
* Errors trying to recreate OSD’s in disk removed from the system.</p>
<p>So there is an important thing to consider in order to simplify everything for the user and for the implementation:
<strong>Avoid the “declarative” use of the drive groups</strong></p>
<p><strong>GUI</strong>:</p>
<p>The user should be able to define the set of physical disk devices that are going to be used to support OSD’s.
This means to make simple things like create one OSD in a certain device, and also to define in an easy way how to create multiple OSD’s across multiple devices in different hosts.</p>
<p>We should take into account different premises:</p>
<p>We use only bluestore OSD’s, this means that in order to create an OSD we can decide between different strategies: consume only a single device for the OSD, use an additional device for the WAL, and/or use another different device for the DB.
To split the different bluestore OSD data components between different devices only makes sense if the WAL/DB devices are faster than the main storage device.
And the split of devices are always inside the same host, although the configuration will be applicable to other hosts with the same storage devices schema.</p>
<p>A massive creation of OSDs in a production system can result in real disaster because rebalancing can affect negatively to the normal system performance.
The same massive OSD creation in a cluster that is being installed for the first time probably is the desired behaviour.
So we should provide a mechanism to allow the user to select in which way the OSDs are going to be created. It seems that we have two possibilities:
* Fast creation - fast but harmful for performance -
Create the OSDs directly
* Conservative creation - Slow but respectful with performance -
Create all the OSDs with 0 weight. Once all OSd’ are installed, start to assign the right weight to each OSD one by one.</p>
<p>With all these premises into account it is proposed the following interface with two different modes:</p>
<p><strong>Device mode</strong>:</p>
<p>An inventory list with all the available devices and filter/listing capability is presented to the user, the user can “play” with this list obtaining a set of preferred cluster physical storage devices.</p>
<p>The user can select from the “preferred devices list” ,one, several or all the devices. These selected devices will be the ones used to create OSD’s (1 per physical device.).</p>
<p>OSD ids coming from previously deleted OSDs can be available. The user should indicate if these ids must be used or not in the new OSDs.</p>
<p>The user interface proposed could be like:</p>
<img alt="../../../../_images/OSD_Creation_device_mode.svg" class="align-center" src="../../../../_images/OSD_Creation_device_mode.svg" /><p><strong>Host mode</strong>:</p>
<p>Is basically an OSD configuration using the storage devices in a host. This configuration will be used as a base pattern to apply the same schema in other hosts.</p>
<p>The user must select a base host.
Once the host is selected, we should provide three lists (or ways to select) of available devices in the host:
* “slow devices” with the “hdd” devices
* “fast devices (WAL)” with the “sdd/nvme” devices that can be used for Bluestore WAL data
* “fast devices(DB)” with the “sdd/nvme” devices that can be used for Bluestore DB data</p>
<p>The user, using filters over the list of “slow devices” should select one,several,or all the devices in the list for OSD creation.
If the user wants to split Bluestore data components in several devices, the same operation will be needed to be performed in the other two “fast devices” lists.</p>
<p>OSD ids coming from previously deleted OSDs can be available. The user should indicate if these ids must be used or not in the new OSDs</p>
<p>Once the devices are selected we can provide a “preview “ of the OSD’s that are going to be created. (the fast devices potentially will store the WAL/DB for several OSDs).</p>
<p>OSD creation should have the inventory, and analyse to determine whether the OSD creation can be hybrid, dedicated - present those as options to the user (they never see a device group!) - then they click create.</p>
<p>When the user is happy with the OSD configuration in the host, we should provide a way to present a list of hosts where it is possible to apply the same OSD configuration. The user will select from this list the hosts where he wants to create the OSD’s.</p>
<p>A preview/summary of the creation of OSD’s in all the hosts must be provided, and if the user wants this configuration, then it will be applied, resulting in a bulk OSD creation in multiple hosts.</p>
<p>Information about the progress of OSD creation in all the hosts should be provided.</p>
<img alt="../../../../_images/OSD_Creation_host_mode.svg" class="align-center" src="../../../../_images/OSD_Creation_host_mode.svg" /></section>
<section id="key-points-to-consider">
<h3>Key points to consider:<a class="headerlink" href="#key-points-to-consider" title="Permalink to this heading"></a></h3>
<p><strong>1. Context is everything</strong>:
The current OSD creation flow doesn’t provide any indications of available devices or hosts. This leaves the user clicking on the add button and seeing nothing, if there are no devices available - at which point the user assumes there are no available devices. Both host-mode and device-mode UI flows illustrate a couple of usability features that should be implemented as a bare minimum.</p>
<blockquote>
<div><ol class="loweralpha">
<li><p>If there are no devices available, the add button should be disabled</p></li>
<li><p>The UI for OSD creation should include a summary of discovered hosts with disks and the total number of available disks that could be used for OSD creation. This should also show total raw. E.g. 5 hosts, 50 HDDs (80TB), 10 NVME (5TB)</p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>The discovered configuration could also</dt><dd><ul>
<li><p>Use the hosts rack ID annotations to look at the capacity from a fault domain perspective to ensure it’s balanced - and warn if not.</p></li>
<li><p>Confirm whether the host configurations are identical (homogeneous). Heterogeneous configurations could therefore be accompanied by a INFO/WARN message in the UI to highlight the potential balance issues of heterogeneous clusters.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</li>
<li><dl class="simple">
<dt>Once the deployment decision is made, display a summary of the selection, that the user CONFIRMs</dt><dd><ul class="simple">
<li><p>Total devices by type that would be used</p></li>
<li><p>Total number of OSDs that would be created</p></li>
<li><p>Overall raw capacity of the creation request, together with the potential raw cluster capacity once the OSD addition is complete</p></li>
<li><p>Use a rule-of-thumb to determine approximate deployment time - set an expectation.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</div></blockquote>
<p><strong>2. Enabling new capacity</strong>:
Policy option for how new disks are added to the cluster (this is present in both host-mode and device-mode designs)
- Phased by OSD: All OSDs added are at weight 0. The orchestrator then reweights each OSD in turn to drive the rebalance
- Phased by host: all OSDs on a given host are reweighted at the same time
- Immediate: don’t use reweight. Bring the OSDs up/in straight away (on an empty cluster, this should be the default)</p>
<p><strong>3. UI redesign</strong>:
Discover the devices, suggest a layout based on these devices combined with best practice, inform if there are is not enough flash for the number of HDDs, inform if there are no free devices, and also provide the advanced use case which is what we see today (which echoes the drive group approach)</p>
<p><strong>4. Imperative not Declarative</strong>:
The use of declarative “drive groups” is a problem in several aspects:</p>
<p>For the final user:</p>
<p>The “admin persona” who is going to install a cluster by first time knows what is the current hardware composition and will create the OSD’s possibly using all the storage devices in the hosts of the cluster planified to harbor OSDs.</p>
<p>But we are not telling the “admin persona” that this initial decision will be inmutable in the future and applied automatically without any warning.</p>
<p>This will result in several undesired situations:</p>
<blockquote>
<div><p>1. Storage devices with OSD’s cannot be used  for other purposes. Because they are reinstalled as OSD’s as soon as they are cleaned. Seems difficult to explain that if you do not want that, you need to add the device to a black list,  or create the OSD using the “unmanaged” parameter. (not provided in the UI)
Another horrible situation can be: you buy a new device for one of your hosts in order to store the minecraft server. You have bad luck and this device is more or less the same as the ones you used for OSD’s … then you won’t install your minecraft server because the device is automatically used for OSDs.
Another stressful situation… your lab team installed 10 new disks in your cluster, and they decided to do that just where you have more traffic in the cluster network. Rebalance of data will cause a funny situation for the “admin persona”.
This is a good example about  how we can make the users life more difficult managing OSDs</p>
<ol class="arabic simple" start="2">
<li><p>Probably after a couple of years the requirements will grow. New different storage devices will be added. And the “admin persona” will need to specify that these devices will harbor OSDs. Then we have to store the initial “drive group” used to create the initial OSDs, and also the new “drive group” definition for the new devices. So now we have more than one “drive group”, so this implies two possibilities, add a “drive groups” management tool, or merge “the two definitions” in only one!</p></li>
</ol>
<p>In any case this is a good example about how we can make the users and developers life more difficult.</p>
</div></blockquote>
<p>All these things can be avoided using imperative drive groups, we are going to provide the same functionality but without all the undesired collateral effects.
From the development point of view , this will also simplify things, so it seems a very good idea to move from “declarative” drive groups to “imperative” drive groups.</p>
</section>
</section>
<section id="remove-osds">
<h2>3. Remove OSDs<a class="headerlink" href="#remove-osds" title="Permalink to this heading"></a></h2>
<section id="id3">
<h3>A. Current workflow<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h3>
<dl class="simple">
<dt><strong>CLI</strong>:</dt><dd><ul class="simple">
<li><p>We can launch the command to delete a OSD (one by one)</p></li>
</ul>
</dd>
</dl>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>osd<span class="w"> </span>rm<span class="w"> </span>&lt;svc_id<span class="o">(</span>s<span class="o">)</span>&gt;<span class="w"> </span><span class="o">[</span>--replace<span class="o">]</span><span class="w"> </span><span class="o">[</span>--force<span class="o">]</span></span>
<span class="prompt1"></span>
<span class="prompt1"><span class="w">  </span>*<span class="w"> </span>We<span class="w"> </span>can<span class="w"> </span>verify<span class="w"> </span>what<span class="w"> </span>is<span class="w"> </span>the<span class="w"> </span>status<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>delete<span class="w"> </span>operation</span>
</pre></div></div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>osd<span class="w"> </span>rm<span class="w"> </span>status</span>
<span class="prompt1"></span>
<span class="prompt1"><span class="w">  </span>*<span class="w"> </span>Finally<span class="w"> </span>we<span class="w"> </span>can<span class="w"> </span>“clean”<span class="w"> </span>completely<span class="w"> </span>the<span class="w"> </span>device<span class="w"> </span>used<span class="w"> </span><span class="k">in</span><span class="w"> </span>the<span class="w"> </span>OSD</span>
</pre></div></div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>device<span class="w"> </span>zap<span class="w"> </span>my_hostname<span class="w"> </span>/dev/sdx</span>
</pre></div></div><p><strong>GUI</strong>:</p>
<p>In the cluster OSD section we have a button to execute different primitive operations over the OSD’s selected. One of these primitives is delete.</p>
<p>When the “delete” primitive is selected and the action button is pressed, a dialog box to confirm the operation and a check box to ask about preserving the “osd id” is shown. After accepting nothing seems to happen….</p>
<p>No way to know what is the progress of the delete operation.</p>
<p>We tend to show all the primitives for osd management in the UI - question is, does that make the environment more complex? Should the UI focus on the key workflows of osd management to cover 90% of the work quickly and easily, and leave the 10% to the CLI?</p>
</section>
<section id="id4">
<h3>B. Proposed workflow<a class="headerlink" href="#id4" title="Permalink to this heading"></a></h3>
<p><strong>CLI</strong>:</p>
<blockquote>
<div><ul class="simple">
<li><p>Need a way to know in advance how much time is going to be needed to delete an OSD (if we rebalance data)</p></li>
<li><p>The current set of command can satisfy main requirements</p></li>
</ul>
</div></blockquote>
<p><strong>GUI</strong>:</p>
<p>The user should select the OSD (or set of OSDs) to remove from a list with filtering capabilities.</p>
<p>The OSD removal should provide an option to preserve the OSD id for use when creating new OSD’s. An assessment about the time that is going to take the operation is another important  element to decide how to do the operation and when is the best moment.</p>
<p>When the user decides to execute the removal operation, the system should follow a safe procedure, with a certain degree of intelligence.</p>
<p>Depending of the OSD state (in(out, up/down) and the situation ( we are in a low/high cpu/network utilization time interval), probably we will need to do different things.</p>
<ul class="simple">
<li><p>Direct removal of the OSD:</p></li>
</ul>
<p>we are going to execute the OSD deletion operations without any wait.</p>
<ul class="simple">
<li><p>Safe OSD Removal:</p></li>
</ul>
<p>We want to remove the OSD in the most safe way. This means wait until we know that the OSD is not storing information. The user must receive a notification when it will be safe to remove the OSD</p>
<ul class="simple">
<li><p>Scheduled OSD removal:</p></li>
</ul>
<p>We want to execute the removal in the future. Besides that,  it is probable that we only will want to execute the removal if the system utilization is below certain limit</p>
</section>
</section>
<section id="replace-osds">
<h2>4. Replace OSDs<a class="headerlink" href="#replace-osds" title="Permalink to this heading"></a></h2>
<section id="id5">
<h3>A. Current workflow<a class="headerlink" href="#id5" title="Permalink to this heading"></a></h3>
<p>Is the same workflow used for removing OSDs, but we just need to use the “replace” parameter in order to preserve the OSD id for future use when we are deleting.
In the GUI the replace parameter appears as a checkbox.</p>
</section>
<section id="id6">
<h3>B. Proposed workflow<a class="headerlink" href="#id6" title="Permalink to this heading"></a></h3>
<p>Follow the directives we have in the proposed workflow for OSD removal</p>
</section>
</section>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../../compliance-check/" class="btn btn-neutral float-left" title="Compliance Check" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../scalability-notes/" class="btn btn-neutral float-right" title="Notes and Thoughts on Cephadm’s scalability" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>