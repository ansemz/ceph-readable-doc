

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>PoseidonStore &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" />
    <link rel="next" title="项目管理" href="../../../governance/" />
    <link rel="prev" title="BackfillMachine" href="../backfillmachine/" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../internals/">Ceph 内幕</a></li>
          <li class="breadcrumb-item"><a href="../">Crimson developer documentation</a></li>
      <li class="breadcrumb-item active">PoseidonStore</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/dev/crimson/poseidonstore.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../../" class="icon icon-home"> Ceph
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer_guide/">开发者指南</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../internals/">Ceph 内幕</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../balancer-design/">Ceph 如何均衡（读写、容量）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../blkin/">Tracing Ceph With LTTng</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../blkin/#tracing-ceph-with-blkin">Tracing Ceph With Blkin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../bluestore/">BlueStore Internals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ceph_krb_auth/">如何配置好 Ceph Kerberos 认证的详细文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cephfs-mirroring/">CephFS Mirroring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cephfs-reclaim/">CephFS Reclaim Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cephfs-snapshots/">CephFS 快照</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cephx/">Cephx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cephx_protocol/">Cephx 认证协议详细阐述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../config/">配置管理系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../config-key/">config-key layout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../context/">CephContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../continuous-integration/">Continuous Integration Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../corpus/">资料库结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cpu-profiler/">Oprofile 的安装</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../crush-msr/">CRUSH MSR (Multi-step Retry)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cxx/">C++17 and libstdc++ ABI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deduplication/">去重</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../delayed-delete/">CephFS delayed deletion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev_cluster_deployment/">开发集群的部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dev_cluster_deployment/#id5">在同一机器上部署多套开发集群</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../development-workflow/">开发流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../documenting/">为 Ceph 写作文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../dpdk/">Ceph messenger DPDKStack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../encoding/">序列化（编码、解码）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../erasure-coded-pool/">纠删码存储池</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../file-striping/">File striping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../freebsd/">FreeBSD Implementation details</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../generatedocs/">Ceph 文档的构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../health-reports/">Health Reports</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../iana/">IANA 号</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kclient/">Testing changes to the Linux Kernel CephFS driver</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kclient/#step-one-build-the-kernel">Step One: build the kernel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kclient/#step-two-create-a-vm">Step Two: create a VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kclient/#step-three-networking-the-vm">Step Three: Networking the VM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../kubernetes/">Hacking on Ceph in Kubernetes with Rook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libcephfs_proxy/">Design of the libcephfs proxy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libs/">库体系结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../logging/">集群日志的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../logs/">调试日志</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../macos/">在 MacOS 上构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mempool_accounting/">What is a mempool?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mempool_accounting/#some-common-mempools-that-we-can-track">Some common mempools that we can track</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../messenger/">Messenger notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mon-bootstrap/">Monitor bootstrap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mon-elections/">Monitor Elections</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mon-on-disk-formats/">ON-DISK FORMAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mon-osdmap-prune/">FULL OSDMAP VERSION PRUNING</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../msgr2/">msgr2 协议（ msgr2.0 和 msgr2.1 ）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../network-encoding/">Network Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../network-protocol/">网络协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../object-store/">对象存储架构概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../osd-class-path/">OSD class path issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../peering/">互联</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../perf/">Using perf</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../perf_counters/">性能计数器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../perf_histograms/">Perf histograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../placement-group/">PG （归置组）说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../quick_guide/">开发者指南（快速）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rados-client-protocol/">RADOS 客户端协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rbd-diff/">RBD 增量备份</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rbd-export/">RBD Export &amp; Import</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rbd-layering/">RBD Layering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../release-checklists/">Release checklists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../release-process/">Ceph Release Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../seastore/">SeaStore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sepia/">Sepia 社区测试实验室</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../session_authentication/">Session Authentication for the Cephx Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../testing/">测试笔记</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../versions/">Public OSD Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../vstart-ganesha/">NFS CephFS-RGW Developer Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../wireshark/">Wireshark Dissector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../zoned-storage/">Zoned Storage Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../osd_internals/">OSD 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../mds_internals/">MDS 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../radosgw/">RADOS 网关开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ceph-volume/">ceph-volume 开发者文档</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../">Crimson developer documentation</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../crimson/">Crimson</a></li>
<li class="toctree-l3"><a class="reference internal" href="../osd/">OSDState</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pipeline/">The ClientRequest Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="../error-handling/">Error Handling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../backfillmachine/">BackfillMachine</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">PoseidonStore</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#key-concepts-and-goals">Key concepts and goals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#design">Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="#detailed-design">Detailed Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="#plans">Plans</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hardware-monitoring/">硬件监控</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="poseidonstore">
<h1>PoseidonStore<a class="headerlink" href="#poseidonstore" title="Permalink to this heading"></a></h1>
<section id="key-concepts-and-goals">
<h2>Key concepts and goals<a class="headerlink" href="#key-concepts-and-goals" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>As one of the pluggable backend stores for Crimson, PoseidonStore targets only
high-end NVMe SSDs (not concerned with ZNS devices).</p></li>
<li><p>Designed entirely for low CPU consumption</p>
<ul>
<li><p>Hybrid update strategies for different data types (in-place, out-of-place) to
minimize CPU consumption by reducing host-side GC.</p></li>
<li><p>Remove a black-box component like RocksDB and a file abstraction layer in BlueStore
to avoid unnecessary overheads (e.g., data copy and serialization/deserialization)</p></li>
<li><p>Utilize NVMe feature (atomic large write command, Atomic Write Unit Normal).
Make use of io_uring, new kernel asynchronous I/O interface, to selectively use the interrupt
driven mode for CPU efficiency (or polled mode for low latency).</p></li>
</ul>
</li>
<li><p>Sharded data/processing model</p></li>
</ul>
<section id="background">
<h3>Background<a class="headerlink" href="#background" title="Permalink to this heading"></a></h3>
<p>Both in-place and out-of-place update strategies have their pros and cons.</p>
<ul>
<li><p>Log-structured store</p>
<p>Log-structured based storage system is a typical example that adopts an update-out-of-place approach.
It never modifies the written data. Writes always go to the end of the log. It enables I/O sequentializing.</p>
<ul class="simple">
<li><p>Pros</p>
<ul>
<li><p>Without a doubt, one sequential write is enough to store the data</p></li>
<li><p>It naturally supports transaction (this is no overwrite, so the store can rollback
previous stable state)</p></li>
<li><p>Flash friendly (it mitigates GC burden on SSDs)</p></li>
</ul>
</li>
<li><p>Cons</p>
<ul>
<li><p>There is host-side GC that induces overheads</p>
<ul>
<li><p>I/O amplification (host-side)</p></li>
<li><p>More host-CPU consumption</p></li>
</ul>
</li>
<li><p>Slow metadata lookup</p></li>
<li><p>Space overhead (live and unused data co-exist)</p></li>
</ul>
</li>
</ul>
</li>
<li><p>In-place update store</p>
<p>The update-in-place strategy has been used widely for conventional file systems such as ext4 and xfs.
Once a block has been placed in a given disk location, it doesn’t move.
Thus, writes go to the corresponding location in the disk.</p>
<ul class="simple">
<li><p>Pros</p>
<ul>
<li><p>Less host-CPU consumption (No host-side GC is required)</p></li>
<li><p>Fast lookup</p></li>
<li><p>No additional space for log-structured, but there is internal fragmentation</p></li>
</ul>
</li>
<li><p>Cons</p>
<ul>
<li><p>More writes occur to record the data (metadata and data section are separated)</p></li>
<li><p>It cannot support transaction. Some form of WAL required to ensure update atomicity
in the general case</p></li>
<li><p>Flash unfriendly (Give more burdens on SSDs due to device-level GC)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="motivation-and-key-idea">
<h3>Motivation and Key idea<a class="headerlink" href="#motivation-and-key-idea" title="Permalink to this heading"></a></h3>
<p>In modern distributed storage systems, a server node can be equipped with multiple
NVMe storage devices. In fact, ten or more NVMe SSDs could be attached on a server.
As a result, it is hard to achieve NVMe SSD’s full performance due to the limited CPU resources
available in a server node. In such environments, CPU tends to become a performance bottleneck.
Thus, now we should focus on minimizing host-CPU consumption, which is the same as the Crimson’s objective.</p>
<p>Towards an object store highly optimized for CPU consumption, three design choices have been made.</p>
<ul>
<li><p><strong>PoseidonStore does not have a black-box component like RocksDB in BlueStore.</strong></p>
<p>Thus, it can avoid unnecessary data copy and serialization/deserialization overheads.
Moreover, we can remove an unnecessary file abstraction layer, which was required to run RocksDB.
Object data and metadata is now directly mapped to the disk blocks.
Eliminating all these overheads will reduce CPU consumption (e.g., pre-allocation, NVME atomic feature).</p>
</li>
<li><p><strong>PoseidonStore uses hybrid update strategies for different data size, similar to BlueStore.</strong></p>
<p>As we discussed, both in-place and out-of-place update strategies have their pros and cons.
Since CPU is only bottlenecked under small I/O workloads, we chose update-in-place for small I/Os to mininize CPU consumption
while choosing update-out-of-place for large I/O to avoid double write. Double write for small data may be better than host-GC overhead
in terms of CPU consumption in the long run. Although it leaves GC entirely up to SSDs,</p>
</li>
<li><p><strong>PoseidonStore makes use of io_uring, new kernel asynchronous I/O interface to exploit interrupt-driven I/O.</strong></p>
<p>User-space driven I/O solutions like SPDK provide high I/O performance by avoiding syscalls and enabling zero-copy
access from the application. However, it does not support interrupt-driven I/O, which is only possible with kernel-space driven I/O.
Polling is good for low-latency but bad for CPU efficiency. On the other hand, interrupt is good for CPU efficiency and bad for
low-latency (but not that bad as I/O size increases). Note that network acceleration solutions like DPDK also excessively consume
CPU resources for polling. Using polling both for network and storage processing aggravates CPU consumption.
Since network is typically much faster and has a higher priority than storage, polling should be applied only to network processing.</p>
</li>
</ul>
<p>high-end NVMe SSD has enough powers to handle more works. Also, SSD lifespan is not a practical concern these days
(there is enough program-erase cycle limit <a class="footnote-reference brackets" href="#f1" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>). On the other hand, for large I/O workloads, the host can afford process host-GC.
Also, the host can garbage collect invalid objects more effectively when their size is large</p>
</section>
<section id="observation">
<h3>Observation<a class="headerlink" href="#observation" title="Permalink to this heading"></a></h3>
<p>Two data types in Ceph</p>
<ul class="simple">
<li><p>Data (object data)</p>
<ul>
<li><p>The cost of double write is high</p></li>
<li><p>The best method to store this data is in-place update</p>
<ul>
<li><p>At least two operations required to store the data: 1) data and 2) location of
data. Nevertheless, a constant number of operations would be better than out-of-place
even if it aggravates WAF in SSDs</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Metadata or small data (e.g., object_info_t, snapset, pg_log, and collection)</p>
<ul>
<li><p>Multiple small-sized metadata entries for an object</p></li>
<li><p>The best solution to store this data is WAL + Using cache</p>
<ul>
<li><p>The efficient way to store metadata is to merge all metadata related to data
and store it though a single write operation even though it requires background
flush to update the data partition</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="design">
<h2>Design<a class="headerlink" href="#design" title="Permalink to this heading"></a></h2>
<p class="ditaa">
<img src="../../../_images/ditaa-72dc91113fe37b2aae54a128a103124acabbf60f.png"/>
</p>
<ul class="simple">
<li><p>WAL</p>
<ul>
<li><p>Log, metadata and small data are stored in the WAL partition</p></li>
<li><p>Space within the WAL partition is continually reused in a circular manner</p></li>
<li><p>Flush data to trim WAL as necessary</p></li>
</ul>
</li>
<li><p>Disk layout</p>
<ul>
<li><p>Data blocks are metadata blocks or data blocks</p></li>
<li><p>Freelist manages the root of free space B+tree</p></li>
<li><p>Super block contains management info for a data partition</p></li>
<li><p>Onode radix tree info contains the root of onode radix tree</p></li>
</ul>
</li>
</ul>
<section id="i-o-procedure">
<h3>I/O procedure<a class="headerlink" href="#i-o-procedure" title="Permalink to this heading"></a></h3>
<ul>
<li><p>Write</p>
<p>For incoming writes, data is handled differently depending on the request size;
data is either written twice (WAL) or written in a log-structured manner.</p>
<ol class="arabic">
<li><p>If Request Size ≤ Threshold (similar to minimum allocation size in BlueStore)</p>
<p>Write data and metadata to [WAL] —flush—&gt; Write them to [Data section (in-place)] and
[Metadata section], respectively.</p>
<p>Since the CPU becomes the bottleneck for small I/O workloads, in-place update scheme is used.
Double write for small data may be better than host-GC overhead in terms of CPU consumption
in the long run</p>
</li>
<li><p>Else if Request Size &gt; Threshold</p>
<p>Append data to [Data section (log-structure)] —&gt; Write the corresponding metadata to [WAL]
—flush—&gt; Write the metadata to [Metadata section]</p>
</li>
</ol>
<p>For large I/O workloads, the host can afford process host-GC
Also, the host can garbage collect invalid objects more effectively when their size is large</p>
<p>Note that Threshold can be configured to a very large number so that only the scenario (1) occurs.
With this design, we can control the overall I/O procedure with the optimizations for crimson
as described above.</p>
<ul>
<li><p>Detailed flow</p>
<p>We make use of a NVMe write command which provides atomicity guarantees (Atomic Write Unit Power Fail)
For example, 512 Kbytes of data can be atomically written at once without fsync().</p>
<ul class="simple">
<li><p>stage 1</p>
<ul>
<li><p>if the data is small
WAL (written) --&gt; | TxBegin A | Log Entry | TxEnd A |
Append a log entry that contains pg_log, snapset, object_infot_t and block allocation
using NVMe atomic write command on the WAL</p></li>
<li><p>if the data is large
Data partition (written) --&gt; | Data blocks |</p></li>
</ul>
</li>
<li><p>stage 2</p>
<ul>
<li><p>if the data is small
No need.</p></li>
<li><p>if the data is large
Then, append the metadata to WAL.
WAL --&gt; | TxBegin A | Log Entry | TxEnd A |</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Read</p>
<ul class="simple">
<li><p>Use the cached object metadata to find out the data location</p></li>
<li><p>If not cached, need to search WAL after checkpoint and Object meta partition to find the
latest meta data</p></li>
</ul>
</li>
<li><p>Flush (WAL --&gt; Data partition)</p>
<ul class="simple">
<li><p>Flush WAL entries that have been committed. There are two conditions
(1. the size of WAL is close to full, 2. a signal to flush).
We can mitigate the overhead of frequent flush via batching processing, but it leads to
delaying completion.</p></li>
</ul>
</li>
</ul>
</section>
<section id="crash-consistency">
<h3>Crash consistency<a class="headerlink" href="#crash-consistency" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Large case</p>
<ol class="arabic simple">
<li><p>Crash occurs right after writing Data blocks</p>
<ul>
<li><p>Data partition --&gt; | Data blocks |</p></li>
<li><p>We don’t need to care this case. Data is not alloacted yet in reality. The blocks will be reused.</p></li>
</ul>
</li>
<li><p>Crash occurs right after WAL</p>
<ul>
<li><p>Data partition --&gt; | Data blocks |</p></li>
<li><p>WAL --&gt; | TxBegin A | Log Entry | TxEnd A |</p></li>
<li><p>Write procedure is completed, so there is no data loss or inconsistent state</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Small case</p>
<ol class="arabic simple">
<li><p>Crash occurs right after writing WAL</p>
<ul>
<li><p>WAL --&gt; | TxBegin A | Log Entry| TxEnd A |</p></li>
<li><p>All data has been written</p></li>
</ul>
</li>
</ol>
</li>
</ul>
</section>
<section id="comparison">
<h3>Comparison<a class="headerlink" href="#comparison" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Best case (pre-allocation)</p>
<ul>
<li><p>Only need writes on both WAL and Data partition without updating object metadata (for the location).</p></li>
</ul>
</li>
<li><p>Worst case</p>
<ul>
<li><p>At least three writes are required additionally on WAL, object metadata, and data blocks.</p></li>
<li><p>If the flush from WAL to the data parition occurs frequently, radix tree onode structure needs to be update
in many times. To minimize such overhead, we can make use of batch processing to minimize the update on the tree
(the data related to the object has a locality because it will have the same parent node, so updates can be minimized)</p></li>
</ul>
</li>
<li><p>WAL needs to be flushed if the WAL is close to full or a signal to flush.</p>
<ul>
<li><p>The premise behind this design is OSD can manage the latest metadata as a single copy. So,
appended entries are not to be read</p></li>
</ul>
</li>
<li><p>Either best of the worst case does not produce severe I/O amplification (it produce I/Os, but I/O rate is constant)
unlike LSM-tree DB (the proposed design is similar to LSM-tree which has only level-0)</p></li>
</ul>
</section>
</section>
<section id="detailed-design">
<h2>Detailed Design<a class="headerlink" href="#detailed-design" title="Permalink to this heading"></a></h2>
<ul>
<li><p>Onode lookup</p>
<ul>
<li><p>Radix tree
Our design is entirely based on the prefix tree. Ceph already makes use of the characteristic of OID’s prefix to split or search
the OID (e.g., pool id + hash + oid). So, the prefix tree fits well to store or search the object. Our scheme is designed
to lookup the prefix tree efficiently.</p></li>
<li><p>Sharded partition
A few bits (leftmost bits of the hash) of the OID determine a sharded partition where the object is located.
For example, if the number of partitions is configured as four, The entire space of the hash in hobject_t
can be divided into four domains (0x0xxx ~ 0x3xxx, 0x4xxx ~ 0x7xxx, 0x8xxx ~ 0xBxxx and 0xCxxx ~ 0xFxxx).</p></li>
<li><p>Ondisk onode</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">stuct</span><span class="w"> </span><span class="n">onode</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">extent_tree</span><span class="w"> </span><span class="n">block_maps</span><span class="p">;</span>
<span class="w">  </span><span class="n">b</span><span class="o">+</span><span class="n">_tree</span><span class="w"> </span><span class="n">omaps</span><span class="p">;</span>
<span class="w">  </span><span class="n">map</span><span class="w"> </span><span class="n">xattrs</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>onode contains the radix tree nodes for lookup, which means we can search for objects using tree node information in onode.
Also, if the data size is small, the onode can embed the data and xattrs.
The onode is fixed size (256 or 512 byte). On the other hands, omaps and block_maps are variable-length by using pointers in the onode.</p>
<p class="ditaa">
<img src="../../../_images/ditaa-75b677947b3541b99cef4099073c45c4a1b64019.png"/>
</p>
</li>
<li><p>Lookup
The location of the root of onode tree is specified on Onode radix tree info, so we can find out where the object
is located by using the root of prefix tree. For example, shared partition is determined by OID as described above.
Using the rest of the OID’s bits and radix tree, lookup procedure find outs the location of the onode.
The extent tree (block_maps) contains where data chunks locate, so we finally figure out the data location.</p></li>
</ul>
</li>
<li><p>Allocation</p>
<ul>
<li><p>Sharded partitions</p>
<p>The entire disk space is divided into several  data chunks called sharded partition (SP).
Each SP has its own data structures to manage the partition.</p>
</li>
<li><p>Data allocation</p>
<p>As we explained above, the management infos (e.g., super block, freelist info, onode radix tree info) are pre-allocated
in each shared partition. Given OID, we can map any data in Data block section to the extent tree in the onode.
Blocks can be allocated by searching the free space tracking data structure (we explain below).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">+-----------------------------------+</span>
<span class="o">|</span> <span class="n">onode</span> <span class="n">radix</span> <span class="n">tree</span> <span class="n">root</span> <span class="n">node</span> <span class="n">block</span>  <span class="o">|</span>
<span class="o">|</span>          <span class="p">(</span><span class="n">Per</span><span class="o">-</span><span class="n">SP</span> <span class="n">Meta</span><span class="p">)</span>            <span class="o">|</span>
<span class="o">|</span>                                   <span class="o">|</span>
<span class="o">|</span>           <span class="c1"># of records            |</span>
<span class="o">|</span>    <span class="n">left_sibling</span> <span class="o">/</span> <span class="n">right_sibling</span>   <span class="o">|</span>
<span class="o">|</span> <span class="o">+--------------------------------+|</span>
<span class="o">|</span> <span class="o">|</span> <span class="n">keys</span><span class="p">[</span><span class="c1"># of records]             ||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">+-----------------------------+||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">|</span>    <span class="n">start</span> <span class="n">onode</span> <span class="n">ID</span>           <span class="o">|||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">|</span>           <span class="o">...</span>               <span class="o">|||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">+-----------------------------+||</span>
<span class="o">|</span> <span class="o">+--------------------------------||</span>
<span class="o">|</span> <span class="o">+--------------------------------+|</span>
<span class="o">|</span> <span class="o">|</span> <span class="n">ptrs</span><span class="p">[</span><span class="c1"># of records]             ||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">+-----------------------------+||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">|</span>       <span class="n">SP</span> <span class="n">block</span> <span class="n">number</span>       <span class="o">|||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">|</span>           <span class="o">...</span>               <span class="o">|||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">+-----------------------------+||</span>
<span class="o">|</span> <span class="o">+--------------------------------+|</span>
<span class="o">+-----------------------------------+</span>
</pre></div>
</div>
</li>
<li><p>Free space tracking
The freespace is tracked on a per-SP basis. We can use extent-based B+tree in XFS for free space tracking.
The freelist info contains the root of free space B+tree. Granularity is a data block in Data blocks partition.
The data block is the smallest and fixed size unit of data.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">+-----------------------------------+</span>
<span class="o">|</span> <span class="n">Free</span> <span class="n">space</span> <span class="n">B</span><span class="o">+</span><span class="n">tree</span> <span class="n">root</span> <span class="n">node</span> <span class="n">block</span> <span class="o">|</span>
<span class="o">|</span>          <span class="p">(</span><span class="n">Per</span><span class="o">-</span><span class="n">SP</span> <span class="n">Meta</span><span class="p">)</span>            <span class="o">|</span>
<span class="o">|</span>                                   <span class="o">|</span>
<span class="o">|</span>           <span class="c1"># of records            |</span>
<span class="o">|</span>    <span class="n">left_sibling</span> <span class="o">/</span> <span class="n">right_sibling</span>   <span class="o">|</span>
<span class="o">|</span> <span class="o">+--------------------------------+|</span>
<span class="o">|</span> <span class="o">|</span> <span class="n">keys</span><span class="p">[</span><span class="c1"># of records]             ||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">+-----------------------------+||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">|</span>   <span class="n">startblock</span> <span class="o">/</span> <span class="n">blockcount</span>   <span class="o">|||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">|</span>           <span class="o">...</span>               <span class="o">|||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">+-----------------------------+||</span>
<span class="o">|</span> <span class="o">+--------------------------------||</span>
<span class="o">|</span> <span class="o">+--------------------------------+|</span>
<span class="o">|</span> <span class="o">|</span> <span class="n">ptrs</span><span class="p">[</span><span class="c1"># of records]             ||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">+-----------------------------+||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">|</span>       <span class="n">SP</span> <span class="n">block</span> <span class="n">number</span>       <span class="o">|||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">|</span>           <span class="o">...</span>               <span class="o">|||</span>
<span class="o">|</span> <span class="o">|</span> <span class="o">+-----------------------------+||</span>
<span class="o">|</span> <span class="o">+--------------------------------+|</span>
<span class="o">+-----------------------------------+</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Omap and xattr
In this design, omap and xattr data is tracked by b+tree in onode. The onode only has the root node of b+tree.
The root node contains entires which indicate where the key onode exists.
So, if we know the onode, omap can be found via omap b+tree.</p></li>
<li><p>Fragmentation</p>
<ul>
<li><p>Internal fragmentation</p>
<p>We pack different types of data/metadata in a single block as many as possible to reduce internal fragmentation.
Extent-based B+tree may help reduce this further by allocating contiguous blocks that best fit for the object</p>
</li>
<li><p>External fragmentation</p>
<p>Frequent object create/delete may lead to external fragmentation
In this case, we need cleaning work (GC-like) to address this.
For this, we are referring the NetApp’s Continuous Segment Cleaning, which seems similar to the SeaStore’s approach
Countering Fragmentation in an Enterprise Storage System (NetApp, ACM TOS, 2020)</p>
</li>
</ul>
</li>
</ul>
<p class="ditaa">
<img src="../../../_images/ditaa-6c6d2588bf36b08b102874c67eab3ad1c8c0978f.png"/>
</p>
<section id="wal">
<h3>WAL<a class="headerlink" href="#wal" title="Permalink to this heading"></a></h3>
<p>Each SP has a WAL.
The datas written to the WAL are metadata updates, free space update and small data.
Note that only data smaller than the predefined threshold needs to be written to the WAL.
The larger data is written to the unallocated free space and its onode’s extent_tree is updated accordingly
(also on-disk extent tree). We statically allocate WAL partition aside from data partition pre-configured.</p>
</section>
<section id="partition-and-reactor-thread">
<h3>Partition and Reactor thread<a class="headerlink" href="#partition-and-reactor-thread" title="Permalink to this heading"></a></h3>
<p>In early stage development, PoseidonStore will employ static allocation of partition. The number of sharded partitions
is fixed and the size of each partition also should be configured before running cluster.
But, the number of partitions can grow as below. We leave this as a future work.
Also, each reactor thread has a static set of SPs.</p>
<p class="ditaa">
<img src="../../../_images/ditaa-c9ec85abfc04ae3c783efa77b4c216c09eefede1.png"/>
</p>
</section>
<section id="cache">
<h3>Cache<a class="headerlink" href="#cache" title="Permalink to this heading"></a></h3>
<p>There are mainly two cache data structures; onode cache and block cache.
It looks like below.</p>
<ol class="arabic simple">
<li><p>Onode cache:
lru_map &lt;OID, OnodeRef&gt;;</p></li>
<li><p>Block cache (data and omap):
Data cache --&gt; lru_map &lt;paddr, value&gt;</p></li>
</ol>
<p>To fill the onode data structure, the target onode needs to be retrieved using the prefix tree.
Block cache is used for caching a block contents. For a transaction, all the updates to blocks
(including object meta block, data block) are first performed in the in-memory block cache.
After writing a transaction to the WAL, the dirty blocks are flushed to their respective locations in the
respective partitions.
PoseidonStore can configure cache size for each type. Simple LRU cache eviction strategy can be used for both.</p>
</section>
<section id="sharded-partitions-with-cross-sp-transaction">
<h3>Sharded partitions (with cross-SP transaction)<a class="headerlink" href="#sharded-partitions-with-cross-sp-transaction" title="Permalink to this heading"></a></h3>
<p>The entire disk space is divided into a number of chunks called sharded partitions (SP).
The prefixes of the parent collection ID (original collection ID before collection splitting. That is, hobject.hash)
is used to map any collections to SPs.
We can use BlueStore’s approach for collection splitting, changing the number of significant bits for the collection prefixes.
Because the prefixes of the parent collection ID do not change even after collection splitting, the mapping between
the collection and SP are maintained.
The number of SPs may be configured to match the number of CPUs allocated for each disk so that each SP can hold
a number of objects large enough for cross-SP transaction not to occur.</p>
<p>In case of need of cross-SP transaction, we could use the global WAL. The coordinator thread (mainly manages global partition) handles
cross-SP transaction via acquire the source SP and target SP locks before processing the cross-SP transaction.
Source and target probably are blocked.</p>
<p>For the load unbalanced situation,
Poseidonstore can create partitions to make full use of entire space efficiently and provide load balaning.</p>
</section>
<section id="cow-clone">
<h3>CoW/Clone<a class="headerlink" href="#cow-clone" title="Permalink to this heading"></a></h3>
<p>As for CoW/Clone, a clone has its own onode like other normal objects.</p>
<p>Although each clone has its own onode, data blocks should be shared between the original object and clones
if there are no changes on them to minimize the space overhead.
To do so, the reference count for the data blocks is needed to manage those shared data blocks.</p>
<p>To deal with the data blocks which has the reference count, poseidon store makes use of shared_blob
which maintains the referenced data block.</p>
<p>As shown the figure as below,
the shared_blob tracks the data blocks shared between other onodes by using a reference count.
The shared_blobs are managed by shared_blob_list in the superblock.</p>
<p class="ditaa">
<img src="../../../_images/ditaa-339fcaa5cd205474908e08350a9ca538bbe50cce.png"/>
</p>
</section>
</section>
<section id="plans">
<h2>Plans<a class="headerlink" href="#plans" title="Permalink to this heading"></a></h2>
<p>All PRs should contain unit tests to verify its minimal functionality.</p>
<ul>
<li><p>WAL and block cache implementation</p>
<p>As a first step, we are going to build the WAL including the I/O procedure to read/write the WAL.
With WAL development, the block cache needs to be developed together.
Besides, we are going to add an I/O library to read/write from/to the NVMe storage to
utilize NVMe feature and the asynchronous interface.</p>
</li>
<li><p>Radix tree and onode</p>
<p>First, submit a PR against this file with a more detailed on disk layout and lookup strategy for the onode radix tree.
Follow up with implementation based on the above design once design PR is merged.
The second PR will be the implementation regarding radix tree which is the key structure to look up
objects.</p>
</li>
<li><p>Extent tree</p>
<p>This PR is the extent tree to manage data blocks in the onode. We build the extent tree, and
demonstrate how it works when looking up the object.</p>
</li>
<li><p>B+tree for omap</p>
<p>We will put together a simple key/value interface for omap. This probably will be a separate PR.</p>
</li>
<li><p>CoW/Clone</p>
<p>To support CoW/Clone, shared_blob and shared_blob_list will be added.</p>
</li>
<li><p>Integration to Crimson as to I/O interfaces</p>
<p>At this stage, interfaces for interacting with Crimson such as queue_transaction(), read(), clone_range(), etc.
should work right.</p>
</li>
<li><p>Configuration</p>
<p>We will define Poseidon store configuration in detail.</p>
</li>
<li><p>Stress test environment and integration to teuthology</p>
<p>We will add stress tests and teuthology suites.</p>
</li>
</ul>
<p class="rubric">Footnotes</p>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="f1" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Stathis Maneas, Kaveh Mahdaviani, Tim Emami, Bianca Schroeder: A Study of SSD Reliability in Large Scale Enterprise Storage Deployments. FAST 2020: 137-149</p>
</aside>
</aside>
</section>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../backfillmachine/" class="btn btn-neutral float-left" title="BackfillMachine" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../../governance/" class="btn btn-neutral float-right" title="项目管理" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>