

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>SeaStore &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="Sepia 社区测试实验室" href="../sepia/" />
    <link rel="prev" title="Ceph Release Process" href="../release-process/" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../internals/">Ceph 内幕</a> &raquo;</li>
        
      <li>SeaStore</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/dev/seastore.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../">
          

          
            
            <img src="../../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../start/intro/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">安装 Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephadm/">Cephadm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer_guide/">开发者指南</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../internals/">Ceph 内幕</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../blkin/">用 Blkin 追踪 Ceph</a></li>
<li class="toctree-l2"><a class="reference internal" href="../bluestore/">BlueStore Internals</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cache-pool/">Cache pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ceph_krb_auth/">如何配置好 Ceph Kerberos 认证的详细文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephadm/">Developing with cephadm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-mirroring/">CephFS Mirroring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-reclaim/">CephFS Reclaim Interface</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephfs-snapshots/">CephFS 快照</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephx/">Cephx</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cephx_protocol/">Cephx 认证协议详细阐述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../config/">配置管理系统</a></li>
<li class="toctree-l2"><a class="reference internal" href="../config-key/">config-key layout</a></li>
<li class="toctree-l2"><a class="reference internal" href="../context/">CephContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="../corpus/">资料库结构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cpu-profiler/">Oprofile 的安装</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cxx/">C++17 and libstdc++ ABI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deduplication/">去重</a></li>
<li class="toctree-l2"><a class="reference internal" href="../delayed-delete/">CephFS delayed deletion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev_cluster_deployement/">开发集群的部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dev_cluster_deployement/#id5">在同一机器上部署多套开发集群</a></li>
<li class="toctree-l2"><a class="reference internal" href="../development-workflow/">开发流程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../documenting/">为 Ceph 写作文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../encoding/">序列化（编码、解码）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../erasure-coded-pool/">纠删码存储池</a></li>
<li class="toctree-l2"><a class="reference internal" href="../file-striping/">File striping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../freebsd/">FreeBSD Implementation details</a></li>
<li class="toctree-l2"><a class="reference internal" href="../generatedocs/">Ceph 文档的构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../iana/">IANA 号</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kubernetes/">Hacking on Ceph in Kubernetes with Rook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libs/">Library architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../logging/">集群日志的用法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../logs/">调试日志</a></li>
<li class="toctree-l2"><a class="reference internal" href="../macos/">在 MacOS 上构建</a></li>
<li class="toctree-l2"><a class="reference internal" href="../messenger/">Messenger notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-bootstrap/">Monitor bootstrap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-on-disk-formats/">ON-DISK FORMAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mon-osdmap-prune/">FULL OSDMAP VERSION PRUNING</a></li>
<li class="toctree-l2"><a class="reference internal" href="../msgr2/">msgr2 协议（ msgr2.0 和 msgr2.1 ）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../network-encoding/">Network Encoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../network-protocol/">网络协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../object-store/">对象存储架构概述</a></li>
<li class="toctree-l2"><a class="reference internal" href="../osd-class-path/">OSD class path issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../peering/">互联</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf/">Using perf</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf_counters/">性能计数器</a></li>
<li class="toctree-l2"><a class="reference internal" href="../perf_histograms/">Perf histograms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../placement-group/">PG （归置组）说明</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quick_guide/">开发者指南（快速）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rados-client-protocol/">RADOS 客户端协议</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-diff/">RBD 增量备份</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-export/">RBD Export &amp; Import</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rbd-layering/">RBD Layering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../release-checklists/">Release checklists</a></li>
<li class="toctree-l2"><a class="reference internal" href="../release-process/">Ceph Release Process</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">SeaStore</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#goals-and-basics">Goals and Basics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#motivation-and-background">Motivation and background</a></li>
<li class="toctree-l4"><a class="reference internal" href="#data-layout-basics">Data layout basics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#persistent-memory">Persistent Memory</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#design">Design</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#segment-layout">Segment Layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="#journal-and-atomicity">Journal and Atomicity</a></li>
<li class="toctree-l4"><a class="reference internal" href="#block-cache">Block Cache</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gc">GC</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#logical-layout">Logical Layout</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lbamanager-btreelbamanager">LBAManager/BtreeLBAManager</a></li>
<li class="toctree-l4"><a class="reference internal" href="#transactionmanager">TransactionManager</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#next-steps">Next Steps</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#journal">Journal</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cache">Cache</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lbamanager">LBAManager</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">GC</a></li>
<li class="toctree-l4"><a class="reference internal" href="#other">Other</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#objectstore-considerations">ObjectStore considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#splits-merges-and-sharding">Splits, merges, and sharding</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../sepia/">Sepia 社区测试实验室</a></li>
<li class="toctree-l2"><a class="reference internal" href="../session_authentication/">Session Authentication for the Cephx Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../testing/">测试笔记</a></li>
<li class="toctree-l2"><a class="reference internal" href="../versions/">Public OSD Version</a></li>
<li class="toctree-l2"><a class="reference internal" href="../wireshark/">Wireshark Dissector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../zoned-storage/">Zoned Storage Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../osd_internals/">OSD 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../mds_internals/">MDS 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../radosgw/">RADOS 网关开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ceph-volume/">ceph-volume 开发者文档</a></li>
<li class="toctree-l2"><a class="reference internal" href="../crimson/">Crimson developer documentation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <div class="section" id="seastore">
<h1>SeaStore<a class="headerlink" href="#seastore" title="Permalink to this headline">¶</a></h1>
<div class="section" id="goals-and-basics">
<h2>Goals and Basics<a class="headerlink" href="#goals-and-basics" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Target NVMe devices.  Not primarily concerned with pmem or HDD.</p></li>
<li><p>make use of SPDK for user-space driven IO</p></li>
<li><p>Use Seastar futures programming model to facilitate
run-to-completion and a sharded memory/processing model</p></li>
<li><p>Allow zero- (or minimal) data copying on read and write paths when
combined with a seastar-based messenger using DPDK</p></li>
</ul>
<div class="section" id="motivation-and-background">
<h3>Motivation and background<a class="headerlink" href="#motivation-and-background" title="Permalink to this headline">¶</a></h3>
<p>All flash devices are internally structured in terms of segments that
can be written efficiently but must be erased in their entirety.  The
NVMe device generally has limited knowledge about what data in a
segment is still “live” (hasn’t been logically discarded), making the
inevitable garbage collection within the device inefficient.  We can
design an on-disk layout that is friendly to GC at lower layers and
drive garbage collection at higher layers.</p>
<p>In principle a fine-grained discard could communicate our intent to
the device, but in practice discard is poorly implemented in the
device and intervening software layers.</p>
<p>The basic idea is that all data will be stream out sequentially to
large segments on the device.  In the SSD hardware, segments are
likely to be on the order of 100’s of MB to tens of GB.</p>
<p>SeaStore’s logical segments would ideally be perfectly aligned with
the hardware segments.  In practice, it may be challenging to
determine geometry and to sufficiently hint to the device that LBAs
being written should be aligned to the underlying hardware.  In the
worst case, we can structure our logical segments to correspond to
e.g. 5x the physical segment size so that we have about ~20% of our
data misaligned.</p>
<p>When we reach some utilization threshold, we mix cleaning work in with
the ongoing write workload in order to evacuate live data from
previously written segments.  Once they are completely free we can
discard the entire segment so that it can be erased and reclaimed by
the device.</p>
<p>The key is to mix a small bit of cleaning work with every write
transaction to avoid spikes and variance in write latency.</p>
</div>
<div class="section" id="data-layout-basics">
<h3>Data layout basics<a class="headerlink" href="#data-layout-basics" title="Permalink to this headline">¶</a></h3>
<p>One or more cores/shards will be reading and writing to the device at
once.  Each shard will have its own independent data it is operating
on and stream to its own open segments.  Devices that support streams
can be hinted accordingly so that data from different shards is not
mixed on the underlying media.</p>
</div>
<div class="section" id="persistent-memory">
<h3>Persistent Memory<a class="headerlink" href="#persistent-memory" title="Permalink to this headline">¶</a></h3>
<p>As the intial sequential design above matures, we’ll introduce
persistent memory support for metadata and caching structures.</p>
</div>
</div>
<div class="section" id="design">
<h2>Design<a class="headerlink" href="#design" title="Permalink to this headline">¶</a></h2>
<p>The design is based heavily on both f2fs and btrfs.  Each reactor
manages its own root.  Prior to reusing a segment, we rewrite any live
blocks to an open segment.</p>
<p>Because we are only writing sequentially to open segments, we must
“clean” one byte of an existing segment for every byte written at
steady state.  Generally, we’ll need to reserve some portion of the
usable capacity in order to ensure that write amplification remains
acceptably low (20% for 2x? – TODO: find prior work).  As a design
choice, we want to avoid a background gc scheme as it tends to
complicate estimating operation cost and tends to introduce
non-deterministic latency behavior.  Thus, we want a set of structures
that permits us to relocate blocks from existing segments inline with
ongoing client IO.</p>
<p>To that end, at a high level, we’ll maintain 2 basic metadata trees.
First, we need a tree mapping ghobject_t-&gt;onode_t (onode_by_hobject).
Second, we need a way to find live blocks within a segment and a way
to decouple internal references from physical locations (lba_tree).</p>
<p>Each onode contains xattrs directly as well as the top of the omap and
extent trees (optimization: we ought to be able to fit small enough
objects into the onode).</p>
<div class="section" id="segment-layout">
<h3>Segment Layout<a class="headerlink" href="#segment-layout" title="Permalink to this headline">¶</a></h3>
<p>The backing storage is abstracted into a set of segments.  Each
segment can be in one of 3 states: empty, open, closed.  The byte
contents of a segment are a sequence of records.  A record is prefixed
by a header (including length and checksums) and contains a sequence
of deltas and/or blocks.  Each delta describes a logical mutation for
some block.  Each included block is an aligned extent addressable by
&lt;segment_id_t, segment_offset_t&gt;.  A transaction can be implemented by
constructing a record combining deltas and updated blocks and writing
it to an open segment.</p>
<p>Note that segments will generally be large (something like &gt;=256MB),
so there will not typically be very many of them.</p>
<p>record: [ header | delta | delta… | block | block … ]
segment: [ record … ]</p>
<p>See src/crimson/os/seastore/journal.h for Journal implementation
See src/crimson/os/seastore/seastore_types.h for most seastore structures.</p>
<p>Each shard will keep open N segments for writes</p>
<ul class="simple">
<li><p>HDD: N is probably 1 on one shard</p></li>
<li><p>NVME/SSD: N is probably 2/shard, one for “journal” and one for
finished data records as their lifetimes are different.</p></li>
</ul>
<p>I think the exact number to keep open and how to partition writes
among them will be a tuning question – gc/layout should be flexible.
Where practical, the goal is probably to partition blocks by expected
lifetime so that a segment either has long lived or short lived
blocks.</p>
<p>The backing physical layer is exposed via a segment based interface.
See src/crimson/os/seastore/segment_manager.h</p>
</div>
<div class="section" id="journal-and-atomicity">
<h3>Journal and Atomicity<a class="headerlink" href="#journal-and-atomicity" title="Permalink to this headline">¶</a></h3>
<p>One open segment is designated to be the journal.  A transaction is
represented by an atomically written record.  A record will contain
blocks written as part of the transaction as well as deltas which
are logical mutations to existing physical extents.  Transaction deltas
are always written to the journal.  If the transaction is associated
with blocks written to other segments, final record with the deltas
should be written only once the other blocks are persisted.  Crash
recovery is done by finding the segment containing the beginning of
the current journal, loading the root node, replaying the deltas, and
loading blocks into the cache as needed.</p>
<p>See src/crimson/os/seastore/journal.h</p>
</div>
<div class="section" id="block-cache">
<h3>Block Cache<a class="headerlink" href="#block-cache" title="Permalink to this headline">¶</a></h3>
<p>Every block is in one of two states:</p>
<ul class="simple">
<li><p>clean: may be in cache or not, reads may cause cache residence or
not</p></li>
<li><p>dirty: the current version of the record requires overlaying deltas
from the journal.  Must be fully present in the cache.</p></li>
</ul>
<p>Periodically, we need to trim the journal (else, we’d have to replay
journal deltas from the beginning of time).  To do this, we need to
create a checkpoint by rewriting the root blocks and all currently
dirty blocks.  Note, we can do journal checkpoints relatively
infrequently, and they needn’t block the write stream.</p>
<p>Note, deltas may not be byte range modifications.  Consider a btree
node structured with keys to the left and values to the right (common
trick for improving point query/key scan performance).  Inserting a
key/value into that node at the min would involve moving a bunch of
bytes, which would be expensive (or verbose) to express purely as a
sequence of byte operations.  As such, each delta indicates the type
as well as the location of the corresponding extent.  Each block
type can therefore implement CachedExtent::apply_delta as appopriate.</p>
<p>See src/os/crimson/seastore/cached_extent.h.
See src/os/crimson/seastore/cache.h.</p>
</div>
<div class="section" id="gc">
<h3>GC<a class="headerlink" href="#gc" title="Permalink to this headline">¶</a></h3>
<p>Prior to reusing a segment, we must relocate all live blocks.  Because
we only write sequentially to empty segments, for every byte we write
to currently open segments, we need to clean a byte of an existing
closed segment.  As a design choice, we’d like to avoid background
work as it complicates estimating operation cost and has a tendency to
create non-deterministic latency spikes.  Thus, under normal operation
each seastore reactor will be inserting enough work to clean a segment
at the same rate as incoming operations.</p>
<p>In order to make this cheap for sparse segments, we need a way to
positively identify dead blocks.  Thus, for every block written, an
entry will be added to the lba tree with a pointer to the previous lba
in the segment.  Any transaction that moves a block or modifies the
reference set of an existing one will include deltas/blocks required
to update the lba tree to update or remove the previous block
allocation.  The gc state thus simply needs to maintain an iterator
(of a sort) into the lba tree segment linked list for segment
currently being cleaned and a pointer to the next record to be
examined – records not present in the allocation tree may still
contain roots (like allocation tree blocks) and so the record metadata
must be checked for a flag indicating root blocks.</p>
<p>For each transaction, we evaluate a heuristic function of the
currently available space and currently live space in order to
determine whether we need to do cleaning work (could be simply a range
of live/used space ratios).</p>
<p>TODO: there is not yet a GC implementation</p>
</div>
</div>
<div class="section" id="logical-layout">
<h2>Logical Layout<a class="headerlink" href="#logical-layout" title="Permalink to this headline">¶</a></h2>
<p>Using the above block and delta semantics, we build two root level trees:
- onode tree: maps hobject_t to onode_t
- lba_tree: maps lba_t to lba_range_t</p>
<p>Each of the above structures is comprised of blocks with mutations
encoded in deltas.  Each node of the above trees maps onto a block.
Each block is either physically addressed (root blocks and the
lba_tree nodes) or is logically addressed (everything else).
Physically addressed blocks are located by a paddr_t: &lt;segment_id_t,
segment_off_t&gt; tuple and are marked as physically addressed in the
record.  Logical blocks are addressed by laddr_t and require a lookup in
the lba_tree to address.</p>
<p>Because the cache/transaction machinery lives below the level of the
lba tree, we can represent atomic mutations of the lba tree and other
structures by simply including both in a transaction.</p>
<div class="section" id="lbamanager-btreelbamanager">
<h3>LBAManager/BtreeLBAManager<a class="headerlink" href="#lbamanager-btreelbamanager" title="Permalink to this headline">¶</a></h3>
<p>Implementations of the LBAManager interface are responsible for managing
the logical-&gt;physical mapping – see crimson/os/seastore/lba_manager.h.</p>
<p>The BtreeLBAManager implements this interface directly on top of
Journal and SegmentManager using a wandering btree approach.</p>
<p>Because SegmentManager does not let us predict the location of a
committed record (a property of both SMR and Zone devices), references
to blocks created within the same transaction will necessarily be
<em>relative</em> addresses.  The BtreeLBAManager maintains an invariant by
which the in-memory copy of any block will contain only absolute
addresses when !is_pending() – on_commit and complete_load fill in
absolute addresses based on the actual block addr and on_delta_write
does so based on the just committed record.  When is_pending(), if
is_initial_pending references in memory are block_relative (because
they will be written to the original block location) and
record_relative otherwise (value will be written to delta).</p>
</div>
<div class="section" id="transactionmanager">
<h3>TransactionManager<a class="headerlink" href="#transactionmanager" title="Permalink to this headline">¶</a></h3>
<p>The TransactionManager is responsible for presenting a unified
interface on top of the Journal, SegmentManager, Cache, and
LBAManager.  Users can allocate and mutate extents based on logical
addresses with segment cleaning handled in the background.</p>
<p>See crimson/os/seastore/transaction_manager.h</p>
</div>
</div>
<div class="section" id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this headline">¶</a></h2>
<div class="section" id="journal">
<h3>Journal<a class="headerlink" href="#journal" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Support for scanning a segment to find physically addressed blocks</p></li>
<li><p>Add support for trimming the journal and releasing segments.</p></li>
</ul>
</div>
<div class="section" id="cache">
<h3>Cache<a class="headerlink" href="#cache" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Support for rewriting dirty blocks</p>
<ul>
<li><p>Need to add support to CachedExtent for finding/updating
dependent blocks</p></li>
<li><p>Need to add support for adding dirty block writout to
try_construct_record</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="lbamanager">
<h3>LBAManager<a class="headerlink" href="#lbamanager" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Add support for pinning</p></li>
<li><p>Add segment -&gt; laddr for use in GC</p></li>
<li><p>Support for locating remaining used blocks in segments</p></li>
</ul>
</div>
<div class="section" id="id1">
<h3>GC<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Initial implementation</p></li>
<li><p>Support in BtreeLBAManager for tracking used blocks in segments</p></li>
<li><p>Heuristic for identifying segments to clean</p></li>
</ul>
</div>
<div class="section" id="other">
<h3>Other<a class="headerlink" href="#other" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Add support for periodically generating a journal checkpoint.</p></li>
<li><p>Onode tree</p></li>
<li><p>Extent tree</p></li>
<li><p>Remaining ObjectStore integration</p></li>
</ul>
</div>
</div>
<div class="section" id="objectstore-considerations">
<h2>ObjectStore considerations<a class="headerlink" href="#objectstore-considerations" title="Permalink to this headline">¶</a></h2>
<div class="section" id="splits-merges-and-sharding">
<h3>Splits, merges, and sharding<a class="headerlink" href="#splits-merges-and-sharding" title="Permalink to this headline">¶</a></h3>
<p>One of the current ObjectStore requirements is to be able to split a
collection (PG) in O(1) time.  Starting in mimic, we also need to be
able to merge two collections into one (i.e., exactly the reverse of a
split).</p>
<p>However, the PGs that we split into would hash to different shards of
the OSD in the current sharding scheme.  One can imagine replacing
that sharding scheme with a temporary mapping directing the smaller
child PG to the right shard since we generally then migrate that PG to
another OSD anyway, but this wouldn’t help us in the merge case where
the constituent pieces may start out on different shards and
ultimately need to be handled in the same collection (and be operated
on via single transactions).</p>
<p>This suggests that we likely need a way for data written via one shard
to “switch ownership” and later be read and managed by a different
shard.</p>
</div>
</div>
</div>



           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../sepia/" class="btn btn-neutral float-right" title="Sepia 社区测试实验室" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../release-process/" class="btn btn-neutral float-left" title="Ceph Release Process" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>