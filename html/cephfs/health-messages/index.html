
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>CephFS health messages &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../" />
    <link rel="up" title="Ceph Filesystem" href="../" />
    <link rel="next" title="Troubleshooting" href="../troubleshooting/" />
    <link rel="prev" title="Handling a full Ceph filesystem" href="../full/" />
    <script type="text/javascript" src="http://ayni.ceph.com/public/js/ceph.js"></script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../troubleshooting/" title="Troubleshooting"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../full/" title="Handling a full Ceph filesystem"
             accesskey="P">previous</a> |</li>
        <li><a href="../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../" accesskey="U">Ceph Filesystem</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="cephfs-health-messages">
<h1>CephFS health messages<a class="headerlink" href="#cephfs-health-messages" title="Permalink to this headline">¶</a></h1>
<div class="section" id="cluster-health-checks">
<h2>Cluster health checks<a class="headerlink" href="#cluster-health-checks" title="Permalink to this headline">¶</a></h2>
<p>The Ceph monitor daemons will generate health messages in response
to certain states of the filesystem map structure (and the enclosed MDS maps).</p>
<p>Message: mds rank(s) <em>ranks</em> have failed
Description: One or more MDS ranks are not currently assigned to
an MDS daemon; the cluster will not recover until a suitable replacement
daemon starts.</p>
<p>Message: mds rank(s) <em>ranks</em> are damaged
Description: One or more MDS ranks has encountered severe damage to
its stored metadata, and cannot start again until it is repaired.</p>
<p>Message: mds cluster is degraded
Description: One or more MDS ranks are not currently up and running, clients
may pause metadata IO until this situation is resolved.  This includes
ranks being failed or damaged, and additionally includes ranks
which are running on an MDS but have not yet made it to the <em>active</em>
state (e.g. ranks currently in <em>replay</em> state).</p>
<p>Message: mds <em>names</em> are laggy
Description: The named MDS daemons have failed to send beacon messages
to the monitor for at least <tt class="docutils literal"><span class="pre">mds_beacon_grace</span></tt> (default 15s), while
they are supposed to send beacon messages every <tt class="docutils literal"><span class="pre">mds_beacon_interval</span></tt>
(default 4s).  The daemons may have crashed.  The Ceph monitor will
automatically replace laggy daemons with standbys if any are available.</p>
<p>Message: insufficient standby daemons available
Description: One or more file systems are configured to have a certain number
of standby daemons available (including daemons in standby-replay) but the
cluster does not have enough standby daemons. The standby deamons not in replay
count towards any file system (i.e. they may overlap). This warning can
configured by setting <tt class="docutils literal"><span class="pre">ceph</span> <span class="pre">fs</span> <span class="pre">set</span> <span class="pre">&lt;fs&gt;</span> <span class="pre">standby_count_wanted</span> <span class="pre">&lt;count&gt;</span></tt>.  Use
zero for <tt class="docutils literal"><span class="pre">count</span></tt> to disable.</p>
</div>
<div class="section" id="daemon-reported-health-checks">
<h2>Daemon-reported health checks<a class="headerlink" href="#daemon-reported-health-checks" title="Permalink to this headline">¶</a></h2>
<p>MDS daemons can identify a variety of unwanted conditions, and
indicate these to the operator in the output of <tt class="docutils literal"><span class="pre">ceph</span> <span class="pre">status</span></tt>.
This conditions have human readable messages, and additionally
a unique code starting MDS_HEALTH which appears in JSON output.</p>
<p>Message: &#8220;Behind on trimming...&#8221;
Code: MDS_HEALTH_TRIM
Description: CephFS maintains a metadata journal that is divided into
<em>log segments</em>.  The length of journal (in number of segments) is controlled
by the setting <tt class="docutils literal"><span class="pre">mds_log_max_segments</span></tt>, and when the number of segments
exceeds that setting the MDS starts writing back metadata so that it
can remove (trim) the oldest segments.  If this writeback is happening
too slowly, or a software bug is preventing trimming, then this health
message may appear.  The threshold for this message to appear is for the
number of segments to be double <tt class="docutils literal"><span class="pre">mds_log_max_segments</span></tt>.</p>
<p>Message: &#8220;Client <em>name</em> failing to respond to capability release&#8221;
Code: MDS_HEALTH_CLIENT_LATE_RELEASE, MDS_HEALTH_CLIENT_LATE_RELEASE_MANY
Description: CephFS clients are issued <em>capabilities</em> by the MDS, which
are like locks.  Sometimes, for example when another client needs access,
the MDS will request clients release their capabilities.  If the client
is unresponsive or buggy, it might fail to do so promptly or fail to do
so at all.  This message appears if a client has taken longer than
<tt class="docutils literal"><span class="pre">mds_revoke_cap_timeout</span></tt> (default 60s) to comply.</p>
<p>Message: &#8220;Client <em>name</em> failing to respond to cache pressure&#8221;
Code: MDS_HEALTH_CLIENT_RECALL, MDS_HEALTH_CLIENT_RECALL_MANY
Description: Clients maintain a metadata cache.  Items (such as inodes)
in the client cache are also pinned in the MDS cache, so when the MDS
needs to shrink its cache (to stay within <tt class="docutils literal"><span class="pre">mds_cache_size</span></tt>), it
sends messages to clients to shrink their caches too.  If the client
is unresponsive or buggy, this can prevent the MDS from properly staying
within its <tt class="docutils literal"><span class="pre">mds_cache_size</span></tt> and it may eventually run out of memory
and crash.  This message appears if a client has taken more than
<tt class="docutils literal"><span class="pre">mds_recall_state_timeout</span></tt> (default 60s) to comply.</p>
<p>Message: &#8220;Client <em>name</em> failing to advance its oldest client/flush tid&#8221;
Code: MDS_HEALTH_CLIENT_OLDEST_TID, MDS_HEALTH_CLIENT_OLDEST_TID_MANY
Description: The CephFS client-MDS protocol uses a field called the
<em>oldest tid</em> to inform the MDS of which client requests are fully
complete and may therefore be forgotten about by the MDS.  If a buggy
client is failing to advance this field, then the MDS may be prevented
from properly cleaning up resources used by client requests.  This message
appears if a client appears to have more than <tt class="docutils literal"><span class="pre">max_completed_requests</span></tt>
(default 100000) requests that are complete on the MDS side but haven&#8217;t
yet been accounted for in the client&#8217;s <em>oldest tid</em> value.</p>
<p>Message: &#8220;Metadata damage detected&#8221;
Code: MDS_HEALTH_DAMAGE,
Description: Corrupt or missing metadata was encountered when reading
from the metadata pool.  This message indicates that the damage was
sufficiently isolated for the MDS to continue operating, although
client accesses to the damaged subtree will return IO errors.  Use
the <tt class="docutils literal"><span class="pre">damage</span> <span class="pre">ls</span></tt> admin socket command to get more detail on the damage.
This message appears as soon as any damage is encountered.</p>
<p>Message: &#8220;MDS in read-only mode&#8221;
Code: MDS_HEALTH_READ_ONLY,
Description: The MDS has gone into readonly mode and will return EROFS
error codes to client operations that attempt to modify any metadata.  The
MDS will go into readonly mode if it encounters a write error while
writing to the metadata pool, or if forced to by an administrator using
the <em>force_readonly</em> admin socket command.</p>
<p>Message: <em>N</em> slow requests are blocked&#8221;
Code: MDS_HEALTH_SLOW_REQUEST,
Description: One or more client requests have not been completed promptly,
indicating that the MDS is either running very slowly, or that the RADOS
cluster is not acknowledging journal writes promptly, or that there is a bug.
Use the <tt class="docutils literal"><span class="pre">ops</span></tt> admin socket command to list outstanding metadata operations.
This message appears if any client requests have taken longer than
<tt class="docutils literal"><span class="pre">mds_op_complaint_time</span></tt> (default 30s).</p>
<p>Message: &#8220;Too many inodes in cache&#8221;
Code: MDS_HEALTH_CACHE_OVERSIZED
Description: The MDS is not succeeding in trimming its cache to comply
with the limit set by the administrator.  If the MDS cache becomes too large,
the daemon may exhaust available memory and crash.
This message appears if the actual cache size (in inodes) is at least 50%
greater than <tt class="docutils literal"><span class="pre">mds_cache_size</span></tt> (default 100000).</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../">
              <img class="logo" src="../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../start/intro/">Intro to Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../start/">Installation (Quick)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">Installation (Manual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph Storage Cluster</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../">Ceph Filesystem</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../#using-cephfs">Using CephFS</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../rados/deployment/ceph-deploy-mds/">Add/Remove MDS(s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/">Terminology</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/#referring-to-mds-daemons">Referring to MDS daemons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/#managing-failover">Managing failover</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/#configuring-standby-daemons">Configuring standby daemons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/#examples">Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mds-config-ref/">MDS Configuration Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../client-config-ref/">Client Configuration Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../journaler/">Journaler Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../man/8/ceph-mds/">Manpage ceph-mds</a></li>
<li class="toctree-l3"><a class="reference internal" href="../createfs/">Create CephFS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../kernel/">Mount CephFS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../fuse/">Mount CephFS as FUSE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../fstab/">Mount CephFS in fstab</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../man/8/ceph-fuse/">Manpage ceph-fuse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../man/8/mount.ceph/">Manpage mount.ceph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../best-practices/">Deployment best practices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../administration/">Administrative commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="../posix/">POSIX compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../experimental-features/">Experimental Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../experimental-features/#previously-experimental-features">Previously experimental features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quota/">CephFS Quotas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hadoop/">Using Ceph with Hadoop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cephfs-journal-tool/">cephfs-journal-tool</a></li>
<li class="toctree-l3"><a class="reference internal" href="../file-layouts/">File layouts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../eviction/">Client eviction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../full/">Handling full filesystems</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="">Health messages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cluster-health-checks">Cluster health checks</a></li>
<li class="toctree-l4"><a class="reference internal" href="#daemon-reported-health-checks">Daemon-reported health checks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting/">Troubleshooting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../disaster-recovery/">Disaster recovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="../client-auth/">Client authentication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../upgrading/">Upgrading old filesystems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dirfrags/">Configuring directory fragmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../multimds/">Configuring multiple active MDS daemons</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../#for-developers">For developers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/rbd/">Ceph Block Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph Object Gateway</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph Manager Daemon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/">Ceph Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Glossary</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../troubleshooting/" title="Troubleshooting"
             >next</a> |</li>
        <li class="right" >
          <a href="../full/" title="Handling a full Ceph filesystem"
             >previous</a> |</li>
        <li><a href="../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../" >Ceph Filesystem</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2016, Red Hat, Inc, and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>