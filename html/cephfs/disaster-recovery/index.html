
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Disaster recovery &mdash; Ceph Documentation</title>
    
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     'dev',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="top" title="Ceph Documentation" href="../../" />
    <link rel="up" title="Ceph Filesystem" href="../" />
    <link rel="next" title="CephFS Client Capabilities" href="../client-auth/" />
    <link rel="prev" title="Troubleshooting" href="../troubleshooting/" />
    <script type="text/javascript" src="http://ayni.ceph.com/public/js/ceph.js"></script>

  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../client-auth/" title="CephFS Client Capabilities"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../troubleshooting/" title="Troubleshooting"
             accesskey="P">previous</a> |</li>
        <li><a href="../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../" accesskey="U">Ceph Filesystem</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="disaster-recovery">
<h1>Disaster recovery<a class="headerlink" href="#disaster-recovery" title="Permalink to this headline">¶</a></h1>
<div class="admonition danger">
<p class="first admonition-title">Danger</p>
<p class="last">The notes in this section are aimed at experts, making a best effort
to recovery what they can from damaged filesystems.  These steps
have the potential to make things worse as well as better.  If you
are unsure, do not proceed.</p>
</div>
<div class="section" id="journal-export">
<h2>Journal export<a class="headerlink" href="#journal-export" title="Permalink to this headline">¶</a></h2>
<p>Before attempting dangerous operations, make a copy of the journal like so:</p>
<div class="highlight-python"><pre>cephfs-journal-tool journal export backup.bin</pre>
</div>
<p>Note that this command may not always work if the journal is badly corrupted,
in which case a RADOS-level copy should be made (<a class="reference external" href="http://tracker.ceph.com/issues/9902">http://tracker.ceph.com/issues/9902</a>).</p>
</div>
<div class="section" id="dentry-recovery-from-journal">
<h2>Dentry recovery from journal<a class="headerlink" href="#dentry-recovery-from-journal" title="Permalink to this headline">¶</a></h2>
<p>If a journal is damaged or for any reason an MDS is incapable of replaying it,
attempt to recover what file metadata we can like so:</p>
<div class="highlight-python"><pre>cephfs-journal-tool event recover_dentries summary</pre>
</div>
<p>This command by default acts on MDS rank 0, pass &#8211;rank=&lt;n&gt; to operate on other ranks.</p>
<p>This command will write any inodes/dentries recoverable from the journal
into the backing store, if these inodes/dentries are higher-versioned
than the previous contents of the backing store.  If any regions of the journal
are missing/damaged, they will be skipped.</p>
<p>Note that in addition to writing out dentries and inodes, this command will update
the InoTables of each &#8216;in&#8217; MDS rank, to indicate that any written inodes&#8217; numbers
are now in use.  In simple cases, this will result in an entirely valid backing
store state.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">The resulting state of the backing store is not guaranteed to be self-consistent,
and an online MDS scrub will be required afterwards.  The journal contents
will not be modified by this command, you should truncate the journal
separately after recovering what you can.</p>
</div>
</div>
<div class="section" id="journal-truncation">
<h2>Journal truncation<a class="headerlink" href="#journal-truncation" title="Permalink to this headline">¶</a></h2>
<p>If the journal is corrupt or MDSs cannot replay it for any reason, you can
truncate it like so:</p>
<div class="highlight-python"><pre>cephfs-journal-tool journal reset</pre>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Resetting the journal <em>will</em> lose metadata unless you have extracted
it by other means such as <tt class="docutils literal"><span class="pre">recover_dentries</span></tt>.  It is likely to leave
some orphaned objects in the data pool.  It may result in re-allocation
of already-written inodes, such that permissions rules could be violated.</p>
</div>
</div>
<div class="section" id="mds-table-wipes">
<h2>MDS table wipes<a class="headerlink" href="#mds-table-wipes" title="Permalink to this headline">¶</a></h2>
<p>After the journal has been reset, it may no longer be consistent with respect
to the contents of the MDS tables (InoTable, SessionMap, SnapServer).</p>
<p>To reset the SessionMap (erase all sessions), use:</p>
<div class="highlight-python"><pre>cephfs-table-tool all reset session</pre>
</div>
<p>This command acts on the tables of all &#8216;in&#8217; MDS ranks.  Replace &#8216;all&#8217; with an MDS
rank to operate on that rank only.</p>
<p>The session table is the table most likely to need resetting, but if you know you
also need to reset the other tables then replace &#8216;session&#8217; with &#8216;snap&#8217; or &#8216;inode&#8217;.</p>
</div>
<div class="section" id="mds-map-reset">
<h2>MDS map reset<a class="headerlink" href="#mds-map-reset" title="Permalink to this headline">¶</a></h2>
<p>Once the in-RADOS state of the filesystem (i.e. contents of the metadata pool)
is somewhat recovered, it may be necessary to update the MDS map to reflect
the contents of the metadata pool.  Use the following command to reset the MDS
map to a single MDS:</p>
<div class="highlight-python"><pre>ceph fs reset &lt;fs name&gt; --yes-i-really-mean-it</pre>
</div>
<p>Once this is run, any in-RADOS state for MDS ranks other than 0 will be ignored:
as a result it is possible for this to result in data loss.</p>
<p>One might wonder what the difference is between &#8216;fs reset&#8217; and &#8216;fs remove; fs new&#8217;.  The
key distinction is that doing a remove/new will leave rank 0 in &#8216;creating&#8217; state, such
that it would overwrite any existing root inode on disk and orphan any existing files.  In
contrast, the &#8216;reset&#8217; command will leave rank 0 in &#8216;active&#8217; state such that the next MDS
daemon to claim the rank will go ahead and use the existing in-RADOS metadata.</p>
</div>
<div class="section" id="recovery-from-missing-metadata-objects">
<h2>Recovery from missing metadata objects<a class="headerlink" href="#recovery-from-missing-metadata-objects" title="Permalink to this headline">¶</a></h2>
<p>Depending on what objects are missing or corrupt, you may need to
run various commands to regenerate default versions of the
objects.</p>
<div class="highlight-python"><pre># Session table
cephfs-table-tool 0 reset session
# SnapServer
cephfs-table-tool 0 reset snap
# InoTable
cephfs-table-tool 0 reset inode
# Journal
cephfs-journal-tool --rank=0 journal reset
# Root inodes ("/" and MDS directory)
cephfs-data-scan init</pre>
</div>
<p>Finally, you can regenerate metadata objects for missing files
and directories based on the contents of a data pool.  This is
a two-phase process.  First, scanning <em>all</em> objects to calculate
size and mtime metadata for inodes.  Second, scanning the first
object from every file to collect this metadata and inject
it into the metadata pool.</p>
<div class="highlight-python"><pre>cephfs-data-scan scan_extents &lt;data pool&gt;
cephfs-data-scan scan_inodes &lt;data pool&gt;</pre>
</div>
<p>This command may take a <em>very long</em> time if there are many
files or very large files in the data pool.</p>
<p>To accelerate the process, run multiple instances of the tool.</p>
<p>Decide on a number of workers, and pass each worker a number within
the range 0-(worker_m - 1).</p>
<p>The example below shows how to run 4 workers simultaneously:</p>
<div class="highlight-python"><pre># Worker 0
cephfs-data-scan scan_extents --worker_n 0 --worker_m 4 &lt;data pool&gt;
# Worker 1
cephfs-data-scan scan_extents --worker_n 1 --worker_m 4 &lt;data pool&gt;
# Worker 2
cephfs-data-scan scan_extents --worker_n 2 --worker_m 4 &lt;data pool&gt;
# Worker 3
cephfs-data-scan scan_extents --worker_n 3 --worker_m 4 &lt;data pool&gt;

# Worker 0
cephfs-data-scan scan_inodes --worker_n 0 --worker_m 4 &lt;data pool&gt;
# Worker 1
cephfs-data-scan scan_inodes --worker_n 1 --worker_m 4 &lt;data pool&gt;
# Worker 2
cephfs-data-scan scan_inodes --worker_n 2 --worker_m 4 &lt;data pool&gt;
# Worker 3
cephfs-data-scan scan_inodes --worker_n 3 --worker_m 4 &lt;data pool&gt;</pre>
</div>
<p>It is <strong>important</strong> to ensure that all workers have completed the
scan_extents phase before any workers enter the scan_inodes phase.</p>
<p>After completing the metadata recovery, you may want to run cleanup
operation to delete ancillary data geneated during recovery.</p>
<div class="highlight-python"><pre>cephfs-data-scan cleanup &lt;data pool&gt;</pre>
</div>
</div>
<div class="section" id="finding-files-affected-by-lost-data-pgs">
<h2>Finding files affected by lost data PGs<a class="headerlink" href="#finding-files-affected-by-lost-data-pgs" title="Permalink to this headline">¶</a></h2>
<p>Losing a data PG may affect many files.  Files are split into many objects,
so identifying which files are affected by loss of particular PGs requires
a full scan over all object IDs that may exist within the size of a file.
This type of scan may be useful for identifying which files require
restoring from a backup.</p>
<div class="admonition danger">
<p class="first admonition-title">Danger</p>
<p class="last">This command does not repair any metadata, so when restoring files in
this case you must <em>remove</em> the damaged file, and replace it in order
to have a fresh inode.  Do not overwrite damaged files in place.</p>
</div>
<p>If you know that objects have been lost from PGs, use the <tt class="docutils literal"><span class="pre">pg_files</span></tt>
subcommand to scan for files that may have been damaged as a result:</p>
<div class="highlight-python"><pre>cephfs-data-scan pg_files &lt;path&gt; &lt;pg id&gt; [&lt;pg id&gt;...]</pre>
</div>
<p>For example, if you have lost data from PGs 1.4 and 4.5, and you would like
to know which files under /home/bob might have been damaged:</p>
<div class="highlight-python"><pre>cephfs-data-scan pg_files /home/bob 1.4 4.5</pre>
</div>
<p>The output will be a list of paths to potentially damaged files, one
per line.</p>
<p>Note that this command acts as a normal CephFS client to find all the
files in the filesystem and read their layouts, so the MDS must be
up and running.</p>
</div>
<div class="section" id="using-an-alternate-metadata-pool-for-recovery">
<h2>Using an alternate metadata pool for recovery<a class="headerlink" href="#using-an-alternate-metadata-pool-for-recovery" title="Permalink to this headline">¶</a></h2>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">There has not been extensive testing of this procedure. It should be
undertaken with great care.</p>
</div>
<p>If an existing filesystem is damaged and inoperative, it is possible to create
a fresh metadata pool and attempt to reconstruct the filesystem metadata
into this new pool, leaving the old metadata in place. This could be used to
make a safer attempt at recovery since the existing metadata pool would not be
overwritten.</p>
<div class="admonition caution">
<p class="first admonition-title">Caution</p>
<p class="last">During this process, multiple metadata pools will contain data referring to
the same data pool. Extreme caution must be exercised to avoid changing the
data pool contents while this is the case. Once recovery is complete, the
damaged metadata pool should be deleted.</p>
</div>
<p>To begin this process, first create the fresh metadata pool and initialize
it with empty file system data structures:</p>
<div class="highlight-python"><pre>ceph fs flag set enable_multiple true --yes-i-really-mean-it
ceph osd pool create recovery &lt;pg-num&gt; replicated &lt;crush-ruleset-name&gt;
ceph fs new recovery-fs recovery &lt;data pool&gt; --allow-dangerous-metadata-overlay
cephfs-data-scan init --force-init --filesystem recovery-fs --alternate-pool recovery
ceph fs reset recovery-fs --yes-i-realy-mean-it
cephfs-table-tool recovery-fs:all reset session
cephfs-table-tool recovery-fs:all reset snap
cephfs-table-tool recovery-fs:all reset inode</pre>
</div>
<p>Next, run the recovery toolset using the &#8211;alternate-pool argument to output
results to the alternate pool:</p>
<div class="highlight-python"><pre>cephfs-data-scan scan_extents --alternate-pool recovery --filesystem &lt;original filesystem name&gt;
cephfs-data-scan scan_inodes --alternate-pool recovery --filesystem &lt;original filesystem name&gt; --force-corrupt --force-init &lt;original data pool name&gt;</pre>
</div>
<p>If the damaged filesystem contains dirty journal data, it may be recovered next
with:</p>
<div class="highlight-python"><pre>cephfs-journal-tool --rank=&lt;original filesystem name&gt;:0 event recover_dentries list --alternate-pool recovery
cephfs-journal-tool --rank recovery-fs:0 journal reset --force</pre>
</div>
<p>After recovery, some recovered directories will have incorrect link counts.
Ensure the parameter mds_debug_scatterstat is set to false (the default) to
prevent the MDS from checking the link counts, then run a forward scrub to
repair them. Ensure you have an MDS running and issue:</p>
<div class="highlight-python"><pre>ceph daemon mds.a scrub_path / recursive repair</pre>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../">
              <img class="logo" src="../../_static/logo.png" alt="Logo"/>
            </a></p>
<h3><a href="../../">Table Of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../start/intro/">Intro to Ceph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../start/">Installation (Quick)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/">Installation (Manual)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rados/">Ceph Storage Cluster</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../">Ceph Filesystem</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../#using-cephfs">Using CephFS</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../../rados/deployment/ceph-deploy-mds/">Add/Remove MDS(s)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/">Terminology</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/#referring-to-mds-daemons">Referring to MDS daemons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/#managing-failover">Managing failover</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/#configuring-standby-daemons">Configuring standby daemons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../standby/#examples">Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mds-config-ref/">MDS Configuration Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../client-config-ref/">Client Configuration Settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../journaler/">Journaler Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../man/8/ceph-mds/">Manpage ceph-mds</a></li>
<li class="toctree-l3"><a class="reference internal" href="../createfs/">Create CephFS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../kernel/">Mount CephFS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../fuse/">Mount CephFS as FUSE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../fstab/">Mount CephFS in fstab</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../man/8/ceph-fuse/">Manpage ceph-fuse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../man/8/mount.ceph/">Manpage mount.ceph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../best-practices/">Deployment best practices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../administration/">Administrative commands</a></li>
<li class="toctree-l3"><a class="reference internal" href="../posix/">POSIX compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../experimental-features/">Experimental Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../experimental-features/#previously-experimental-features">Previously experimental features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../quota/">CephFS Quotas</a></li>
<li class="toctree-l3"><a class="reference internal" href="../hadoop/">Using Ceph with Hadoop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../cephfs-journal-tool/">cephfs-journal-tool</a></li>
<li class="toctree-l3"><a class="reference internal" href="../file-layouts/">File layouts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../eviction/">Client eviction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../full/">Handling full filesystems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../health-messages/">Health messages</a></li>
<li class="toctree-l3"><a class="reference internal" href="../troubleshooting/">Troubleshooting</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="">Disaster recovery</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#journal-export">Journal export</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dentry-recovery-from-journal">Dentry recovery from journal</a></li>
<li class="toctree-l4"><a class="reference internal" href="#journal-truncation">Journal truncation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mds-table-wipes">MDS table wipes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mds-map-reset">MDS map reset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#recovery-from-missing-metadata-objects">Recovery from missing metadata objects</a></li>
<li class="toctree-l4"><a class="reference internal" href="#finding-files-affected-by-lost-data-pgs">Finding files affected by lost data PGs</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-an-alternate-metadata-pool-for-recovery">Using an alternate metadata pool for recovery</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../client-auth/">Client authentication</a></li>
<li class="toctree-l3"><a class="reference internal" href="../upgrading/">Upgrading old filesystems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dirfrags/">Configuring directory fragmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../multimds/">Configuring multiple active MDS daemons</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../#for-developers">For developers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../rbd/rbd/">Ceph Block Device</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../radosgw/">Ceph Object Gateway</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mgr/">Ceph Manager Daemon</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../architecture/">Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/">Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release-notes/">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../releases/">Ceph Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary/">Glossary</a></li>
</ul>


<!-- ugly kludge to make genindex look like it's part of the toc -->
<ul style="margin-top: -10px"><li class="toctree-l1"><a class="reference internal" href="../../genindex/">Index</a></li></ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../search/" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex/" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex/" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../client-auth/" title="CephFS Client Capabilities"
             >next</a> |</li>
        <li class="right" >
          <a href="../troubleshooting/" title="Troubleshooting"
             >previous</a> |</li>
        <li><a href="../../">Ceph Documentation</a> &raquo;</li>
          <li><a href="../" >Ceph Filesystem</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2016, Red Hat, Inc, and contributors. Licensed under Creative Commons BY-SA.
    </div>
  </body>
</html>