

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>MON Service &mdash; Ceph Documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/ceph.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../../genindex/" />
    <link rel="search" title="Search" href="../../../search/" />
    <link rel="next" title="MGR Service" href="../mgr/" />
    <link rel="prev" title="Service Management" href="../" /> 
</head>

<body class="wy-body-for-nav">

   
  <header class="top-bar">
    <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../">Cephadm</a></li>
          <li class="breadcrumb-item"><a href="../">Service Management</a></li>
      <li class="breadcrumb-item active">MON Service</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/cephadm/services/mon.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
  </header>
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #eee" >
          

          
            <a href="../../../" class="icon icon-home"> Ceph
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../start/">Ceph 简介</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/">安装 Ceph</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../">Cephadm</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../compatibility/">Compatibility and Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../install/">部署个全新的 Ceph 集群</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../adoption/">现有集群切换到 cephadm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../host-management/">Host Management</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../">Service Management</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">MON Service</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#deploying-additional-monitors">Deploying additional monitors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#further-reading">Further Reading</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mgr/">MGR Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../osd/">OSD Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../rgw/">RGW Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mds/">MDS Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../nfs/">NFS Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../iscsi/">iSCSI Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../custom-container/">Custom Container Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../monitoring/">Monitoring Services</a></li>
<li class="toctree-l3"><a class="reference internal" href="../snmp-gateway/">SNMP Gateway Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tracing/">如何追踪各服务</a></li>
<li class="toctree-l3"><a class="reference internal" href="../smb/">SMB Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mgmt-gateway/">Management Gateway</a></li>
<li class="toctree-l3"><a class="reference internal" href="../oauth2-proxy/">OAuth2 Proxy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#service-status">Service Status</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#daemon-status">Daemon Status</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#service-specification">Service Specification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#daemon-placement">Daemon Placement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#extra-container-arguments">Extra Container Arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#extra-entrypoint-arguments">Extra Entrypoint Arguments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#custom-config-files">Custom Config Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#removing-a-service">Removing a Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../#disabling-automatic-deployment-of-daemons">Disabling automatic deployment of daemons</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../upgrade/">升级 Ceph</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../operations/">Cephadm operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../client-setup/">Client Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../troubleshooting/">Troubleshooting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../dev/cephadm/">Cephadm Feature Planning</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../rados/">Ceph 存储集群</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cephfs/">Ceph 文件系统</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rbd/">Ceph 块设备</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../radosgw/">Ceph 对象网关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/">Ceph 管理器守护进程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mgr/dashboard/">Ceph 仪表盘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../monitoring/">监控概览</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/">API 文档</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../architecture/">体系结构</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/developer_guide/">开发者指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dev/internals/">Ceph 内幕</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../governance/">项目管理</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../foundation/">Ceph 基金会</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../ceph-volume/">ceph-volume</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/general/">Ceph 版本（总目录）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../releases/">Ceph 版本（索引）</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../security/">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hardware-monitoring/">硬件监控</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../glossary/">Ceph 术语</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jaegertracing/">Tracing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../translation_cn/">中文版翻译资源</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../">Ceph</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
<div id="dev-warning" class="admonition note">
  <p class="first admonition-title">Notice</p>
  <p class="last">This document is for a development version of Ceph.</p>
</div>
  <div id="docubetter" align="right" style="padding: 5px; font-weight: bold;">
    <a href="https://pad.ceph.com/p/Report_Documentation_Bugs">Report a Documentation Bug</a>
  </div>

  
  <section id="mon-service">
<h1>MON Service<a class="headerlink" href="#mon-service" title="Permalink to this heading"></a></h1>
<section id="deploying-additional-monitors">
<span id="deploy-additional-monitors"></span><h2>Deploying additional monitors<a class="headerlink" href="#deploying-additional-monitors" title="Permalink to this heading"></a></h2>
<p>A typical Ceph cluster has three or five monitor daemons that are spread
across different hosts.  We recommend deploying five monitors if there are
five or more nodes in your cluster.</p>
<p>Ceph deploys monitor daemons automatically as the cluster grows and Ceph
scales back monitor daemons automatically as the cluster shrinks. The
smooth execution of this automatic growing and shrinking depends upon
proper subnet configuration.</p>
<p>The cephadm bootstrap procedure assigns the first monitor daemon in the
cluster to a particular subnet. <code class="docutils literal notranslate"><span class="pre">cephadm</span></code> designates that subnet as the
default subnet of the cluster. New monitor daemons will be assigned by
default to that subnet unless cephadm is instructed to do otherwise.</p>
<p>If all of the Ceph monitor daemons in your cluster are in the same subnet,
manual administration of the Ceph monitor daemons is not necessary.
<code class="docutils literal notranslate"><span class="pre">cephadm</span></code> will automatically add up to five monitors to the subnet, as
needed, as new hosts are added to the cluster.</p>
<p>By default, cephadm will deploy 5 daemons on arbitrary hosts. See
<a class="reference internal" href="../#orchestrator-cli-placement-spec"><span class="std std-ref">Daemon Placement</span></a> for details of specifying
the placement of daemons.</p>
<section id="designating-a-particular-subnet-for-monitors">
<h3>Designating a Particular Subnet for Monitors<a class="headerlink" href="#designating-a-particular-subnet-for-monitors" title="Permalink to this heading"></a></h3>
<p>To designate a particular IP subnet for use by Ceph monitor daemons, use a
command of the following form, including the subnet’s address in <a class="reference external" href="https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation">CIDR</a>
format (e.g., <code class="docutils literal notranslate"><span class="pre">10.1.2.0/24</span></code>):</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><style type="text/css">
span.prompt1:before {
  content: "# ";
}
</style><span class="prompt1">ceph<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>mon<span class="w"> </span>public_network<span class="w"> </span>*&lt;mon-cidr-network&gt;*</span>
</pre></div></div><p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>mon<span class="w"> </span>public_network<span class="w"> </span><span class="m">10</span>.1.2.0/24</span>
</pre></div></div></div></blockquote>
<p>Cephadm deploys new monitor daemons only on hosts that have IP addresses in
the designated subnet.</p>
<p>You can also specify two public networks by using a list of networks:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>mon<span class="w"> </span>public_network<span class="w"> </span>*&lt;mon-cidr-network1&gt;,&lt;mon-cidr-network2&gt;*</span>
</pre></div></div><p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>mon<span class="w"> </span>public_network<span class="w"> </span><span class="m">10</span>.1.2.0/24,192.168.0.1/24</span>
</pre></div></div></div></blockquote>
</section>
<section id="deploying-monitors-on-a-particular-network">
<h3>Deploying Monitors on a Particular Network<a class="headerlink" href="#deploying-monitors-on-a-particular-network" title="Permalink to this heading"></a></h3>
<p>You can explicitly specify the IP address or CIDR network for each monitor and
control where each monitor is placed.  To disable automated monitor deployment,
run this command:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>mon<span class="w"> </span>--unmanaged</span>
</pre></div></div><p>To deploy each additional monitor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>daemon<span class="w"> </span>add<span class="w"> </span>mon<span class="w"> </span>*&lt;host1:ip-or-network1&gt;</span>
</pre></div></div><p>For example, to deploy a second monitor on <code class="docutils literal notranslate"><span class="pre">newhost1</span></code> using an IP
address <code class="docutils literal notranslate"><span class="pre">10.1.2.123</span></code> and a third monitor on <code class="docutils literal notranslate"><span class="pre">newhost2</span></code> in
network <code class="docutils literal notranslate"><span class="pre">10.1.2.0/24</span></code>, run the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>mon<span class="w"> </span>--unmanaged</span>
<span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>daemon<span class="w"> </span>add<span class="w"> </span>mon<span class="w"> </span>newhost1:10.1.2.123</span>
<span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>daemon<span class="w"> </span>add<span class="w"> </span>mon<span class="w"> </span>newhost2:10.1.2.0/24</span>
</pre></div></div><p>Now, enable automatic placement of Daemons</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>mon<span class="w"> </span>--placement<span class="o">=</span><span class="s2">&quot;newhost1,newhost2,newhost3&quot;</span><span class="w"> </span>--dry-run</span>
</pre></div></div><p>See <a class="reference internal" href="../#orchestrator-cli-placement-spec"><span class="std std-ref">Daemon Placement</span></a> for details of specifying
the placement of daemons.</p>
<p>Finally apply this new placement by dropping <code class="docutils literal notranslate"><span class="pre">--dry-run</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>mon<span class="w"> </span>--placement<span class="o">=</span><span class="s2">&quot;newhost1,newhost2,newhost3&quot;</span></span>
</pre></div></div></div></blockquote>
</section>
<section id="moving-monitors-to-a-different-network">
<h3>Moving Monitors to a Different Network<a class="headerlink" href="#moving-monitors-to-a-different-network" title="Permalink to this heading"></a></h3>
<p>To move Monitors to a new network, deploy new monitors on the new network and
subsequently remove monitors from the old network. It is not advised to
modify and inject the <code class="docutils literal notranslate"><span class="pre">monmap</span></code> manually.</p>
<p>First, disable the automated placement of daemons:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>mon<span class="w"> </span>--unmanaged</span>
</pre></div></div></div></blockquote>
<p>To deploy each additional monitor:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>daemon<span class="w"> </span>add<span class="w"> </span>mon<span class="w"> </span>*&lt;newhost1:ip-or-network1&gt;*</span>
</pre></div></div></div></blockquote>
<p>For example, to deploy a second monitor on <code class="docutils literal notranslate"><span class="pre">newhost1</span></code> using an IP
address <code class="docutils literal notranslate"><span class="pre">10.1.2.123</span></code> and a third monitor on <code class="docutils literal notranslate"><span class="pre">newhost2</span></code> in
network <code class="docutils literal notranslate"><span class="pre">10.1.2.0/24</span></code>, run the following commands:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>mon<span class="w"> </span>--unmanaged</span>
<span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>daemon<span class="w"> </span>add<span class="w"> </span>mon<span class="w"> </span>newhost1:10.1.2.123</span>
<span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>daemon<span class="w"> </span>add<span class="w"> </span>mon<span class="w"> </span>newhost2:10.1.2.0/24</span>
</pre></div></div><p>Subsequently remove monitors from the old network:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>daemon<span class="w"> </span>rm<span class="w"> </span>*mon.&lt;oldhost1&gt;*</span>
</pre></div></div><p>Update the <code class="docutils literal notranslate"><span class="pre">public_network</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>mon<span class="w"> </span>public_network<span class="w"> </span>*&lt;mon-cidr-network&gt;*</span>
</pre></div></div><p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>mon<span class="w"> </span>public_network<span class="w"> </span><span class="m">10</span>.1.2.0/24</span>
</pre></div></div><p>Now, enable automatic placement of Daemons</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>mon<span class="w"> </span>--placement<span class="o">=</span><span class="s2">&quot;newhost1,newhost2,newhost3&quot;</span><span class="w"> </span>--dry-run</span>
</pre></div></div><p>See <a class="reference internal" href="../#orchestrator-cli-placement-spec"><span class="std std-ref">Daemon Placement</span></a> for details of specifying
the placement of daemons.</p>
<p>Finally apply this new placement by dropping <code class="docutils literal notranslate"><span class="pre">--dry-run</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="prompt1">ceph<span class="w"> </span>orch<span class="w"> </span>apply<span class="w"> </span>mon<span class="w"> </span>--placement<span class="o">=</span><span class="s2">&quot;newhost1,newhost2,newhost3&quot;</span></span>
</pre></div></div></div></blockquote>
</section>
<section id="setting-crush-locations-for-monitors">
<h3>Setting Crush Locations for Monitors<a class="headerlink" href="#setting-crush-locations-for-monitors" title="Permalink to this heading"></a></h3>
<p>Cephadm supports setting CRUSH locations for mon daemons
using the mon service spec. The CRUSH locations are set
by hostname. When cephadm deploys a mon on a host that matches
a hostname specified in the CRUSH locations, it will add
<code class="docutils literal notranslate"><span class="pre">--set-crush-location</span> <span class="pre">&lt;CRUSH-location&gt;</span></code> where the CRUSH location
is the first entry in the list of CRUSH locations for that
host. If multiple CRUSH locations are set for one host, cephadm
will attempt to set the additional locations using the
“ceph mon set_location” command.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<blockquote>
<div><p>Setting the CRUSH location in the spec is the recommended way of
replacing tiebreaker mon daemons, as they require having a location
set when they are added.</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Tiebreaker mon daemons are a part of stretch mode clusters. For more
info on stretch mode clusters see <a class="reference internal" href="../../../rados/operations/stretch-mode/#stretch-mode"><span class="std std-ref">Stretch Clusters</span></a></p>
</div>
</div>
<p>Example syntax for setting the CRUSH locations:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">service_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mon</span>
<span class="nt">service_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mon</span>
<span class="nt">placement</span><span class="p">:</span>
<span class="w">  </span><span class="nt">count</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">crush_locations</span><span class="p">:</span>
<span class="w">    </span><span class="nt">host1</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">datacenter=a</span>
<span class="w">    </span><span class="nt">host2</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">datacenter=b</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">rack=2</span>
<span class="w">    </span><span class="nt">host3</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">datacenter=a</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Sometimes, based on the timing of mon daemons being admitted to the mon
quorum, cephadm may fail to set the CRUSH location for some mon daemons
when multiple locations are specified. In this case, the recommended
action is to re-apply the same mon spec to retrigger the service action.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Mon daemons will only get the <code class="docutils literal notranslate"><span class="pre">--set-crush-location</span></code> flag set when cephadm
actually deploys them. This means if a spec is applied that includes a CRUSH
location for a mon that is already deployed, the flag may not be set until
a redeploy command is issued for that mon daemon.</p>
</div>
</section>
</section>
<section id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="../../../rados/operations/#rados-operations"><span class="std std-ref">集群运维</span></a></p></li>
<li><p><a class="reference internal" href="../../../rados/troubleshooting/troubleshooting-mon/#rados-troubleshooting-mon"><span class="std std-ref">监视器故障排除</span></a></p></li>
<li><p><a class="reference internal" href="../../troubleshooting/#cephadm-restore-quorum"><span class="std std-ref">Restoring the Monitor Quorum</span></a></p></li>
</ul>
</section>
</section>



<div id="support-the-ceph-foundation" class="admonition note">
  <p class="first admonition-title">Brought to you by the Ceph Foundation</p>
  <p class="last">The Ceph Documentation is a community resource funded and hosted by the non-profit <a href="https://ceph.io/en/foundation/">Ceph Foundation</a>. If you would like to support this and our other efforts, please consider <a href="https://ceph.io/en/foundation/join/">joining now</a>.</p>
</div>


           </div>
           
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../" class="btn btn-neutral float-left" title="Service Management" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../mgr/" class="btn btn-neutral float-right" title="MGR Service" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2016, Ceph authors and contributors. Licensed under Creative Commons Attribution Share Alike 3.0 (CC-BY-SA-3.0).</p>
  </div>

   

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>